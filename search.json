[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "beter: Beast Templates in R, another way how to create input XML files for the Beast phylogenetic software.\nbaffle: make a waffle plots in base R graphics!\nrrnni: compute phylogenetic distances in the RNNI space"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Moravec, J. C., Lanfear, R., Spector, D. L., Diermeier, S. D., & Gavryushkin, A. (In Press). Testing for phylogenetic signal in single-cell RNA-seq data. Journal of Computational Biology. https://doi.org/10.1101/2021.01.07.425804\n\n\nChen, K., Moravec, J. C., Gavryushkin, A., Welch, D., & Drummond, A. J. (2022). Accounting for Errors in Data Improves Divergence Time Estimates in Single-cell Cancer Evolution. Molecular Biology and Evolution, 39(8), msac143. https://doi.org/10.1093/molbev/msac143\n\n\nPowell, R. M., Pattison, S., Moravec, J. C., Bhat, B., Guirguis, N., Markie, D., Jones, G. T., Copedo, J., Print, C. G., Morison, I. M., Gavryushkin, A., Gray, B., Wyeth, L. J., Eccles, M. R., & Macaulay, E. C. (2022). Tuberous sclerosis complex: A complex case. Cold Spring Harbor Molecular Case Studies, 8(3), a006182. https://doi.org/10.1101/mcs.a006182\n\n\nMoravec, J. C., Marsland, S., & Cox, M. P. (2019). Warfare induces post-marital residence change. Journal of Theoretical Biology, 474, 52–62.\n\n\nHarazim, M., Horáček, I., Jakešová, L., Luermann, K., Moravec, J. C., Morgan, S., Pikula, J., Sosík, P., Vavrušová, Z., Zahradníková, A., Zukal, J., & Martínková, N. (2018). Natural selection in bats with historical exposure to white-nose syndrome. BMC Zoology, 3(1), 8.\n\n\nMoravec, J. C., Atkinson, Q., Bowern, C., Greenhill, S. J., Jordan, F. M., Ross, R. M., Gray, R., Marsland, S., & Cox, M. P. (2018). Post-marital residence patterns show lineage-specific evolution. Evolution and Human Behavior, 39(6), 594–601.\n\n\nPečnerová, P., Moravec, J. C., & Martínková, N. (2015). A skull might lie: Modeling ancestral ranges and diet from genes and shape of tree squirrels. Systematic Biology, 64(6), 1074–1088.\n\n\nJaron, K. S., Moravec, J. C., & Martínková, N. (2014). SigHunt: Horizontal gene transfer finder optimized for eukaryotic genomes. Bioinformatics, 30(8), 1081–1086.\n\n\nMartínková, N., & Moravec, J. (2012). Multilocus phylogeny of arvicoline voles (Arvicolini, Rodentia) shows small tree terrace size. Folia Zoologica, 61(3–4), 254–267."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiří C. Moravec",
    "section": "",
    "text": "Make it pop!\n\n\n\n\n\n\n\nR\n\n\n\n\nImplementing stack behaviour in R\n\n\n\n\n\n\nMar 12, 2024\n\n\n\n\n\n\n  \n\n\n\n\nErrors, builds, and containers (2)\n\n\n\n\n\n\n\nothers\n\n\n\n\nMulti-stage dockerfile\n\n\n\n\n\n\nMar 11, 2024\n\n\n\n\n\n\n  \n\n\n\n\nExploratory Data Analysis: NZ Crash Data\n\n\n\n\n\n\n\nR\n\n\n\n\nLooking at patterns of car crashes in New Zealand\n\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nVirtualBox with Windows 11\n\n\n\n\n\n\n\nothers\n\n\n\n\nHow to get Windows 11 working on VirtualBox to run Windows-only applications\n\n\n\n\n\n\nJul 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nErrors, builds, and containers\n\n\n\n\n\n\n\nothers\n\n\n\n\nSolving unhelpful errors in build systems with containerization\n\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLearn SQL with me: NZ names\n\n\n\n\n\n\n\nsql\n\n\nNZ names\n\n\n\n\nSQL, database design and the most popular names in New Zealand\n\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDigital Cubism (1)\n\n\n\n\n\n\n\nR\n\n\ngraphics\n\n\n\n\nKmeans and CART\n\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase R from A to Z: Operators (3)\n\n\n\n\n\n\n\nR\n\n\n\n\nArithmetic, Logical and Relational operators\n\n\n\n\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase R from A to Z: Types of functions (2)\n\n\n\n\n\n\n\nR\n\n\n\n\nClosures, primitives and internals\n\n\n\n\n\n\nFeb 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase R from A to Z: Introduction (1)\n\n\n\n\n\n\n\nR\n\n\n\n\nOverview of preinstalled packages\n\n\n\n\n\n\nFeb 18, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHello World!\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jiří C. Moravec",
    "section": "",
    "text": "I am a computational biologist, bioinformatician, statistician, or just data scientist for short.\nMy main interests are phylogenetic trees and their applications, while traditionally phylogenetic trees are used to estimate the evolutionary relationship of biological species from DNA, I have applied phylogenetic trees to study population dynamics of viruses, animals, cancer cells and languages.\nMy language of choice is typically R as it is usually the fastest way to read and process data. But I am proficient with Python, C/C++ and Java as well.\nHere you can view my publications or CV."
  },
  {
    "objectID": "posts/digital_cubism/1_kmeans_cart.html",
    "href": "posts/digital_cubism/1_kmeans_cart.html",
    "title": "Digital Cubism (1)",
    "section": "",
    "text": "knitr::knit_hooks$set(crop = knitr::hook_pdfcrop)\n\nI generaly do not like cubism, to me it looks messy, with body parts all over the place. But there is a subset of cubism that appeals to me, pieces like Girl with Mandolin, with the sharp outline and geometric appearance, yet the figure is still easily recognized.\n\n\n\nGirl with mandolin, Credit: Wikipedia\n\n\nThis gave me an idea to try approximate image with a series of geometric shapes. Ideally, we would use only a few squares, rectangles and so on, but achieve a recognizable image. Mabye this is isn’t truly cubism, but more something akin to mosaic or Stained Glass technique but it sounds like fun.\nI wasn’t the only one who came with this idea. Found an R blog is.R(), who approached this problem through k-means.\nFor these experiments, I will use my own photo, there are plenty convenient colour contrasts, but you can use whatever you want.\n\nk-means\nFirst, we will reproduce the technique through k-means. This is quite smart idea, because the code is really simple. We just need to unroll an image from its raster representation (RGB array) into a data.frame, and back.\n\n\nCode\nimage2df = function(img){\n    cols = ncol(img)\n    rows = nrow(img)\n\n    x = rep(seq_len(cols), each=rows)\n    y = rep(seq_len(rows), times=cols)\n\n    # R matrices are in column format, so we can just convert them to vectors\n    r = img[,,1] |> as.vector()\n    g = img[,,2] |> as.vector()\n    b = img[,,3] |> as.vector()\n\n    data.frame(x = x, y = y, r = r, g = g, b = b)\n    }\n\n\ndf2image = function(df){\n    img = array( dim = c(max(df$y), max(df$x), 3) )\n    img[,,1] = df$r\n    img[,,2] = df$g\n    img[,,3] = df$b\n\n    structure(img, class = c(\"image\", \"array\"))\n    }\n\n\nplot.image = function(x){\n    plot.new()\n    par(\"mar\" = c(0, 0, 0, 0))\n    plot.window(c(1, ncol(x)), c(1, nrow(x)), xaxs = \"i\", yaxs = \"i\", asp=1)\n    rasterImage(x, 1, 1, ncol(x), nrow(x), interpolate = FALSE)\n    }\n\n\nOnce we have that, applying k-means is simple:\n\nkmeans.image = function(img, centers, iter.max = 10, ...){\n    df = image2df(img)\n    means = kmeans(df, centers, iter.max = iter.max, ...)\n    color = means$centers[means$cluster, 3:5]\n    df2 = cbind(df[,1:2], color)\n    img2 = df2image(df2)\n    img2\n    }\n\n\nlibrary(\"jpeg\")\n\n# use your own image here\nimg = structure( readJPEG(\"profile.jpg\"), class = c(\"image\", \"array\") )\nkmeans.image(img, 500) |> plot.image()\n\n\n\n\nAll we do here is basically applying k-means on the coordinates. This means that all we are doing is constructing voronoi polygons. The colour information does not seem to have a strong effect because on the 0-1 scale, the colour differences are miniscule compared to differences between coordinates. Yet, running kmeans with colour calculates the average colour of each polygon.\nThe number of polygons used to approximate the image has quite an visual effect, from a more abstract cubic-like drawing, to a mosaic-like appearance.\n\npar(\"mfrow\" = c(2,3))\nfor(i in c(50, 100, 200, 500, 1000, 2000))\n    kmeans.image(img, i) |> plot.image()\n\n\n\n\nThe difference might be quite apparent for a terrain pictures, such as this one from New Zealand:\n\npar(\"mfrow\" = c(1,1))\nterrain = structure( readJPEG(\"terrain.jpg\"), class = c(\"image\", \"array\") )\nterrain |> plot.image()\n\n\n\npar(\"mfrow\" = c(2,3))\nfor(i in c(50, 100, 200, 500, 1000, 2000))\n    kmeans.image(terrain, i) |> plot.image()\n\n\n\n\nOn the higher range (200 and more), it lookst just as pixelation. Small number of polygons are not able to approximate the look of the image well enough, but 200-1000 looks like a sweet spot.\n\n\nCART\nOriginally I planned to go with this polygonal approach and do some adaptive voronoi polygons, move the points so that the error of the approximation is minimal. But then I was thinking, are there some other classical statistical methods that would be easily adaptable to this problem?\nThink what we want to do, divide image into areas. Well, the CART does just that, it divides the dimensions of the explanatory variables into subsets, and the response in each subset is the average response of all points in the subset. This is quite close to what we want to do! And since we have our image in a data.frame representation, running the CART algorithm is easy!\n\nlibrary(\"tree\")\ntree.image = function(img, ...){\n    df = image2df(img)\n    \n    col = list()\n    for(i in c(\"r\",\"g\",\"b\")){\n        fit = paste0(i, \" ~ x + y\") |> formula() |> tree(data=df, ...)\n        col[[i]] =  predict(fit, df)\n        }\n\n    df[names(col)] = col\n    img2 = df2image(df)\n    img2\n    }\n\ntree.image(img) |> plot.image()\n\n\n\n\nThe disadvantage of this approach is that the tree() does not allows for a multiple dependent variables, so we need to run the algorithms in each colour space separately.\n\n\nCode\n# Modified version of plot.image\n# plot a selection of the colour layers\nplot.image = function(x, col=c(\"r\",\"g\",\"b\")){\n    col = match.arg(col, several = TRUE)\n    for( i in which(!c(\"r\",\"g\",\"b\") %in% col))\n        x[,,i] = 0\n\n    plot.new()\n    par(\"mar\" = c(0,0,0,0))\n    plot.window(c(1, ncol(x)), c(1, nrow(x)), xaxs = \"i\", yaxs = \"i\", asp=1)\n    rasterImage(x, 1, 1, ncol(x), nrow(x), interpolate=FALSE)\n    }\n\npredicted = tree.image(img)\n\npar(\"mfrow\"=c(1,3))\nfor(i in c(\"r\",\"g\",\"b\"))\n    plot.image(predicted, i)\n\n\n\n\n\nSurprisingly, the information carried by green and blue is almost identical. Only reds seems to carry significantly different information.\nWhat controls the amount of approximation is the mindev parameter, other parameters starts to have an effect as a stopping parameters only when mindev is decreased significantly.\n\n\nCode\npar(\"mfrow\"=c(1,1))\ntree.image(img, mindev=10^-4) |> plot.image()\n\n\n\n\n\nCode\npar(\"mfrow\" = c(1,3))\nfor(i in c(10^-3, 10^-4, 10^-5))\n    tree.image(img, mindev=i) |> plot.image()\n\n\n\n\n\nLooks like CART can produce a sharper borders, the outline of is already quite visible with mindev=10^-3. But it doesn’t handle details well, unless we use a CART tree with a lot of tips.\nSo like before, lets look how this method works on terrain:\n\n\nCode\npar(\"mfrow\"=c(1,1))\ntree.image(terrain, mindev=10^-4) |> plot.image()\n\n\n\n\n\nCode\npar(\"mfrow\" = c(1, 3))\nfor(i in c(10^-3, 10^-4, 10^-5))\n    tree.image(terrain, mindev=i) |> plot.image()\n\n\n\n\n\n\n\nSummary\nYou can do some cool things with R in just a few lines if you apply basic machine-learning methods creatively. The results are not exactly what I wanted however, so I will try to look into this problem a bit more, such as implementing the CART algorithm for multiple response variables."
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html",
    "href": "posts/sql_nz_names/sql_nz_names.html",
    "title": "Learn SQL with me: NZ names",
    "section": "",
    "text": "In this blogpost, we will look at the most popular names in New Zealand and build a relational database out of them. We will go through database design, such as first, second and third normal form, consider how the database will be used, how the data will be queried, and how to try to de-duplicate any information.\nNote of warning: I am not an expert on database design or SQL, I am just learning and trying to put the theoretical knowledge of a few books into practice using an interesting project that is more than just a toy example."
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html#the-data",
    "href": "posts/sql_nz_names/sql_nz_names.html#the-data",
    "title": "Learn SQL with me: NZ names",
    "section": "The Data",
    "text": "The Data\nThe data we will be working with comes from the Department of Internal Affairs. It contains the 100 most popular male and female names from 1954 to 2022. The data is presented as a series of columns containing a tuple (name, count), with male and female names separated into their own tables.\n\n\n\n\n\n\n\n\n\n\n\n\nRank\n1954\n-\n1955\n-\n1956\n-\n\n\n\n\n-\nName\nCount\nName\nCount\nName\nCount\n\n\n1\nChristine\n779\nSusan\n743\nSusan\n851\n\n\n2\nSusan\n735\nChristine\n689\nChristine\n754\n\n\n3\nMargaret\n562\nMargaret\n559\nKaren\n615\n\n\n\nFor the following database design, there are some considerations:\n\nEach column contains 100 names, but that doesn’t mean that we are operating with only 100 names. Some names popular in 1960 might not be popular in 2000.\nThe data covers names over 69 years, but we would like to make adding new data (new years) easy.\nSome names are unisex and may appear in both girl and boy tables. For instance, Ashley used to be a popular unisex name in the 1980s.\n\nThe data is presented in what would one call the wide format. We could go even wider by including a sex column and merging the tables into a single wide table, moving each name into its row and removing rank, and so on.\nConsider a much simpler row-oriented long format:\n\n\n\nName\nYear\nCount\n\n\n\n\nChristine\n1954\n779\n\n\nSusan\n1954\n735\n\n\nMargaret\n1954\n562\n\n\nSusan\n1955\n743\n\n\nChristine\n1955\n689\n\n\nMargaret\n1955\n559\n\n\n\nWe could easily add an additional column for sex and represent the whole data with just 4 columns. Or 5, if we also include rank, but rank feels like a derived variable. We can imagine that new data, such as a newly discovered book from a now-defunct hospital could bring information about a few more births such that historical rank would change. We would have to recalculate the rank.\nIn data science, the long format is especially popular because it is easy to work with. And from my experience, it is much easier for software to grog a million rows rather than a million columns.\nBoth of these forms are flat file representations, something you would get in a CSV file (and loaded in an R or such)."
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html#the-design",
    "href": "posts/sql_nz_names/sql_nz_names.html#the-design",
    "title": "Learn SQL with me: NZ names",
    "section": "The Design",
    "text": "The Design\nWhen designing the SQL table, we want to consider a few issues:\n\nhow the data will be queried\ndesign a database to prevent duplication\ndesign a database to prevent adding invalid data\n\nInvalid data and duplication is prevented by normalizing the database schema, the famous first, second and third normal forms. Database in Depth from C. J. Date mentions fourth and fifth normal forms and more, but that book feels to me very theoretical, it will probably make more sense to me once I am a bit more familiar with the practice.\nOn the other hand, the practical usage of the database might sometimes run against normalization. Apparently, there is a lot of discussion about the need for normalization or, on the other hand, considering normalization as common sense.\nFirst of all, the wide format. If we had only a few columns, it would be incredibly easy to define a names table, to serve as a foreign key, and then a table for every year, such as year1954, year1955 and so on. Querying data and joining tables would be relatively easy. And since there is no relationship between years, they all would depend only on the names table, adding an additional year table would be relatively easy.\n\n\n\nExample of a database design using the wide format.\n\n\nHowever, most databases are optimized towards rows, not columns. Adding a new table is a relatively expensive operation compared to adding an additional row. This way, we would end with 69 tables for each year, with more tables every year. That is quite a lot of tables! Additionally, if we would like to know the most common name since 1954, we would have to query every single table.\nYet, this approach has some advantages. Typically, old tables are not modified and tables are populated only once. New tables are added only once a year as the data are obtained. The database structure would be simple, every name could occur in every year table only once, giving it a simple constraint.\nWhat I think would be a better and more traditional approach is to model this relationship as one-to-many, one name to many years. We would have a name table, years table, and a value table. The first two tables are simple tables containing names or years respectively.\n\n\n\nExample of a database design using the long format.\n\n\nAnd this is the database we will implement."
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html#the-implementation",
    "href": "posts/sql_nz_names/sql_nz_names.html#the-implementation",
    "title": "Learn SQL with me: NZ names",
    "section": "The Implementation",
    "text": "The Implementation\nThe primary key is a constraint that uniquely identifies every row of a table. Each table can have only a single primary key. This is an incredibly powerful tool to enforce the integrity of a table.\nSimilarly, the foreign key is a field that refers to a primary key of another table, this enforces referential integrity.\nA common practice is to create an ID that serves as a primary key, but we do not need that. We might need it if we allowed for varied spelling and treated all instances of that as a single unique name.\nCREATE TABLE Names (\n    name varchar(50),\n    PRIMARY KEY (name)\n);\n\nCREATE TABLE Years (\n    year int,\n    PRIMARY KEY (year)\n);\nThe _counts_ table is more interesting. We know that we can have any combination of a name and a year, but each combination can be in a table only once. We can enforce this by creating a primary key from a tuple of (name, year) while enforcing that both of these values depend on the Names and Years table through foreign keys.\nCREATE TABLE Counts (\n    name varchar(50) NOT NULL,\n    year int NOT NULL,\n    count int NOT NULL,\n    PRIMARY KEY (name, year),\n    FOREIGN KEY (name) REFERENCES Names(name),\n    FOREIGN KEY (year) REFERENCES Years(year)\n);\nNow we can populate tables:\nINSERT INTO Years(year) VALUES (1954);\n\nINSERT INTO Names(name) VALUES (\"Christine\");\nINSERT INTO Counts (name, year, count) VALUES (\"Christine\", 1954, 779);\n\nINSERT INTO Names(name) VALUES (\"Susan\");\nINSERT INTO Counts (name, year, count) VALUES (\"Susan\", 1954, 735);\n\nINSERT INTO Names(name) VALUES(\"Margaret\");\nINSERT INTO Counts (name, year, count) VALUES (\"Margaret\", 1954, 562);\nThe Names table is a bit extra but gives us referential integrity. It protects us against typos, gives us the option to include a potential Rank table, and would allow us to add alternative spelling, translation and other features. It gives us also a visible list of all names. All these advantages are, I think, worth a little more pain when populating tables.\nNow, populating tables manually is a pain, so here we do it in R using the DBI and RSQLite packages.\n\n\nClick to unroll\n\nFirst, we need to parse the file and get data for boys and girls:\nlibrary(\"openxlsx\")\nfile = \"Top-100-girls-and-boys-names-since-1954.xlsx\"\n\n\nget_name_table = function(file, sheet){\n    # The raw data has rather complex structure:\n    #  * 4 rows are noise/empty\n    #  * 2 rows of headers (year and name/number)\n    #  * 1 empty row\n    #  * 100 rows of data\n    #   * 3 rows are noise/empty\n    # Helper function\n    get_year_table = function(data, cols, year){\n        data.frame(\"Name\" = data[,cols[1]],\n                   \"Year\" = as.integer(year),\n                   \"Count\" = data[,cols[2]])\n        }\n\n    data = openxlsx::read.xlsx(\n        file, sheet=sheet,\n        skipEmptyCols=FALSE, skipEmptyRows=FALSE,\n        colNames=FALSE\n        )\n\n    # Automatic skipEmptyCols and skipEmptyRows does not work properly in this case\n    data = data[-(1:4),] # 4 rows are noise\n    data = head(data, n=-3) # last 3 rows of noise\n    data = data[,-(1:2)] # First two columns are noise/rank\n\n    # convert to numeric first to remove any NAs that would turn into \"NA\" string otherwise\n    years = as.numeric(data[1,]) |> na.omit() |> c() |> as.character()\n\n    # Now we can remove the 2 rows of header and one empty row\n    data = data[-(1:3),]\n\n    # First year is formed by two column (name, value)\n    res = list()\n    res[[years[1]]] = get_year_table(data, c(1,2), years[1])\n\n    # Every other year is formed by three column (empty, name, value)\n    for(i in seq(2, length(years))){\n        cols = 2:3+3*(i-2)+2\n        year = years[i]\n        res[[year]] = get_year_table(data, cols, year)\n        }\n\n    res = do.call(rbind, res)\n    rownames(res) = NULL\n\n    res\n    }\n\ngirls = get_name_table(file, 1)\nboys = get_name_table(file, 2)\nThen we create tables as shown before:\nlibrary(\"RSQLite\")\nlibrary(\"DBI\")\nlibrary(\"glue\") # for better multiline strings\ndatabase = dbConnect(RSQLite::SQLite(), \"nznames.db\")\n\n# Names and counts splitted according to gender\ndbExecute(database, \"PRAGMA foreign_keys = ON;\") |> invisible()\ndbExecute(database, glue(\"\n    CREATE TABLE GirlNames (\n        name varchar(50),\n        PRIMARY KEY (name)\n    );\")) |> invisible()\n\ndbExecute(database, glue(\"\n    CREATE TABLE BoyNames (\n        name varchar(50),\n        PRIMARY KEY (name)\n    );\")) |> invisible()\n\ndbExecute(database, glue(\"\n    CREATE TABLE Years (\n        year int,\n        PRIMARY KEY (year)\n    );\")) |> invisible()\n\ndbExecute(database, glue(\"\n    CREATE TABLE GirlCounts (\n        name varchar(50) NOT NULL,\n        year int NOT NULL,\n        count int NOT NULL,\n        PRIMARY KEY (name, year),\n        FOREIGN KEY (name) REFERENCES GirlNames(name),\n        FOREIGN KEY (year) REFERENCES Years(year)\n    );\")) |> invisible()\n\ndbExecute(database, glue(\"\n    CREATE TABLE BoyCounts (\n        name varchar(50) NOT NULL,\n        year int NOT NULL,\n        count int NOT NULL,\n        PRIMARY KEY (name, year),\n        FOREIGN KEY (name) REFERENCES BoyNames(name),\n        FOREIGN KEY (year) REFERENCES Years(year)\n    );\")) |> invisible()\nAnd now we can populate tables:\nyears = data.frame(\"year\"=union(girls$Year, boys$Year) |> sort())\ngirl_names = data.frame(\"name\" = girls$Name |> unique())\nboy_names = data.frame(\"name\" = boys$Name |> unique())\n\ndbAppendTable(database, \"Years\", years)\ndbAppendTable(database, \"GirlNames\", girl_names)\ndbAppendTable(database, \"BoyNames\", boy_names)\ndbAppendTable(database, \"GirlCounts\", girls)\ndbAppendTable(database, \"BoyCounts\", boys)\nInterestingly, the last statement fails because a primary key would be duplicated. How so? Turns out someone typed Michael twice!\nany(duplicated(boys[,1:2]))\n\nfind_duplicated = function(x, cols){\n    x[duplicated(x[,cols]) | duplicated(x[,cols], fromLast=TRUE),]\n    }\nfind_duplicated(boys, 1:2)\nIn 1989, 741 boys were named Michael. And somehow also 52 boys. This is surely wrong and given the popularity of Michael, the second occurence is surely wrong. Unfortunately, we don’t know what the true value should be. Maybe if they used properly designed database like we did, it would warn them about this typo!\nSo we clean the data and rerun the statement.\nboys = boys[-anyDuplicated(boys[,1:2]),]\ndbAppendTable(database, \"BoyCounts\", boys)\ndbDisconnect(database)\n\nNow that we have the database, we can construct queries!"
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html#the-querying",
    "href": "posts/sql_nz_names/sql_nz_names.html#the-querying",
    "title": "Learn SQL with me: NZ names",
    "section": "The Querying",
    "text": "The Querying\nSelect the most popular girl name since 2010:\nSELECT name, MAX(count),year\nFROM GirlCounts\nWHERE year >= 2010\nGROUP BY year;\n\n\nClick to unroll\n\nRuby|335|2011\nOlivia|312|2012\nCharlotte|303|2013\nCharlotte|255|2014\nOlivia|268|2015\nOlivia|266|2016\nCharlotte|277|2017\nCharlotte|233|2018\nAmelia|255|2019\nIsla|243|2020\nCharlotte|227|2021\nIsla|246|2022\n\nCharlotte seems to be quite popular, how popular was the name historically?\nSELECT count, year\nFROM GirlCounts\nWHERE name == \"Charlotte\";\n\n\nClick to unroll\n\n65|1974\n65|1975\n75|1976\n106|1977\n93|1978\n95|1979\n88|1980\n85|1981\n93|1982\n111|1983\n87|1984\n106|1985\n135|1986\n123|1987\n118|1988\n158|1989\n149|1990\n155|1991\n175|1992\n155|1993\n160|1994\n130|1995\n140|1996\n138|1997\n133|1998\n150|1999\n149|2000\n152|2001\n201|2002\n243|2003\n330|2004\n278|2005\n324|2006\n263|2007\n269|2008\n257|2009\n305|2010\n258|2011\n285|2012\n303|2013\n255|2014\n260|2015\n262|2016\n277|2017\n233|2018\n248|2019\n222|2020\n227|2021\n208|2022\n\nCharlotte started to be popular since 1974 and steadily increase till 2010, after which it begun to drop slightly.\nAnother interesting question would be to find names that are steadily popular over the 69 year period. This means names that occur multiple times in our table.\nSELECT name, count(name)\nFROM GirlCounts\nGROUP BY name\nORDER BY COUNT(name) DESC;\n\n\nClick to unroll\n\nElizabeth|69\nAnna|63\nSarah|62\nMaria|56\nEmma|54\nVictoria|50\nCharlotte|49\nJennifer|49\nEmily|48\nStephanie|47\nOlivia|46\nRebecca|46\nCatherine|45\nHannah|45\nAlice|44\nAmber|44\nMegan|44\nHayley|43\nJessica|43\nMichelle|43\nJasmine|42\nKate|42\nLucy|42\nNatalie|42\nRachel|42\nKatherine|41\nKathryn|41\nLouise|41\nAngela|40\nNicola|40\nNicole|39\nSophie|39\nAmy|38\nAmanda|37\nAmelia|37\nChristina|37\nChristine|37\nHelen|37\nHolly|37\nKatie|37\nMary|37\nSamantha|37\nGrace|36\nLeah|36\nLisa|36\nMelissa|36\nChloe|35\nJade|35\nNatasha|35\nZoe|35\nClaire|34\nJane|34\nLauren|34\nGeorgia|33\nJacqueline|33\nJoanne|33\nJulie|33\nKaren|33\nAndrea|32\nFiona|32\nTracey|32\nAbigail|31\nDeborah|31\nDonna|31\nRuby|31\nToni|31\nLaura|30\nPaige|30\nSusan|30\nAlexandra|29\nBrooke|29\nCaroline|29\nDanielle|29\nElla|29\nJulia|29\nKayla|29\nKelly|29\nMadison|29\nMelanie|29\nRenee|29\nTania|29\nVanessa|29\nAimee|28\nEden|28\nIsabella|28\nKim|28\nSharon|28\nTeresa|28\nWendy|28\nHeather|27\nMaddison|27\nPaula|27\nPhoebe|27\nRose|27\nSophia|27\nSuzanne|27\nAlison|26\nAshley|26\nKatrina|26\nLily|26\nMarie|26\nRachael|26\nSally|26\nShannon|26\nCaitlin|25\nLinda|25\nShelley|25\nSummer|25\nTracy|25\nBronwyn|24\nCarolyn|24\nGemma|24\nJoanna|24\nMaia|24\nMargaret|24\nMonique|24\nSandra|24\nStacey|24\nKirsty|23\nLeanne|23\nMolly|23\nPatricia|23\nPenelope|23\nVicki|23\nZara|23\nBridget|22\nMia|22\nRobyn|22\nStella|22\nTessa|22\nAnita|21\nBrenda|21\nCourtney|21\nErin|21\nEva|21\nIsabelle|21\nJanine|21\nKerry|21\nRochelle|21\nTaylor|21\nAshleigh|20\nAva|20\nBella|20\nChelsea|20\nDenise|20\nDiane|20\nKathleen|20\nMackenzie|20\nMaree|20\nMikayla|20\nPhilippa|20\nRebekah|20\nRuth|20\nAaliyah|19\nAnne|19\nAnnette|19\nDebra|19\nIsla|19\nKylie|19\nNikita|19\nTina|19\nCarol|18\nCasey|18\nLilly|18\nPauline|18\nPoppy|18\nRaewyn|18\nScarlett|18\nSienna|18\nTayla|18\nAlana|17\nBarbara|17\nCheryl|17\nJudith|17\nKimberley|17\nLynda|17\nLynette|17\nPamela|17\nTanya|17\nAnn|16\nAria|16\nBrooklyn|16\nDebbie|16\nJamie|16\nJessie|16\nMadeleine|16\nMillie|16\nRiley|16\nSara|16\nSheryl|16\nAlyssa|15\nAyla|15\nDianne|15\nFaith|15\nFrances|15\nHazel|15\nJan|15\nKirsten|15\nPiper|15\nSofia|15\nYvonne|15\nBelinda|14\nCherie|14\nCrystal|14\nGabrielle|14\nGeorgina|14\nGillian|14\nShona|14\nAddison|13\nJanet|13\nJillian|13\nLayla|13\nMatilda|13\nMaya|13\nMila|13\nOlive|13\nRosemary|13\nViolet|13\nWillow|13\nAlexis|12\nAngel|12\nColleen|12\nDiana|12\nEllie|12\nEvelyn|12\nIvy|12\nJenna|12\nJordan|12\nKay|12\nKeira|12\nLesley|12\nLorraine|12\nLynne|12\nPeyton|12\nAlicia|11\nAnnabelle|11\nAriana|11\nBrianna|11\nBrittany|11\nClaudia|11\nGail|11\nHarper|11\nIndie|11\nIsabel|11\nJanice|11\nJeanette|11\nKrystal|11\nMaureen|11\nMichele|11\nMorgan|11\nNevaeh|11\nNina|11\nShirley|11\nAurora|10\nEvie|10\nFlorence|10\nHeidi|10\nJustine|10\nKelsey|10\nVivienne|10\nAbby|9\nAshlee|9\nBeverley|9\nCaitlyn|9\nDaisy|9\nFreya|9\nHarriet|9\nImogen|9\nLola|9\nPippa|9\nSavannah|9\nThea|9\nAdrienne|8\nCassandra|8\nCharlie|8\nEleanor|8\nEmilia|8\nFrankie|8\nGlenda|8\nJosephine|8\nMichaela|8\nQuinn|8\nSadie|8\nSonya|8\nTara|8\nTiana|8\nZoey|8\nElsie|7\nJocelyn|7\nKaitlyn|7\nLydia|7\nNaomi|7\nParis|7\nTrinity|7\nBillie|6\nCheyenne|6\nHarmony|6\nJill|6\nJodie|6\nJorja|6\nJoy|6\nKaryn|6\nKatelyn|6\nLuna|6\nMya|6\nTegan|6\nAmaia|5\nCarla|5\nElaine|5\nGina|5\nGlenys|5\nJolene|5\nKiara|5\nLucia|5\nTamara|5\nValerie|5\nAda|4\nDorothy|4\nEsther|4\nHarlow|4\nJemma|4\nLois|4\nMadeline|4\nMaisie|4\nManaia|4\nMarilyn|4\nMarion|4\nSonia|4\nAnahera|3\nBailey|3\nCarmen|3\nDestiny|3\nEliza|3\nHaley|3\nHope|3\nJoan|3\nKaia|3\nKaye|3\nKristy|3\nLynley|3\nMaeve|3\nMargot|3\nRosie|3\nShania|3\nSharlene|3\nSheree|3\nVicky|3\nAleisha|2\nAmara|2\nAthena|2\nBonnie|2\nCora|2\nDelilah|2\nEllen|2\nEloise|2\nGabriella|2\nGloria|2\nIrene|2\nIsobel|2\nKhloe|2\nKiri|2\nKora|2\nLynnette|2\nMarley|2\nNadine|2\nNgaire|2\nRhonda|2\nSkye|2\nSkyla|2\nTyla|2\nAbbey|1\nAngelina|1\nAnika|1\nApril|1\nArabella|1\nAroha|1\nBriana|1\nBritney|1\nChantelle|1\nCindy|1\nClara|1\nClare|1\nCleo|1\nDawn|1\nEilish|1\nEliana|1\nElise|1\nGaylene|1\nIndi|1\nIris|1\nJaime|1\nJanette|1\nJean|1\nJodi|1\nKaitlin|1\nKarla|1\nKellie|1\nKyla|1\nLeonie|1\nLibby|1\nLynn|1\nMacKenzie|1\nMarian|1\nMikaela|1\nMonica|1\nNiamh|1\nNikki|1\nPayton|1\nRachelle|1\nSasha|1\nShakira|1\nSinead|1\nSkylar|1\nTori|1\nVirginia|1\nWhitney|1\n\nOnly a single name, Elizabeth, was steadily popular over the 69 year period, occuring among the top 100 names every single year.\nFinally, I said that we don’t need rank table, because we can calculate the ranks from counts. We can do that using advanced SQL commands as well!\nHere we look at 5 most popular names since 2020\nSELECT name, year, count_rank, count\nFROM\n    (SELECT\n        name,\n        year,\n        count,\n        RANK() OVER (PARTITION BY year ORDER BY count DESC) count_rank\n    FROM GirlCounts\n    )\nWHERE count_rank <= 5 AND YEAR >= 2020;\n\n\nClick to unroll\n\nIsla|2020|1|243\nCharlotte|2020|2|222\nAmelia|2020|3|213\nOlivia|2020|4|208\nWillow|2020|5|184\nCharlotte|2021|1|227\nIsla|2021|2|214\nAmelia|2021|3|206\nOlivia|2021|4|185\nAva|2021|5|184\nIsla|2022|1|246\nAmelia|2022|2|210\nCharlotte|2022|3|208\nMila|2022|4|182\nLily|2022|5|180"
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html#the-books",
    "href": "posts/sql_nz_names/sql_nz_names.html#the-books",
    "title": "Learn SQL with me: NZ names",
    "section": "The Books",
    "text": "The Books\nFinally, if you would like to learn more about SQL as I did, here is a small review of books.\nThe best book for beginners is The Manga Guide to Databases. It is the most comprehensive introduction to databases, database design, and SQL. Every other book I have read assumes you already know about databases or SQL, so they often skip over important details.\nNext, I would suggest SQL Antipatterns. It introduces you to some patterns and anti-patterns in SQL and database design. Easy to read, comprehensive and quite practical if you want to get into designing databases.\nThe Seven Databases in Seven Weeks would be next in line. Unlike the previous ones, it doesn’t teach you about database design or SQL tricks. Instead, it is an overview of different database types and products, not just relational databases and not just SQL. It is not an in-depth introduction to SQL or relational databases, so do not make this the first book you read, instead, this book will introduce you to a multitude of other solutions.\nFinally, there is a gold standard Database in Depth. It already assumes that you know quite a lot about databases, and doesn’t talk much about SQL, instead, it introduces language D, the perfect relational language, and talks more about the theory. Unlike other theory books I have read on other topics, this one seems to be quite fluffy. I have read 30 pages and I think that information could be condensed into 10. But then, you don’t need to stare into a single equation with no accompanying explanation for 2 hours to understand it, like in those very dense theory books, because there is a 10-page explanation. So fluffiness might not be disadvantageous."
  },
  {
    "objectID": "posts/sql_nz_names/sql_nz_names.html#the-summary",
    "href": "posts/sql_nz_names/sql_nz_names.html#the-summary",
    "title": "Learn SQL with me: NZ names",
    "section": "The Summary",
    "text": "The Summary\nWe have picked a dataset and gone over some ideas and thoughts on how to convert it into a database. We then implemented the database using SQLite, populated it through R and queried a bunch of interesting data. The database we designed was not the most exciting, since the structure of the dataset was quite simple. Yet, I have learned quite a lot. I hope you enjoyed this journey as I did. I will try to explore the dataset a bit more because I think there is a story hidden inside."
  },
  {
    "objectID": "posts/baseR_AZ/1_introduction.html",
    "href": "posts/baseR_AZ/1_introduction.html",
    "title": "Base R from A to Z: Introduction (1)",
    "section": "",
    "text": "Introduction\nR has quite a rich standard library1, not just to process text, read and work with files, do parallel computing, but also a whole load of statistical functions, including simple neural networks, additive models, survival analysis, and three whole packages for plotting and graphics (graphics for base graphics, grid, and lattice). You can read about all functions in the R’s standard library in the R reference manual.\nThe disadvantage is that while R has a very large standard library, it doesn’t use namespaces. Don’t get me wrong, R has namespaces. Every package has a namespace. And R comes with a bunch of preinstalled packages. But while some specialized statistical functions are in aptly named packages (survival for survival analysis), often a group of unrelated functions are grouped into a single namespace. This presents a significant barrier when it comes to the discoverability of these functions. For instance, consider Python. If you want to work with paths, you know that the functions will be in the os.path module or in the pathlib module, if you want to go for an object-oriented way of handling paths. Additionally, Python’s documentation groups functions according to their usage. Compare this to the R’s reference manual linked above. So if you want to, for instance, know all the functions for text parsing, well good luck. You will have to do a lot of contextualized searches through R’s help system or find a book that does this for you. But while there are books about some popular user-created packages, for instance, the R markdown book, or books that serve as an in-depth exploration of R, such as Advanced R or R packages, there is no in-depth exploration of functions available in base R. This often means that a lot of people will keep reinventing wheels or coming up with complicated solutions requiring one or even multiple packages when there is a more performant one-liner in base R available.\nThe purpose of this series is to explore functions in base R and later perhaps to create a book that will serve as additional documentation of functionalities available in base R.\n\n\nPreinstalled packages\nR comes with a number of preinstalled packages, they are labelled as base and recommended:\n\npackages = installed.packages() |>\n    as.data.frame() |>\n    subset(select=c(\"Package\", \"Priority\")) |>\n    unique() |>\n    split(formula(\"~Priority\"))\nsapply(packages, nrow)\n\n       base recommended \n         14          15 \n\n\nThe 14 base packages are usually the main workhorse of R, while the 15 recommended packages are predominantly of statistical nature. Do not expect the latest, fastest and most feature-full implementation of them, but it also means that if you need something like k-nearest-neighbours, you will find it in the R standard library, which is something that is not typically true for most programming languages.\nBecause of this, the packages have quite a variable number of objects. Here we don’t count only functions, but also included example datasets or pre-set variables. For example T is an alias for TRUE, but you can easily change it such as by setting T=FALSE. So please don’t use T in your scripts, interactive usage only.\nBefore we show the number of objects in each package, we define some helper functions, that way the code will be more readable.\n\n# get object names from a package  \ngetObjects = function(name, all=TRUE){\n    getNamespace(name) |> ls(all.names=all)\n    }\n\n# count the number of objects in a package\ncountObjects = function(name, all=TRUE){\n    getObjects(name, all=all) |> length()\n    }\n\n# apply the countObjects on the `packages` objects defined above\nn_objects = sapply(packages, function(x){\n    x[,1] |> sapply(countObjects) |> sort(decreasing=TRUE)\n    })\nn_objects\n\n$base\n     base     stats      grid   methods     tools     utils     tcltk  compiler \n     1370      1134       872       761       733       529       313       289 \ngrDevices  graphics  parallel   splines    stats4  datasets \n      254       170       145        50        30         3 \n\n$recommended\n    Matrix       nlme       mgcv   survival    lattice       MASS    cluster \n       996        607        495        421        289        232         91 \n codetools       boot    spatial      rpart    foreign       nnet      class \n        89         84         55         51         42         41         29 \nKernSmooth \n        27 \n\n\nFor the packages labelled as base, the base package leads with 1370 objects. The majority of common operations are implemented in this package, from file-system operations, and text-parsing functions, but also mathematical functions like min, mean or set operations. The second largest package is stats, which implements a large number of mathematical and statistical functions. The bread and butter statistical functions like t.test, anova, lm or a general-purpose optimization algorithm optim or functions to work with time-series data are all included in this package. The third largest package is the grid package with 872 objects. grid is one of the alternatives to base graphics, which are implemented in the graphics package. Surprisingly, the graphics package has only 170 objects. Outside of base, stats, tools and utils, the packages starts to be more specialized and also smaller. The packages utils and especially tools are already supposed to be specialized for making packages, but since making packages requires a lot of tooling, and this tooling often has quite a lot of utility, you might occasionally use functions from tools. utils on the other hand has kind of everything, from functions to make packages, download packages, spellchecker, but also functions like help, head, read.table. Classical if you don’t know where to put it, put it into utils.\nIf we look at the packages labelled as recommended, the largest package is the Matrix package, which is the third overall package by size out of preinstalled packages. The Matrix package implements several kinds of sparse matrices and operations on them. This is very important in linear algebra and statistics, as the solution of many statistical models often relies on eigenvalues.\n\nExported functions\nYou might have noticed that I have shown objects using ls(). This way, all objects, whether they are exported or not, are counted. After all, I don’t remember reading about 145 functions in the parallel documentation, so something is off.\nIf we look only at exported objects, the situation would be like this:\n\nn_exported_objects = sapply(packages, function(x){\n    x[,1] |> sapply(\\(y){\n        getNamespaceExports(y) |> length()\n        }) |> sort(decreasing=TRUE)\n    })\nn_exported_objects\n\n$base\n     base     stats   methods     tcltk     utils      grid grDevices     tools \n     1370       456       371       269       221       212       119       119 \n graphics  parallel    stats4   splines  compiler  datasets \n       88        33        28        13         9         0 \n\n$recommended\n    Matrix       mgcv    lattice       nlme       MASS   survival       boot \n       367        182        144        109         78         77         36 \n   spatial    cluster  codetools    foreign      class      rpart KernSmooth \n        25         24         19         17         15         14          7 \n      nnet \n         7 \n\n\nWe can see that the number of objects changes drastically, although the base package remained unaltered. Interestingly, the package datasets doesn’t have a single exported object. This is because datasets consist only of datasets, which are loaded lazily. That is until you use the dataset, it does not occupy the computer memory.\nNow, let’s move to the core of the series, the base package.\n\n\n\nBase R\nWe have explored the number of objects across preinstalled packages, but how many functions are in the base package? The core of base R?\n\n# You can see a similar code in ?Filter examples\nfuns = Filter(is.function, sapply(\n    ls(baseenv(), all.names=TRUE), get, baseenv()\n    ))\nlength(funs)\n\n[1] 1325\n\n\nQuite a lot, 1325 functions. However, a great deal of them are S3 methods. For instance, there are 36 methods just for the S3 generic print.\n\ngrep(\"print.\", baseenv() |> ls(all.names=TRUE),\n    value=TRUE, fixed=TRUE) |> length()\n\n[1] 36\n\n\nThese methods are very important to seamlessly work with different S3 classes. After all, no one wants to call print.foo(x) and print.bar(y), when we can just type print(z). This reduces typing for sure, but also decreases mental overload. In fact, many R users do not know about S3 system, but the dispatch of various methods still work like magic. But enough about S3. For the purpose of this exercise, these are not very interesting. Maybe in the future, we will explore what kind of S3 classes are defined in base R.\nTo filter the S3 methods, we can use isS3method. The only issue is that this function fails for any object starting with a dot. Bummer. Objects starting with a dot are typically considered hidden objects, which should not be used unless you know what you are doing (such as .C or .Call, which are important tools when writing packages).\n\n# this would fail:\n# isS3method(\".C\")\n\nvisible = grep(\"^\\\\.\", names(funs), value=TRUE, invert=TRUE)\n\nnormal = Filter(Negate(isS3method), visible)\nnormal |> length()\n\n[1] 865\n\n\nThis means that to explore all the functions in base package, we need to go through 865 functions.\nThere is a small caveat we have talked about before. ls() does not distinguish whether the functions are or are not exported. Luckily for us, we saw that all the functions in the base package are exported. But just to be sure:\n\n# helper, take a string instead of a function object\nis_function = function(name, env=baseenv()){\n    f = get(name, envir=env)\n    is.function(f)\n    }\n\nnormal = getNamespaceExports(\"base\") |>\n    Filter(f=is_function) |>\n    grep(pattern=\"^\\\\.\", value=TRUE, invert=TRUE) |>\n    Filter(f=Negate(isS3method)) |> sort()\nnormal |> length()\n\n[1] 865\n\n\nUsing slightly different calls, we have arrived at the same number. Great!\n\n\nWhat can you expect next?\nIn the following part of the series, we will start going through the functions alphabetically. Although when we will find a group of similar or related functions, we will describe them together, such as sub and gsub.\nWe will start with the special symbols or operators, make a small segway about different types of functions in R, like .Primitive, .Internal, and we will have to talk a bit more about the generics.\nI hope that you have found this small exploration of the preinstalled packages interesting and that you are as excited as me about continuing in this series.\n\n\nList of functions in the base package\nWe will end with a list of all functions that we will go through in this series:\n\n\nClick to unroll\n\n\n\n  [1] \"-\"                              \":\"                             \n  [3] \"::\"                             \":::\"                           \n  [5] \"!\"                              \"!=\"                            \n  [7] \"(\"                              \"[\"                             \n  [9] \"[[\"                             \"[[<-\"                          \n [11] \"[<-\"                            \"{\"                             \n [13] \"@\"                              \"@<-\"                           \n [15] \"*\"                              \"/\"                             \n [17] \"&\"                              \"&&\"                            \n [19] \"%*%\"                            \"%/%\"                           \n [21] \"%%\"                             \"%in%\"                          \n [23] \"%o%\"                            \"%x%\"                           \n [25] \"^\"                              \"+\"                             \n [27] \"<\"                              \"<-\"                            \n [29] \"<<-\"                            \"<=\"                            \n [31] \"=\"                              \"==\"                            \n [33] \">\"                              \">=\"                            \n [35] \"|\"                              \"||\"                            \n [37] \"~\"                              \"$\"                             \n [39] \"$<-\"                            \"abbreviate\"                    \n [41] \"abs\"                            \"acos\"                          \n [43] \"acosh\"                          \"activeBindingFunction\"         \n [45] \"addNA\"                          \"addTaskCallback\"               \n [47] \"agrep\"                          \"agrepl\"                        \n [49] \"alist\"                          \"all\"                           \n [51] \"all.equal\"                      \"all.names\"                     \n [53] \"all.vars\"                       \"allowInterrupts\"               \n [55] \"any\"                            \"anyDuplicated\"                 \n [57] \"anyNA\"                          \"aperm\"                         \n [59] \"append\"                         \"apply\"                         \n [61] \"Arg\"                            \"args\"                          \n [63] \"array\"                          \"arrayInd\"                      \n [65] \"as.array\"                       \"as.call\"                       \n [67] \"as.character\"                   \"as.complex\"                    \n [69] \"as.data.frame\"                  \"as.Date\"                       \n [71] \"as.difftime\"                    \"as.double\"                     \n [73] \"as.environment\"                 \"as.expression\"                 \n [75] \"as.factor\"                      \"as.function\"                   \n [77] \"as.hexmode\"                     \"as.integer\"                    \n [79] \"as.list\"                        \"as.logical\"                    \n [81] \"as.matrix\"                      \"as.name\"                       \n [83] \"as.null\"                        \"as.numeric\"                    \n [85] \"as.numeric_version\"             \"as.octmode\"                    \n [87] \"as.ordered\"                     \"as.package_version\"            \n [89] \"as.pairlist\"                    \"as.POSIXct\"                    \n [91] \"as.POSIXlt\"                     \"as.qr\"                         \n [93] \"as.raw\"                         \"as.single\"                     \n [95] \"as.symbol\"                      \"as.table\"                      \n [97] \"as.vector\"                      \"asin\"                          \n [99] \"asinh\"                          \"asNamespace\"                   \n[101] \"asplit\"                         \"asS3\"                          \n[103] \"asS4\"                           \"assign\"                        \n[105] \"atan\"                           \"atan2\"                         \n[107] \"atanh\"                          \"attach\"                        \n[109] \"attachNamespace\"                \"attr\"                          \n[111] \"attr.all.equal\"                 \"attr<-\"                        \n[113] \"attributes\"                     \"attributes<-\"                  \n[115] \"autoload\"                       \"autoloader\"                    \n[117] \"backsolve\"                      \"baseenv\"                       \n[119] \"basename\"                       \"besselI\"                       \n[121] \"besselJ\"                        \"besselK\"                       \n[123] \"besselY\"                        \"beta\"                          \n[125] \"bindingIsActive\"                \"bindingIsLocked\"               \n[127] \"bindtextdomain\"                 \"bitwAnd\"                       \n[129] \"bitwNot\"                        \"bitwOr\"                        \n[131] \"bitwShiftL\"                     \"bitwShiftR\"                    \n[133] \"bitwXor\"                        \"body\"                          \n[135] \"body<-\"                         \"bquote\"                        \n[137] \"break\"                          \"browser\"                       \n[139] \"browserCondition\"               \"browserSetDebug\"               \n[141] \"browserText\"                    \"builtins\"                      \n[143] \"by\"                             \"bzfile\"                        \n[145] \"c\"                              \"call\"                          \n[147] \"callCC\"                         \"capabilities\"                  \n[149] \"casefold\"                       \"cat\"                           \n[151] \"cbind\"                          \"ceiling\"                       \n[153] \"char.expand\"                    \"character\"                     \n[155] \"charmatch\"                      \"charToRaw\"                     \n[157] \"chartr\"                         \"check_tzones\"                  \n[159] \"chkDots\"                        \"chol\"                          \n[161] \"chol2inv\"                       \"choose\"                        \n[163] \"class\"                          \"class<-\"                       \n[165] \"clearPushBack\"                  \"close\"                         \n[167] \"closeAllConnections\"            \"col\"                           \n[169] \"colMeans\"                       \"colnames\"                      \n[171] \"colnames<-\"                     \"colSums\"                       \n[173] \"commandArgs\"                    \"comment\"                       \n[175] \"comment<-\"                      \"complex\"                       \n[177] \"computeRestarts\"                \"conditionCall\"                 \n[179] \"conditionMessage\"               \"conflictRules\"                 \n[181] \"conflicts\"                      \"Conj\"                          \n[183] \"contributors\"                   \"cos\"                           \n[185] \"cosh\"                           \"cospi\"                         \n[187] \"crossprod\"                      \"Cstack_info\"                   \n[189] \"cummax\"                         \"cummin\"                        \n[191] \"cumprod\"                        \"cumsum\"                        \n[193] \"curlGetHeaders\"                 \"cut\"                           \n[195] \"data.class\"                     \"data.frame\"                    \n[197] \"data.matrix\"                    \"date\"                          \n[199] \"debug\"                          \"debuggingState\"                \n[201] \"debugonce\"                      \"default.stringsAsFactors\"      \n[203] \"delayedAssign\"                  \"deparse\"                       \n[205] \"deparse1\"                       \"det\"                           \n[207] \"detach\"                         \"determinant\"                   \n[209] \"dget\"                           \"diag\"                          \n[211] \"diag<-\"                         \"diff\"                          \n[213] \"difftime\"                       \"digamma\"                       \n[215] \"dim\"                            \"dim<-\"                         \n[217] \"dimnames\"                       \"dimnames<-\"                    \n[219] \"dir\"                            \"dir.create\"                    \n[221] \"dir.exists\"                     \"dirname\"                       \n[223] \"do.call\"                        \"dontCheck\"                     \n[225] \"double\"                         \"dput\"                          \n[227] \"dQuote\"                         \"drop\"                          \n[229] \"droplevels\"                     \"dump\"                          \n[231] \"duplicated\"                     \"dyn.load\"                      \n[233] \"dyn.unload\"                     \"dynGet\"                        \n[235] \"eapply\"                         \"eigen\"                         \n[237] \"emptyenv\"                       \"enc2native\"                    \n[239] \"enc2utf8\"                       \"encodeString\"                  \n[241] \"Encoding\"                       \"Encoding<-\"                    \n[243] \"endsWith\"                       \"enquote\"                       \n[245] \"env.profile\"                    \"environment\"                   \n[247] \"environment<-\"                  \"environmentIsLocked\"           \n[249] \"environmentName\"                \"errorCondition\"                \n[251] \"eval\"                           \"eval.parent\"                   \n[253] \"evalq\"                          \"exists\"                        \n[255] \"exp\"                            \"expand.grid\"                   \n[257] \"expm1\"                          \"expression\"                    \n[259] \"extSoftVersion\"                 \"factor\"                        \n[261] \"factorial\"                      \"fifo\"                          \n[263] \"file\"                           \"file.access\"                   \n[265] \"file.append\"                    \"file.choose\"                   \n[267] \"file.copy\"                      \"file.create\"                   \n[269] \"file.exists\"                    \"file.info\"                     \n[271] \"file.link\"                      \"file.mode\"                     \n[273] \"file.mtime\"                     \"file.path\"                     \n[275] \"file.remove\"                    \"file.rename\"                   \n[277] \"file.show\"                      \"file.size\"                     \n[279] \"file.symlink\"                   \"Filter\"                        \n[281] \"Find\"                           \"find.package\"                  \n[283] \"findInterval\"                   \"findPackageEnv\"                \n[285] \"findRestart\"                    \"floor\"                         \n[287] \"flush\"                          \"for\"                           \n[289] \"force\"                          \"forceAndCall\"                  \n[291] \"formals\"                        \"formals<-\"                     \n[293] \"format\"                         \"format.info\"                   \n[295] \"format.pval\"                    \"formatC\"                       \n[297] \"formatDL\"                       \"forwardsolve\"                  \n[299] \"function\"                       \"gamma\"                         \n[301] \"gc\"                             \"gc.time\"                       \n[303] \"gcinfo\"                         \"gctorture\"                     \n[305] \"gctorture2\"                     \"get\"                           \n[307] \"get0\"                           \"getAllConnections\"             \n[309] \"getCallingDLL\"                  \"getCallingDLLe\"                \n[311] \"getConnection\"                  \"getDLLRegisteredRoutines\"      \n[313] \"getElement\"                     \"geterrmessage\"                 \n[315] \"getExportedValue\"               \"getHook\"                       \n[317] \"getLoadedDLLs\"                  \"getNamespace\"                  \n[319] \"getNamespaceExports\"            \"getNamespaceImports\"           \n[321] \"getNamespaceInfo\"               \"getNamespaceName\"              \n[323] \"getNamespaceUsers\"              \"getNamespaceVersion\"           \n[325] \"getNativeSymbolInfo\"            \"getOption\"                     \n[327] \"getRversion\"                    \"getSrcLines\"                   \n[329] \"getTaskCallbackNames\"           \"gettext\"                       \n[331] \"gettextf\"                       \"getwd\"                         \n[333] \"gl\"                             \"globalCallingHandlers\"         \n[335] \"globalenv\"                      \"gregexec\"                      \n[337] \"gregexpr\"                       \"grep\"                          \n[339] \"grepl\"                          \"grepRaw\"                       \n[341] \"grouping\"                       \"gsub\"                          \n[343] \"gzcon\"                          \"gzfile\"                        \n[345] \"I\"                              \"iconv\"                         \n[347] \"iconvlist\"                      \"icuGetCollate\"                 \n[349] \"icuSetCollate\"                  \"identical\"                     \n[351] \"identity\"                       \"if\"                            \n[353] \"ifelse\"                         \"Im\"                            \n[355] \"importIntoEnv\"                  \"infoRDS\"                       \n[357] \"inherits\"                       \"integer\"                       \n[359] \"interaction\"                    \"interactive\"                   \n[361] \"intersect\"                      \"intToBits\"                     \n[363] \"intToUtf8\"                      \"inverse.rle\"                   \n[365] \"invisible\"                      \"invokeRestart\"                 \n[367] \"invokeRestartInteractively\"     \"is.array\"                      \n[369] \"is.atomic\"                      \"is.call\"                       \n[371] \"is.character\"                   \"is.complex\"                    \n[373] \"is.data.frame\"                  \"is.double\"                     \n[375] \"is.element\"                     \"is.environment\"                \n[377] \"is.expression\"                  \"is.factor\"                     \n[379] \"is.finite\"                      \"is.function\"                   \n[381] \"is.infinite\"                    \"is.integer\"                    \n[383] \"is.language\"                    \"is.list\"                       \n[385] \"is.loaded\"                      \"is.logical\"                    \n[387] \"is.matrix\"                      \"is.na\"                         \n[389] \"is.na<-\"                        \"is.name\"                       \n[391] \"is.nan\"                         \"is.null\"                       \n[393] \"is.numeric\"                     \"is.numeric_version\"            \n[395] \"is.object\"                      \"is.ordered\"                    \n[397] \"is.package_version\"             \"is.pairlist\"                   \n[399] \"is.primitive\"                   \"is.qr\"                         \n[401] \"is.R\"                           \"is.raw\"                        \n[403] \"is.recursive\"                   \"is.single\"                     \n[405] \"is.symbol\"                      \"is.table\"                      \n[407] \"is.unsorted\"                    \"is.vector\"                     \n[409] \"isa\"                            \"isatty\"                        \n[411] \"isBaseNamespace\"                \"isdebugged\"                    \n[413] \"isFALSE\"                        \"isIncomplete\"                  \n[415] \"isNamespace\"                    \"isNamespaceLoaded\"             \n[417] \"ISOdate\"                        \"ISOdatetime\"                   \n[419] \"isOpen\"                         \"isRestart\"                     \n[421] \"isS4\"                           \"isSeekable\"                    \n[423] \"isSymmetric\"                    \"isTRUE\"                        \n[425] \"jitter\"                         \"julian\"                        \n[427] \"kappa\"                          \"kronecker\"                     \n[429] \"l10n_info\"                      \"La_library\"                    \n[431] \"La_version\"                     \"La.svd\"                        \n[433] \"labels\"                         \"lapply\"                        \n[435] \"lazyLoad\"                       \"lazyLoadDBexec\"                \n[437] \"lazyLoadDBfetch\"                \"lbeta\"                         \n[439] \"lchoose\"                        \"length\"                        \n[441] \"length<-\"                       \"lengths\"                       \n[443] \"levels\"                         \"levels<-\"                      \n[445] \"lfactorial\"                     \"lgamma\"                        \n[447] \"libcurlVersion\"                 \"library\"                       \n[449] \"library.dynam\"                  \"library.dynam.unload\"          \n[451] \"licence\"                        \"license\"                       \n[453] \"list\"                           \"list.dirs\"                     \n[455] \"list.files\"                     \"list2DF\"                       \n[457] \"list2env\"                       \"load\"                          \n[459] \"loadedNamespaces\"               \"loadingNamespaceInfo\"          \n[461] \"loadNamespace\"                  \"local\"                         \n[463] \"lockBinding\"                    \"lockEnvironment\"               \n[465] \"log\"                            \"log10\"                         \n[467] \"log1p\"                          \"log2\"                          \n[469] \"logb\"                           \"logical\"                       \n[471] \"lower.tri\"                      \"ls\"                            \n[473] \"make.names\"                     \"make.unique\"                   \n[475] \"makeActiveBinding\"              \"Map\"                           \n[477] \"mapply\"                         \"margin.table\"                  \n[479] \"marginSums\"                     \"mat.or.vec\"                    \n[481] \"match\"                          \"match.arg\"                     \n[483] \"match.call\"                     \"match.fun\"                     \n[485] \"matrix\"                         \"max\"                           \n[487] \"max.col\"                        \"mean\"                          \n[489] \"mem.maxNSize\"                   \"mem.maxVSize\"                  \n[491] \"memCompress\"                    \"memDecompress\"                 \n[493] \"memory.profile\"                 \"merge\"                         \n[495] \"message\"                        \"mget\"                          \n[497] \"min\"                            \"missing\"                       \n[499] \"Mod\"                            \"mode\"                          \n[501] \"mode<-\"                         \"months\"                        \n[503] \"mostattributes<-\"               \"names\"                         \n[505] \"names<-\"                        \"namespaceExport\"               \n[507] \"namespaceImport\"                \"namespaceImportClasses\"        \n[509] \"namespaceImportFrom\"            \"namespaceImportMethods\"        \n[511] \"nargs\"                          \"nchar\"                         \n[513] \"ncol\"                           \"NCOL\"                          \n[515] \"Negate\"                         \"new.env\"                       \n[517] \"next\"                           \"NextMethod\"                    \n[519] \"ngettext\"                       \"nlevels\"                       \n[521] \"noquote\"                        \"norm\"                          \n[523] \"normalizePath\"                  \"nrow\"                          \n[525] \"NROW\"                           \"nullfile\"                      \n[527] \"numeric\"                        \"numeric_version\"               \n[529] \"numToBits\"                      \"numToInts\"                     \n[531] \"nzchar\"                         \"objects\"                       \n[533] \"oldClass\"                       \"oldClass<-\"                    \n[535] \"OlsonNames\"                     \"on.exit\"                       \n[537] \"open\"                           \"options\"                       \n[539] \"order\"                          \"ordered\"                       \n[541] \"outer\"                          \"package_version\"               \n[543] \"packageEvent\"                   \"packageHasNamespace\"           \n[545] \"packageNotFoundError\"           \"packageStartupMessage\"         \n[547] \"packBits\"                       \"pairlist\"                      \n[549] \"parent.env\"                     \"parent.env<-\"                  \n[551] \"parent.frame\"                   \"parse\"                         \n[553] \"parseNamespaceFile\"             \"paste\"                         \n[555] \"paste0\"                         \"path.expand\"                   \n[557] \"path.package\"                   \"pcre_config\"                   \n[559] \"pipe\"                           \"plot\"                          \n[561] \"pmatch\"                         \"pmax\"                          \n[563] \"pmax.int\"                       \"pmin\"                          \n[565] \"pmin.int\"                       \"polyroot\"                      \n[567] \"pos.to.env\"                     \"Position\"                      \n[569] \"pretty\"                         \"prettyNum\"                     \n[571] \"print\"                          \"prmatrix\"                      \n[573] \"proc.time\"                      \"prod\"                          \n[575] \"prop.table\"                     \"proportions\"                   \n[577] \"provideDimnames\"                \"psigamma\"                      \n[579] \"pushBack\"                       \"pushBackLength\"                \n[581] \"q\"                              \"qr\"                            \n[583] \"qr.coef\"                        \"qr.fitted\"                     \n[585] \"qr.Q\"                           \"qr.qty\"                        \n[587] \"qr.qy\"                          \"qr.R\"                          \n[589] \"qr.resid\"                       \"qr.solve\"                      \n[591] \"qr.X\"                           \"quarters\"                      \n[593] \"quit\"                           \"quote\"                         \n[595] \"R_system_version\"               \"R.home\"                        \n[597] \"R.Version\"                      \"range\"                         \n[599] \"rank\"                           \"rapply\"                        \n[601] \"raw\"                            \"rawConnection\"                 \n[603] \"rawConnectionValue\"             \"rawShift\"                      \n[605] \"rawToBits\"                      \"rawToChar\"                     \n[607] \"rbind\"                          \"rcond\"                         \n[609] \"Re\"                             \"read.dcf\"                      \n[611] \"readBin\"                        \"readChar\"                      \n[613] \"readline\"                       \"readLines\"                     \n[615] \"readRDS\"                        \"readRenviron\"                  \n[617] \"Recall\"                         \"Reduce\"                        \n[619] \"reg.finalizer\"                  \"regexec\"                       \n[621] \"regexpr\"                        \"registerS3method\"              \n[623] \"registerS3methods\"              \"regmatches\"                    \n[625] \"regmatches<-\"                   \"remove\"                        \n[627] \"removeTaskCallback\"             \"rep\"                           \n[629] \"rep_len\"                        \"rep.int\"                       \n[631] \"repeat\"                         \"replace\"                       \n[633] \"replicate\"                      \"require\"                       \n[635] \"requireNamespace\"               \"restartDescription\"            \n[637] \"restartFormals\"                 \"retracemem\"                    \n[639] \"return\"                         \"returnValue\"                   \n[641] \"rev\"                            \"rle\"                           \n[643] \"rm\"                             \"RNGkind\"                       \n[645] \"RNGversion\"                     \"round\"                         \n[647] \"row\"                            \"row.names\"                     \n[649] \"row.names<-\"                    \"rowMeans\"                      \n[651] \"rownames\"                       \"rownames<-\"                    \n[653] \"rowsum\"                         \"rowSums\"                       \n[655] \"sample\"                         \"sample.int\"                    \n[657] \"sapply\"                         \"save\"                          \n[659] \"save.image\"                     \"saveRDS\"                       \n[661] \"scale\"                          \"scan\"                          \n[663] \"search\"                         \"searchpaths\"                   \n[665] \"seek\"                           \"seq\"                           \n[667] \"seq_along\"                      \"seq_len\"                       \n[669] \"seq.int\"                        \"sequence\"                      \n[671] \"serialize\"                      \"serverSocket\"                  \n[673] \"set.seed\"                       \"setdiff\"                       \n[675] \"setequal\"                       \"setHook\"                       \n[677] \"setNamespaceInfo\"               \"setSessionTimeLimit\"           \n[679] \"setTimeLimit\"                   \"setwd\"                         \n[681] \"showConnections\"                \"shQuote\"                       \n[683] \"sign\"                           \"signalCondition\"               \n[685] \"signif\"                         \"simpleCondition\"               \n[687] \"simpleError\"                    \"simpleMessage\"                 \n[689] \"simpleWarning\"                  \"simplify2array\"                \n[691] \"sin\"                            \"single\"                        \n[693] \"sinh\"                           \"sink\"                          \n[695] \"sink.number\"                    \"sinpi\"                         \n[697] \"slice.index\"                    \"socketAccept\"                  \n[699] \"socketConnection\"               \"socketSelect\"                  \n[701] \"socketTimeout\"                  \"solve\"                         \n[703] \"sort\"                           \"sort.int\"                      \n[705] \"sort.list\"                      \"source\"                        \n[707] \"split\"                          \"split<-\"                       \n[709] \"sprintf\"                        \"sqrt\"                          \n[711] \"sQuote\"                         \"srcfile\"                       \n[713] \"srcfilealias\"                   \"srcfilecopy\"                   \n[715] \"srcref\"                         \"standardGeneric\"               \n[717] \"startsWith\"                     \"stderr\"                        \n[719] \"stdin\"                          \"stdout\"                        \n[721] \"stop\"                           \"stopifnot\"                     \n[723] \"storage.mode\"                   \"storage.mode<-\"                \n[725] \"str2expression\"                 \"str2lang\"                      \n[727] \"strftime\"                       \"strptime\"                      \n[729] \"strrep\"                         \"strsplit\"                      \n[731] \"strtoi\"                         \"strtrim\"                       \n[733] \"structure\"                      \"strwrap\"                       \n[735] \"sub\"                            \"subset\"                        \n[737] \"substitute\"                     \"substr\"                        \n[739] \"substr<-\"                       \"substring\"                     \n[741] \"substring<-\"                    \"sum\"                           \n[743] \"summary\"                        \"suppressMessages\"              \n[745] \"suppressPackageStartupMessages\" \"suppressWarnings\"              \n[747] \"suspendInterrupts\"              \"svd\"                           \n[749] \"sweep\"                          \"switch\"                        \n[751] \"sys.call\"                       \"sys.calls\"                     \n[753] \"Sys.chmod\"                      \"Sys.Date\"                      \n[755] \"sys.frame\"                      \"sys.frames\"                    \n[757] \"sys.function\"                   \"Sys.getenv\"                    \n[759] \"Sys.getlocale\"                  \"Sys.getpid\"                    \n[761] \"Sys.glob\"                       \"Sys.info\"                      \n[763] \"sys.load.image\"                 \"Sys.localeconv\"                \n[765] \"sys.nframe\"                     \"sys.on.exit\"                   \n[767] \"sys.parent\"                     \"sys.parents\"                   \n[769] \"Sys.readlink\"                   \"sys.save.image\"                \n[771] \"Sys.setenv\"                     \"Sys.setFileTime\"               \n[773] \"Sys.setlocale\"                  \"Sys.sleep\"                     \n[775] \"sys.source\"                     \"sys.status\"                    \n[777] \"Sys.time\"                       \"Sys.timezone\"                  \n[779] \"Sys.umask\"                      \"Sys.unsetenv\"                  \n[781] \"Sys.which\"                      \"system\"                        \n[783] \"system.file\"                    \"system.time\"                   \n[785] \"system2\"                        \"t\"                             \n[787] \"table\"                          \"tabulate\"                      \n[789] \"tan\"                            \"tanh\"                          \n[791] \"tanpi\"                          \"tapply\"                        \n[793] \"taskCallbackManager\"            \"tcrossprod\"                    \n[795] \"tempdir\"                        \"tempfile\"                      \n[797] \"textConnection\"                 \"textConnectionValue\"           \n[799] \"tolower\"                        \"topenv\"                        \n[801] \"toString\"                       \"toupper\"                       \n[803] \"trace\"                          \"traceback\"                     \n[805] \"tracemem\"                       \"tracingState\"                  \n[807] \"transform\"                      \"trigamma\"                      \n[809] \"trimws\"                         \"trunc\"                         \n[811] \"truncate\"                       \"try\"                           \n[813] \"tryCatch\"                       \"tryInvokeRestart\"              \n[815] \"typeof\"                         \"unclass\"                       \n[817] \"undebug\"                        \"union\"                         \n[819] \"unique\"                         \"units\"                         \n[821] \"units<-\"                        \"unix.time\"                     \n[823] \"unlink\"                         \"unlist\"                        \n[825] \"unloadNamespace\"                \"unlockBinding\"                 \n[827] \"unname\"                         \"unserialize\"                   \n[829] \"unsplit\"                        \"untrace\"                       \n[831] \"untracemem\"                     \"unz\"                           \n[833] \"upper.tri\"                      \"url\"                           \n[835] \"UseMethod\"                      \"utf8ToInt\"                     \n[837] \"validEnc\"                       \"validUTF8\"                     \n[839] \"vapply\"                         \"vector\"                        \n[841] \"Vectorize\"                      \"warning\"                       \n[843] \"warningCondition\"               \"warnings\"                      \n[845] \"weekdays\"                       \"which\"                         \n[847] \"which.max\"                      \"which.min\"                     \n[849] \"while\"                          \"with\"                          \n[851] \"withAutoprint\"                  \"withCallingHandlers\"           \n[853] \"within\"                         \"withRestarts\"                  \n[855] \"withVisible\"                    \"write\"                         \n[857] \"write.dcf\"                      \"writeBin\"                      \n[859] \"writeChar\"                      \"writeLines\"                    \n[861] \"xor\"                            \"xpdrows.data.frame\"            \n[863] \"xtfrm\"                          \"xzfile\"                        \n[865] \"zapsmall\"                      \n\n\n\n\n\n\n\n\nFootnotes\n\n\nFor the definition of a standard library, see: https://en.wikipedia.org/wiki/Standard_library↩︎"
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html",
    "href": "posts/baseR_AZ/3_operators.html",
    "title": "Base R from A to Z: Operators (3)",
    "section": "",
    "text": "In the third part of the series about base R, we will talk about operators. As we have mentioned in the previous part, operators are construct similar to functions, but with special syntax or semantics. While you can call them as a functions, and there are instances where you want to do that, normally you call them in a special way called infix notation.\n\n\n\n\n\n\nInfix, prefix and postfix\n\n\n\nMost of us are used to the classical infix notation, such as 3 + 5. However, in some older programming languages, there is also a prefix notation + 3 5 and postfix notation 3 5 +. These can be efficiently parsed using a stack, and do not require operator precedence.\nTechnically, R contains a traces of prefix notation in the form \"+\"(3,5) (or in fact, any functional call), but unlike in the typical prefix or postfix languages, the parentheses are required. Additionally, ! is unary, and - and + have unary form, so they can be considered prefix, such as -8 or !a.\nFor more information, see infix, prefix and postfix on Wikipedia.\n\n\nR offers a multitude of operators and a way to define new operators through the %any% notation. Operators can be divided into these groups:\n\nArithmetic operators\n+, -, *, /, ^, %%, %/%\nLogical operators\n!, &, &&, |, ||\nRelational operatos\n<, >, <=, >=, ==, !=\nSubsetting operators\n[, [[, @, $\nAssignment operators\n<-, <<-, =, [<-, [[<-, @<-, $<-\nMatrix operators\n%*%, %o%, %x%\nSpecial operator\n%anything%\nMatching operator\n%in%\nSequence operator\n:\nOperators in formula: ~, :, %in%\nNamespace access: ::, :::\nParentheses and braces\n(, {\n\nWe will talk about Arithmetic, Logical, Relational, Subsetting, Assigment and Matrix operators, as well as the special operator %anything%. Matching operator is just a shorthand for the match() function, so we will leave it for later. The sequence operator : is a primitive for performance (and parsing) reasons, but we will talk about it together with other sequence-generating functions. The :: and ::: are quite complex, so we will talk about them when we go deeper into environments, namespaces and package access, and the ( and { are language features rather than operators or functions, so we skip them for later. Finally, the ? operator is not part of the base, but is in utils."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#operator-precedence",
    "href": "posts/baseR_AZ/3_operators.html#operator-precedence",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Operator precedence",
    "text": "Operator precedence\nFrom basic mathematics, we are used to the idea that multiplication precedes addition, so 3 + 5 * 2 is interpreted as 3 + (5 * 2) without having to use parentheses. These rules are called operator precedence. Here I am reproducing the list from the ?Syntax help command.\nFor listed operators, precedence goes from highest (evaluated first) to lowest. Operators with the same precedence are evaluated in the order they are encountered.\n\n::, :::\n$, @\n[, [[\n^\n-, + in their unary form (-3, +5)\n:\n%any%, |> (base R pipe)\n*, /\n+, - in their binary form (3+5)\n<, >, <=, ==, !=\n!\n&, &&\n|, ||\n~\n->, ->>\n<-, <<-\n=\n?\n\nNow for some interesting consequences. With ^ having such high precedence, -a^2 is interpeted as -(a^2) instead of (-a)^2, but then -1:3 is interpeted as (-1):3 and not -(1:3).\nThe pipe |> has high precedence, which mean you can’t do a + 2 |> ..., because it is interpeted as a + (2 |> ...). This bites me all the time, when I just want to do some simple division or addition inside pipes.\nFinally = has lower precedence than <-. This shouldn’t be an issue since you should use either = or <- as your assignment operator.\nThe takeway is that you should not rely on common sense your knowledge of operator precedence rules, but use parentheses to make operations as much explicit as possible."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#arithmetic-operators",
    "href": "posts/baseR_AZ/3_operators.html#arithmetic-operators",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Arithmetic operators",
    "text": "Arithmetic operators\nArithmetic operators are addition +, subtraction -, multiplication *, division /, exponentiation ^, modulo %% and integer division %/%.\nThe addition, subtraction, multiplication, division, and exponentiation are well known. Module and the integer division are less common, but occasionaly useful in programming. Modulo %% is the remainder after integer division, so that 5 %% 2 = 1. The %/% is the integer division 5 %/% 2 = 2. These are used when we want to know how many times something fit in our number, if we need to do something with a certain frequency, or if you want to know if a number is even. But arguably, some of these are displaced by sequence functions, and if we want to fit k elements into categories of size m, ceiling(k/m) gives us exactly that. So personally, I haven’t used modulo or integer division very often.\nThe + and - operators can be both unary and binary, meaning they accept one or two parameters. The meaning and behaviour, such as operator precedence, are different for the unary and binary version.\n\n+ a # unary `+`\n- a # unary `-`\n\na + b # binary `+`\na - b # binary `-`\n\nThis is related to how numbers are parsed by the interpreter (see literals. Details are not important, just remember that the unary and binary operators are different, and that the unary - makes numbers negative. Fun fact, as a consequence of the parsing rules, this is valid R code:\n\n+ - + - 5 + - + - + + - - + - 1\n\n[1] 4\n\n\nAs with most fun facts, you should not ever write a code like this.\n\n\n\n\n\n\nRecycling\n\n\n\nA lot of basic operations in R do something called recycling. For instance, when you add together two vectors with a different number of elements, the shorter vector is being extended by reusing (recycling) its elements.\nFor instance, consider multiplying a vector of length 3 with a vector of length 1: c(1, 2, 3) * 3. This is identical to c(1,2,3) * c(3, 3, 3) because the shorter vector is being recycled. This work well when the length of the longer vector is a multiple of the shorter vector (i.e., longer %% shorter = 0), for instance:\n\nc(1, 2, 3, 4, 5, 6) + c(1, 2)\n\n[1] 2 4 4 6 6 8\n\n\nYou can see that the shorter vector was recycled as c(1,2, 1,2, 1,2), because the length of the longer vector is a multiple of the shorter one. If this is not the case, the shorter vector is still recycled, but a warning is thrown.\n\nc(1,2,3) * c(1,2)\n\nWarning in c(1, 2, 3) * c(1, 2): longer object length is not a multiple of\nshorter object length\n\n\n[1] 1 4 3\n\n\nRecycling allows you to do some fancy tricks, for example if you want to pick every second element of a vector:\n\nc(1,2,3,4,5,6)[c(FALSE, TRUE)]\n\n[1] 2 4 6\n\n\nThis works because the selection vector is being recycled to the full length using the predefined pattern.\n\n\n\nThe strange case of **\nIn some languages such as Python, ** is a power operator. This is also supported in R:\n\n2 ^ 3\n\n[1] 8\n\n2 ** 3\n\n[1] 8\n\n\nStrangely, ** is not documented, ** is not a primitive, and when you type bare ** in R, you will get a curious error:\n\n**\n\nError: <text>:1:1: unexpected '^'\n1: **\n    ^\n\n\nThere is a small note in the ?Arithmetic. Apparently, ** existed in S (the precedesors of R) but was deprecated. For backward compatibility, the R parser kept this functionality and translates ** into ^. Since it is undocumented and not part of the official language specification, do not use **."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#logical-operators-and-functions",
    "href": "posts/baseR_AZ/3_operators.html#logical-operators-and-functions",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Logical operators and functions",
    "text": "Logical operators and functions\nSince we are already talking about the logical operators, I thought it might be more useful to look more closely at the logical type and functions that operate with it.\nLogical operators are negation !, which is a unary operator, and binary logical and & and &&, logical or | and ||. Aside of these operators, there is also a function xor(), helpers isTRUE(), isFALSE(), useful primitives all() and any(), and functions for the creation, testing, and conversion to the logical type: logical(), is.logical(), and as.logical().\n\nLogical operators\nLogical operators are negation !, logical and &, &&, and logical or |, ||.\nThe negation ! operator just makes a FALSE value TRUE and the other way around. The and and or operators are more interesting in the way they work on vectors.\nThe single symbol and and or operators & and | work like + or -, they are applied element-wise and vectors are recycled as required.\n\nc(TRUE, TRUE, FALSE) & c(TRUE, FALSE, FALSE)\n\n[1]  TRUE FALSE FALSE\n\nc(TRUE, TRUE, FALSE) | c(TRUE, FALSE, FALSE)\n\n[1]  TRUE  TRUE FALSE\n\n\nThe double-symbol and and or are not vectorized, they uses only the first element of a vector, while other elements are ignored. Likewise, the output of && and || is a single TRUE or FALSE value. This makes && and || useful when doing control flow operations with if(condition){...}.\n\nc(TRUE, TRUE) && c(TRUE, FALSE) # only the first value is used\n\n[1] TRUE\n\n\n\nShort-circuting\nIn addition, the && and || short-circuits, the second expression is evaluated only if it is required. For instance, in the expression:\na || b\nIf a evaluates to TRUE, the whole expression evaluates to TRUE regardless of b. This means that if a = TRUE, b doesn’t have to be evaluated and in fact isn’t evaluated. So if the b was a function call with some side effects, the side effects (reading file, printing, increasing a counter) are never evaluated. For instance, consider an expression that prints foo and returns TRUE:\n{print(\"foo\"); TRUE}\nIf we use two of these expression with ||, only single foo will be printed:\n\n{print(\"foo\"); TRUE} || {print(\"foo\"); TRUE}\n\n[1] \"foo\"\n\n\n[1] TRUE\n\n\nContrast this with the & or | which do not short-curcuit:\n\n{print(\"foo\"); TRUE} | {print(\"foo\"); TRUE}\n\n[1] \"foo\"\n[1] \"foo\"\n\n\n[1] TRUE\n\n\nIn some languages, this is used as a control structure, since this behaviour can be transformed into:\nif a then a, else b\nBut I haven’t seen this being used in R. Since the R code is typically light on side-effects, I don’t think there is a much value in trying to be cheeky in this way.\n\n\nNA behaviour\nNA or not available is a peculiar value (or values, since there is NA_character_, NA_integer_ atp.) that symbolize a missing value. Most programing languages do not have this (usually, they only have NULL), but R was designed as a domain-specific language for data-analysis and unknown values are a common problem.\nTypically, any numerical calculation involving NA typically results in NA.\n\n3 + NA\n\n[1] NA\n\n5 * NA\n\n[1] NA\n\n\nLogical operations are slightly different and result in NA only if the expression depends on it. You can intepret this dependence in a similar manner as short-circuting, but this works even for the non-short-circuiting operators | and ||.\nFor example, in the x | NA, if x is TRUE, the whole expression is also TRUE. But if x is FALSE, the whole expression depends on the unknown value NA, which means that the expression evaluates to NA.\n\nTRUE | NA\n\n[1] TRUE\n\nFALSE | NA\n\n[1] NA\n\n\nSimilarly, in the x & NA, if x is FALSE, the whole expression is FALSE. But if x is TRUE, the result of the expression depends on the NA and the expression evaluates to NA.\n\nFALSE & NA\n\n[1] FALSE\n\nTRUE & NA\n\n[1] NA\n\n\nIn the same manner, the !NA depends on the NA so it evaluates to NA and so on.\n\n\n\nLogical functions\nNow, lets move to the functions. The isTRUE() and isFALSE() are also useful shorthands for control flow. They detect if the condition is exactly TRUE or FALSE respectively, a good way to avoid pitalls with NA, NULL, which might arise from some operations and should be processed appropriatelly.\nThe logical operations xor() is elementwise exclusive or. It is not an operator, and not a primitive, likely because it is not used very often. You can see that xor() is just a shorthand for (x | y) & !(x & y)\n\nxor\n\nfunction (x, y) \n{\n    (x | y) & !(x & y)\n}\n<bytecode: 0x55a87771d0f8>\n<environment: namespace:base>\n\n\nVery useful if only x or y are allowed, such as if you are using parameters as a mutually exclusive flags. Consider a function that does either foo or bar, but can’t do both at the same time:\nmyfunc = function(x, doFoo=FALSE, doBar=FALSE){\n    if(!xor(doFoo, doBar))\n        stop(\"Must select either doFoo or doBar\")\n\n    if(doFoo)\n        return(foo(x))\n    if(doBar)\n        return(bar(x))\n    }\nWithout xor, the function might not behave as expected, not returning anything if either both conditions are FALSE, or performing only one operation if both conditions are TRUE.\nYou can turn xor() easily into an infix operator %xor%.\n\n`%xor%` = xor\nTRUE %xor% FALSE\n\n[1] TRUE\n\n\nBefore we start talking about the logical type in general, lets quickly mention the all() and any(). These are vectorized and will tell you if all or any value of the vector are TRUE respectively. For efficiency reason, they are primitives, because they are used quite often for the control flow.\n\nvec = c(TRUE, TRUE, FALSE)\nall(vec) # will be FALSE because not all values are TRUE\n\n[1] FALSE\n\nany(vec) # TRUE as at least one value is TRUE\n\n[1] TRUE\n\n\nYou can define a vectorized variant of xor which tells you that there is exactly one TRUE rather easily:\n\none = function(x){\n    sum(x, na.rm=TRUE) == 1\n    }   \none(c(TRUE, TRUE, FALSE)) # will be FALSE\n\n[1] FALSE\n\none(c(FALSE, TRUE, FALSE)) # will be TRUE\n\n[1] TRUE\n\n\n\n\nConversion rules\nThis brings us to an important step, conversion. You can apply logical operators not only on the logical TRUE and FALSE values, but as with many similar operations in R, the values are automatically converted to the correct type.\nConversion is performed internally using the as.logical primitive. Valid conversions are from numeric (that is, both integer and double), complex, and character. For the numeric and complex, 0 is converted into FALSE and non-zero value is converted into TRUE:\n\nas.logical(0)\n\n[1] FALSE\n\nas.logical(5i+3)\n\n[1] TRUE\n\n\nFor the character type, the conversion is a bit more complex. The strings “T”, “TRUE”, “True” and “true” are converted into TRUE, and similarly “F”, “FALSE”, “False” and “false” are converted into FALSE. Everything else, including “TrUe” and similar messy capitalizations, is converted into NA.\n\nas.logical(c(\"T\", \"TRUE\", \"True\", \"true\"))\n\n[1] TRUE TRUE TRUE TRUE\n\nas.logical(c(\"F\", \"FALSE\", \"False\", \"false\"))\n\n[1] FALSE FALSE FALSE FALSE\n\nas.logical(c(\"1\", \"0\", \"truE\", \"foo\"))\n\n[1] NA NA NA NA\n\n\nThe logical(n) is a shorthand for vector(\"logical\", n) and simply create a logical vector of length n with values initialized to FALSE.\n\nlogical(5)\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\nWhen converting from logical to numeric, the TRUE and FALSE converts to 1 and 0 respectively. This allows for the convenient use of sum or mean to count the number or the percentage of matches. When converting from logical to character, the TRUE and FALSE gets converted simply to \"TRUE\" and \"FALSE\" strings.\n\n\nRaw vectors\nThe logical operators !, & and | have another meaning for raw vectors. raw is a basic type, alongside integer, double, list, and so on. It represents raw bytes (from 0 to 255), here printed in a hexadecimal format:\n\nas.raw(c(0, 10, 255))\n\n[1] 00 0a ff\n\n\nFor these raw vectors, the logical operators have a slightly different bitwise meaning. See bitwNot, botwAnd, and bitwOr for !, &, and | respectively. We will talk in-depth about raw type and operations on them later in this series."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#relational-operators",
    "href": "posts/baseR_AZ/3_operators.html#relational-operators",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Relational operators",
    "text": "Relational operators\nRelational operatos are smaller than >, larger than >, smaller or equal <=, _larger or equal >=, equal ==, and not equal !=. For numeric (integer and double) and logical (which is converted to numeric), the comparisons are what you might expect, just a standard numerical comparisons.\n\n5L > 2L # integers, the `long` type\n\n[1] TRUE\n\n5.3 > 2.8\n\n[1] TRUE\n\nTRUE > FALSE\n\n[1] TRUE\n\n\nFor raw, the numeric order is used so the comparisons also behaves as expected, and for complex, only the == and != comparisons are implemented.\n\nas.raw(5) > as.raw(2)\n\n[1] TRUE\n\n5+1i > 2+1i\n\nError in 5 + (0+1i) > 2 + (0+1i): invalid comparison with complex values\n\n\n\nString comparisons\nFor character strings, the comparisons are quite a bit more complicated. If you consider only standard english alphabet, the problem might feel obvious. But every language has slightly different rules about order of character in their alphabet. Quoting from the R help page:\n\nBeware of making any assumptions about the collation order: e.g. in Estonian ‘Z’ comes between ‘S’ and ‘T’, and collation is not necessarily character-by-character - in Danish ‘aa’ sorts as a single letter, after ‘z’. In Welsh ‘ng’ may or may not be a single sorting unit: if it is it follows ‘g’.\n\nOn top of this, the order is system and locale dependent. This makes sorting extremely unpredictable when comparing strings accross languages.\nFor more information, see Microsoft page or long unicode explanation.\nI am happy that smarter people solved it and I don’t need to know the details.\n\n\nFloating point comparisons\nThere is a caveat when comparing doubles. You might remember that computers work in a binary. This means that every number is represented by combination of bits that can be 0 or 1. You might start sensing that something is off in here. How can I represent a rational number by just combination of 0 and 1? This is because computers are able to represent perfectly only whole numbers, integer. Anything else is represented imperfectly and you get rounding errors. R is trying to hide this imperfection by rounding when printing, but the abstraction will leak if you try to compare for equality.\n\n(1 - 0.9)\n\n[1] 0.1\n\n(1 - 0.9) == 0.1\n\n[1] FALSE\n\n\nThe > and < still works as intended, but when you are comparing floating point numbers, use all.equal instead and select an appropriate precision (by default sqrt(.Machine$double.eps) is used):\n\nall.equal( (1 - 0.9), 0.1)\n\n[1] TRUE\n\n\nUnlike with string comparison, this is common issue that will significantly influence the performance of your code, especially if you are writing any kind of numerical algorithm. You need to be aware of these issues. See shorter explanation and longer explanation and try to understand them."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#group-generic-methods-and-ops",
    "href": "posts/baseR_AZ/3_operators.html#group-generic-methods-and-ops",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Group generic methods and Ops",
    "text": "Group generic methods and Ops\nSome methods are grouped into groups called group generics functions. These methods are dispatched when any of the grouped functions are called. While it might look strange at first look, this allows for some neat tricks to efficiently define common mathematical operations for a large group of functions.\nFor instance, consider Ops.POSIXt which defines arithmetic, logical and relational operators for the S3 class POSIXt.\n\nOps.POSIXt\n\nfunction (e1, e2) \n{\n    if (nargs() == 1L) \n        stop(gettextf(\"unary '%s' not defined for \\\"POSIXt\\\" objects\", \n            .Generic), domain = NA)\n    boolean <- switch(.Generic, `<` = , `>` = , `==` = , `!=` = , \n        `<=` = , `>=` = TRUE, FALSE)\n    if (!boolean) \n        stop(gettextf(\"'%s' not defined for \\\"POSIXt\\\" objects\", \n            .Generic), domain = NA)\n    if (inherits(e1, \"POSIXlt\") || is.character(e1)) \n        e1 <- as.POSIXct(e1)\n    if (inherits(e2, \"POSIXlt\") || is.character(e2)) \n        e2 <- as.POSIXct(e2)\n    check_tzones(e1, e2)\n    NextMethod(.Generic)\n}\n<bytecode: 0x55a877002078>\n<environment: namespace:base>\n\n\nFirst, the function checks if only a single argument was passed, because unary operators are not defined.\nThen it uses switch statement, which evaluates to TRUE if one of <, >, ==, !=, <=, and >= matches the .Generic variable, which is an automatic variable initialized to the dispatched generics from the Ops group. So if the function call was !POSIXt, the .Generic is the negation operator !. The switch statement thus works as a check whether a particular operator makes sense for the class, throwing error if the operation was not defined.\nThis doesn’t mean that operators not specified in the switch are undefined. In fact, both +.POSIXt and -.POSIXt exists.\n\n\n\n\n\n\nPOSIXt, POSIXlt and POSIXct\n\n\n\nR contains a number of different classes for dates.\nPOSIXct is a signed number of seconds since the begining of 1970 (UCT time zone), the Unix time.\nPOSIXlt is a named list of vectors with sec, min, hour, mday and so on.\nPOSIXct is a really simple and convenient way to keep time, but not very human readable (unless you are Unix guru). POSIXlt is a nice human-readable format, but not very convenient when you want to store precise times in a data.frame\nPOSIXt is a virtual class, both POSIXct and POSIXlt inherits from it. This means that when you define a method for POSIXt, it will automatically work for POSIXct and POSIXlt.\n\n\nSince POSIXt is a virtual class, and math with a number of seconds POSIXct is simpler than math on a complex data structure POSIXlt (see the box), what follows is a conversion into POSIXct, and the call to NextMethod(), which simply calls the appropriate method for the .Generic on the modified parameters. There is nothing simple on NextMethod() and we will talk about it later, when we are talking in-depth about the S3 system.\n\nDouble dispatch\nMethods in the Ops group are special because most of them are binary and they need to do something called double dispatch. For instance, if you are adding objects of class foo and bar, you need to dispatch the correct method for this particular addition, taking in account classes of both objects, not just foo or bar independently. To keep addition comutative, method needs should be identical even if the order of addition is changed (bar + foo).\nS3 has rather primitive double-dispatch system, the rules are as follows:\n\nIf neither objects has a method, use internal method.\nIf only one object has a method, use that method.\nIf both objects have a method, and the methods:\n\nare identical, use that method.\nare not identical, throw a warning and use internal methods.\n\n\nTo demonstrate this, we define a helper function and methods for the + generic for classes foo and bar. These simply return NULL and print a message, which will tell us which method was dispatched.\n\n# helper to make object less wordy\nobj = function(class){\n    structure(1, class=class)\n    }\n`+.foo` = function(...){message(\"foo\")}\n`+.bar` = function(...){message(\"bar\")}\n\nThe first rule is just a normal addition:\n\n1 + 1\n\n[1] 2\n\n\nAccording to the second rule, the +.foo method is dispatched:\n\n1 + obj(\"foo\")\n\nfoo\n\n\nNULL\n\nobj(\"foo\") + 1\n\nfoo\n\n\nNULL\n\nobj(\"foo\") + obj(\"baz\") # no method +.baz\n\nfoo\n\n\nNULL\n\n\nAccording to the third rule, since methods are not identical, we will get a warning.\n\nobj(\"foo\") + obj(\"bar\")\n\nWarning: Incompatible methods (\"+.foo\", \"+.bar\") for \"+\"\n\n\n[1] 2\nattr(,\"class\")\n[1] \"foo\"\n\nobj(\"bar\") + obj(\"foo\")\n\nWarning: Incompatible methods (\"+.bar\", \"+.foo\") for \"+\"\n\n\n[1] 2\nattr(,\"class\")\n[1] \"bar\"\n\n\nFinally, if we redefine the +.foo and the +.bar to be identical, addition works again:\n\n`+.foo` = function(...){message(\"foobar\")}\n`+.bar` = function(...){message(\"foobar\")}\nobj(\"foo\") + obj(\"bar\")\n\nfoobar\n\n\nNULL\n\n\nNotice that if one of the object doesn’t have defined an S3 method, we can make the S3 method that is dispatched quite complex and built-in support for different types. But this mechanism will be fragile and once someone defines method for one of the supported types, our method would be ignored and the dispatch would default to an internal method (with warning). For an explicit double dispatch system that can be further extended, we would need to go for S4 classes."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#example-sparse_vector",
    "href": "posts/baseR_AZ/3_operators.html#example-sparse_vector",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Example: sparse_vector",
    "text": "Example: sparse_vector\nAs an example, we will create an S3 class sparse_vector. There is an S4 class in the Matrix package, which is a more reasonable choice since S4 methods can be extended, but we do this only for demonstration purposes.\nFirst, we will define our class, sparse vector is defined by the vector of values x, vector of indices i and the length of the vector. Normally, we would make a barebone new_sparse_vector constructor, and the sparse_vector one would be an interface that would also check validity of input parameters, but we will keep it simple.\nWe also create a method for the as.vector generics, so that we can easily unroll our sparse vector into a normal one.\n\nsparse_vector = function(x, i, length){\n    structure(list(\n        x = x,\n        i = i,\n        length = length\n        ), class = \"sparse_vector\")\n    }\n\nas.vector.sparse_vector = function(x, ...){\n    y = vector(mode=mode(x$x), length=x$length)\n    y[x$i] = x$x\n    y\n    }\n\nprint.sparse_vector = function(x, ...){\n    y = rep(\".\", x$length)\n    y[x$i] = as.character(x$x)\n    print(y, quote=FALSE)\n    }\n\na = sparse_vector(c(3,5,8), c(2,4,6), 10)\na\n\n [1] . 3 . 5 . 8 . . . .\n\nas.vector(a)\n\n [1] 0 3 0 5 0 8 0 0 0 0\n\n\nWe want to support all operations in the group generics Ops.\n\nOps.sparse_vector = function(e1, e2){\n    if(inherits(e1, \"sparse_vector\"))\n        e1 = as.vector(e1)\n    # If Ops is unary, e2 is missing\n    if(!missing(e2) && inherits(e2, \"sparse_vector\"))\n        e2 = as.vector(e2)\n    NextMethod(.Generic)\n    }\n\na + 2\n\n [1]  2  5  2  7  2 10  2  2  2  2\n\n2 + a\n\n [1]  2  5  2  7  2 10  2  2  2  2\n\na * 2\n\n [1]  0  6  0 10  0 16  0  0  0  0\n\na == 0\n\n [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE\n\n- a\n\n [1]  0 -3  0 -5  0 -8  0  0  0  0\n\n\nAnd essentially for free, we suddenly support all operations. But this isn’t very efficient if we are summing two sparse vectors, for instance. So for some operations, we define their own functions.\n\n`+.sparse_vector` = function(e1, e2){\n    message(\"+.sparse_vector dispatched\")\n    if(nargs() == 1L)\n        return(e1)\n\n    if(inherits(e1, \"sparse_vector\") && inherits(e2, \"sparse_vector\")){\n        i = union(e1$i, e2$i) |> sort()\n        j = intersect(e1$i, e2$i) |> sort()\n        l = max(e1$length, e2$length) # ignoring recycling for now\n        y = sparse_vector(rep(0, length(i)), i, l)\n        y$x[i %in% e1$i] = e1$x\n        y$x[i %in% e2$i] = e2$x\n        y$x[i %in% j] = e1$x[e1$i %in% j] + e2$x[e2$i %in% j]\n        return(y)\n        }\n    if(inherits(e1, \"sparse_vector\")){\n        e2[e1$i] = e2[e1$i] + e1$x\n        return(e2)\n        }\n    if(inherits(e2, \"sparse_vector\")){\n        e1[e2$i] = e1[e2$i] + e2$x\n        return(e1)\n        }\n    stop(\"This should be unreachable state\")\n    }\n\nb = sparse_vector(c(1, 2, 3), c(1, 4, 6), 10)\n+ a\n\n+.sparse_vector dispatched\n\n\n [1] . 3 . 5 . 8 . . . .\n\n+ b\n\n+.sparse_vector dispatched\n\n\n [1] 1 . . 2 . 3 . . . .\n\na + b\n\n+.sparse_vector dispatched\n\n\n [1] 1  3  .  7  .  11 .  .  .  . \n\na + rep(1, 10)\n\n+.sparse_vector dispatched\n\n\n [1] 1 4 1 6 1 9 1 1 1 1\n\nb + rep(1, 10)\n\n+.sparse_vector dispatched\n\n\n [1] 2 1 1 3 1 4 1 1 1 1\n\n\nThe code isn’t perfect, we ignore a lot of recycling, which means operations with matrices will not work correctly, and so on. But as an example of the Ops and operator overloading, this should be sufficient. One obvious improvement would be redefining the operator [ to make subsetting of the vector much easier, and thus the code cleaner. We will look at this in the next article."
  },
  {
    "objectID": "posts/baseR_AZ/3_operators.html#summary",
    "href": "posts/baseR_AZ/3_operators.html#summary",
    "title": "Base R from A to Z: Operators (3)",
    "section": "Summary",
    "text": "Summary\nIn this part, we have learned about operators, and more closely explored arithmetic, logical and relational operators. On top of this, we have learned more about group generic functions, double dispatch system, and managed to implement a simple sparse_vector class with working arithmetic, logical and relational operators."
  },
  {
    "objectID": "posts/baseR_AZ/2_functions.html",
    "href": "posts/baseR_AZ/2_functions.html",
    "title": "Base R from A to Z: Types of functions (2)",
    "section": "",
    "text": "In the second part of the series about the base R, I wanted originally to talk about operators. But to talk about properators properly, I wanted to explore how operators are defined first and the difference between classical functions and something called primitives. In the end, the small segway got a bit too big, so I decided to release it as an independent article."
  },
  {
    "objectID": "posts/baseR_AZ/2_functions.html#types-of-functions",
    "href": "posts/baseR_AZ/2_functions.html#types-of-functions",
    "title": "Base R from A to Z: Types of functions (2)",
    "section": "Types of functions",
    "text": "Types of functions\n\nTo understand computations in R, two slogans are helpful:\n\nEverything that exists is an object.\nEverything that happens is a function call.\n– John Chambers (author of S, coauthor of R)\n\n\nAs per the quote, everything that happens in R is a function call. Operators are construct similar to functions, but with special syntax or semantics. But you might have realized that not every function is the same and that R has different types of functions.\nFor instance, when you write the name of a standard function, you will print their code. This doesn’t work with operators and other language elements. To operate with them, you need to escape them, either with quotation marks \" or backticks `. Backticks typically work in most occassions. For instance, we can use backticks to print help of +, print the body of - or call * as if it was a function:\n\n?`+` # prints a help-page of +\n`-` # prints the body of -\n\nfunction (e1, e2)  .Primitive(\"-\")\n\n`*`(3,5) # call as if * was a normal function\n\n[1] 15\n\n\nOperators are special kind of functions, but so are other language elements like for, return, or in fact even ( and {. But unlike with operators, calling language elements like a function doesn’t work. We will talk about these language elements in detail some other time.\nThese special functions are called primitives. Primitives are special in many ways. Outside of primitives, all other functions are standard functions that either call R code, or call a compiled C or Fortran code through the interfaces .Internal, .External, .C, .Call or .Fortran.\nTo explore these function types in base, we will define a helper functions. And because there would be a lot of repeated code, we will construct these functions with a function factory.\n\nis_function_type_factory = function(pattern, fixed=TRUE){\n    force(fixed) # force evaluation\n\n    function(x){\n        if(!is.function(x))\n            return(FALSE)\n        \n        body = body(x)\n        if(is.null(body))\n            return(FALSE) # primitives don't have body\n\n        deparse(body) |> grep(pattern=pattern, fixed=fixed) |> any()\n        }\n    }\n\nis.internal = is_function_type_factory(\".Internal\")\nis.external = is_function_type_factory(\".External\")\nis.ccall    = is_function_type_factory(\".C\") # will match .C and .Call\nis.fortran  = is_function_type_factory(\".Fortran\")\n\n# Get all functions from base, we did this before\nfunctions = Filter(is.function, sapply(ls(baseenv()), get, baseenv()))\n\nSo out of 1242, there are 185 primitives, 415 internals, 0 external calls, 3 calls using the .C or .Call interface, 6 calls to Fortran, with the rest being pure R functions.\n\nPrimitives\nPrimitives are special functions that form the core of R language. Normal functions (called closures in R) have a list of arguments called formals, a body, and an enclosing environment (thus called closures).\n\n# formals, body and environment of `is.internal`\nnot_null = Negate(is.null)\nhas_fbe = function(x){c(\n    \"formals\"     = formals(x) |> not_null(),\n    \"body\"        = body(x) |> not_null(),\n    \"environment\" = environment(x) |> not_null()\n    )}\n    \nhas_fbe(is.internal)\n\n    formals        body environment \n       TRUE        TRUE        TRUE \n\n\nCompared to normal functions, primitives do not have any formals, body, or enclosing environment.\n\nhas_fbe(`-`)\n\n    formals        body environment \n      FALSE       FALSE       FALSE \n\n\nThis is why we put an additional check for body(x) |> is.null() in our function factory. So what about the printed body of - that we saw before? It was a fake! R is lying to us here.\n\n`-` # output of this is fake\n\nfunction (e1, e2)  .Primitive(\"-\")\n\n\n\nminus = function(e1, e2) .Primitive(\"-\")\nminus\n\nfunction(e1, e2) .Primitive(\"-\")\n\n\nSee the extra space in the original -? Now watch this. Calling minus(1,2) just prints the original -.\n\nminus(1,2) # just prints the original `-`\n\nfunction (e1, e2)  .Primitive(\"-\")\n\n\nTo get the correct result, we need to call minus()(1,2).\n\nminus()(1,2)\n\n[1] -1\n\n\nThis is because our function returns the .Primitive(\"-\") which then performs the actual call to the - primitive.\nAll the special language elements like for, ( are primitives, so are the functions like .Internal or .Call that communicate with the internal or external libraries. Finally, there is also a performance consideration, many non-special functions are primitives because these call need to be performant.\n\n\nGenerics\nOne reason why I am talking about these types of functions is because of generics. Generics1 are types of functions that when called, they dispatch a function specific to the type of input object. Typically, the type of object is called a class, and the specific function is called a method. This is the core of the S3 object-oriented system. For instance, in most cases you don’t need to worry what kind of object you are working with and call print anyway, which will then dispatch an appropriate method based for the class of the object, such as print.data.frame or print.Date. This makes interactive use quite convenient, but also allows you to write a more generic code. For instance, instead of having to expect every single possible class in your function, you can rest assured that as.character will provide you a reasonable output.\nTypical S3 generic functions would look like this:\n\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x558f238a6c58>\n<environment: namespace:base>\n\n\nJust a single call to UseMethod(\"print\") alternatively UseMethod(\"print\", x) to make the object for which the method will be dispatched (x in this case) explicit. Notice that no other argument parsing is happening there, this is done behind the curtain.\n\n\nGeneric primitives\nPrimitives can be generics as well, even without the use of UseMethod. There are a few ways how this is achieved.\nFirst of all, there are S3 prototype functions in the .GenericsArgsEnv environment. They look like your standard S3 generic function, just a call to UseMethod.\n\nget_objects = function(x){\n    if(is.character(x))\n        x = getNamespace(x)\n\n    sapply(x |> ls(), get, x)\n    }\n\nprimitive_generics = get_objects(.GenericArgsEnv)\nprimitive_generics[1]\n\n$`-`\nfunction (e1, e2) \nUseMethod(\"-\")\n<bytecode: 0x558f21115ed8>\n<environment: namespace:base>\n\nprimitive_generics |> names() |> head(10)\n\n [1] \"-\"   \"!\"   \"!=\"  \"*\"   \"/\"   \"&\"   \"%/%\" \"%%\"  \"^\"   \"+\"  \n\n\nHowever, I am not sure if they are actually used during calls or are there for other reason, such as documentation. There is a similar environment .ArgsEnv that contains primitives that are not generics.\n\nprimitive_not_generic = get_objects(.ArgsEnv)\n\nprimitive_not_generic[[\"seq_len\"]]\n\nfunction (length.out) \nNULL\n<bytecode: 0x558f21104b30>\n<environment: namespace:base>\n\n\nThese functions, when executed, just return NULL.\n\nprimitive_not_generic[[\"seq_len\"]](5)\n\nNULL\n\n\nCompare this to a standard call to seq_len\n\nseq_len(5)\n\n[1] 1 2 3 4 5\n\n\nThis suggests that these functions, likely both .GenericsArgsEnv and .ArgsEnv are not involved in the function dispatch, or if they are, they are involved much later after the .Primitive() call is evaluated.\nThis brings us to the second way how primitives can be generic. But this dispatch is being performed directly in the internal C code using either the DispatchOrEval call or DispatchGeneric. I will stop here and won’t got deeper, because the situation is quickly getting complicated. For instance, despite the manual saying that something called group generics are using the DispatchGeneric, this is not actually the case and this function doesn’t exist in the C code. Instead, many generics are grouped, thus called group, have a complex system of dispatching based utilizing DispatchGroup (instead of DispatchGeneric), switch and integer values likely related to the functions in the src/main/names.c source code.\nYou can see which functions are group generics in two ways, reading the help file for ?groupGeneric (it has no associated object) or by removing non-group generic functions from all primitive generics:\n\nsetdiff(primitive_generics |> names(), .S3PrimitiveGenerics)\n\n [1] \"-\"        \"!\"        \"!=\"       \"*\"        \"/\"        \"&\"       \n [7] \"%/%\"      \"%%\"       \"^\"        \"+\"        \"<\"        \"<=\"      \n[13] \"==\"       \">\"        \">=\"       \"|\"        \"abs\"      \"acos\"    \n[19] \"acosh\"    \"all\"      \"any\"      \"Arg\"      \"asin\"     \"asinh\"   \n[25] \"atan\"     \"atanh\"    \"ceiling\"  \"Conj\"     \"cos\"      \"cosh\"    \n[31] \"cospi\"    \"cummax\"   \"cummin\"   \"cumprod\"  \"cumsum\"   \"digamma\" \n[37] \"exp\"      \"expm1\"    \"floor\"    \"gamma\"    \"Im\"       \"lgamma\"  \n[43] \"log\"      \"log10\"    \"log1p\"    \"log2\"     \"max\"      \"min\"     \n[49] \"Mod\"      \"prod\"     \"range\"    \"Re\"       \"round\"    \"sign\"    \n[55] \"signif\"   \"sin\"      \"sinh\"     \"sinpi\"    \"sqrt\"     \"sum\"     \n[61] \"tan\"      \"tanh\"     \"tanpi\"    \"trigamma\" \"trunc\"   \n\n\nNote that these lists of functions are not generated from the C source code, so there is no way other than reading the C source code to know if a primitive function is generic or not.\n\n\nOperator overloading\nEquations would be quite messy if you had to write a different + for scalars, vector and matrices. In case of operators, this is called operator overloading and can be a dangerous tool if not done carefully.\nThe whole point of operator overloading is that you can have quite complex objects and operate with them in a nice and polite manner as long as the semantics of operators make sense. For instance, the Matrix package defines sparse matrices. Sparse matrices are matrices where the majority of elements are equal to zero. Because of this, it is quite a bit more efficient if they are respresented not as a standard matrix with n*m elements, but with a special representation where you represent only elements that are non-zero. This saves memory and makes some operations faster. At the same time, you want them to behave like standard matrices with operations like addition, multiplication, or subsetting. So in standard S3 way (or S4 in this case), you overload the + operator to dispatch a particular function for addition of two standard matrices, one standard and one sparse, or two sparse matrices. From user perspective, nothing has changed and the operation looks the same M + N regardless of what types the matrices are summed. In languages like Java, which forgeos operator overloading in favour of heavy object-oriented system, you would have to do something of this sort:\n# without operator overloading\nM.add(N)\nYou can see that complex equation might look quite messy due to this. At least Java has function overloading. Without that, you would have to detect the particular type of objects and dispatch an appropriate function by yourself. This is exactly what the R source code written in C has to do.\nNote that this operator overloading does not always work and you can’t easily overload operators for basic types. For instance, some languages like to overload the + operator for the addition of strings, performing string concatenation. This makes sense in some cases, doesn’t in other cases and is another contentious problems, there is a whole discussion about this on the R mailing list.\n\n`+.character` = function(x,y){paste0(x,y)}\n\"foo\" + \"bar\"\n\nError in \"foo\" + \"bar\": non-numeric argument to binary operator\n\n\nThis is character is a basic type, which do not have a class and thus do not immediatelly work with all primitive generics this way. What matters here is the attribute class that is added to S3 objects. But if you try to explore this attribute with the class(), R will lie to you. You need to use the oldClass() function to see the S3 attribute.\n\nclass(\"foo\") # will lie to you\n\n[1] \"character\"\n\noldClass(\"foo\")\n\nNULL\n\n\nThis overloading will work well for user-defined S3 classes.\n\n`+.foobar` = function(x,y){paste0(x,y)}\nstructure(\"foo\", class = \"foobar\") + structure(\"bar\", class = \"foobar\")\n\n[1] \"foobar\"\n\n\nOr you can define the class explicitelly as a character.\n\n`+.character` = function(x,y){paste0(x,y)}\nstructure(\"foo\", class = \"character\") + structure(\"bar\", class = \"character\")\n\n[1] \"foobar\"\n\n\nBut it is better to not due this.\nAnother way is to overwrite the + function with user-defined function, but don’t do this either. If you think that the + semantics makes sense for strings, just define the %+% operator:\n\n`%+%` = function(x,y){paste0(x,y)}\n\"foo\" %+% \"bar\"\n\n[1] \"foobar\"\n\n\nOne famous case of using the operator + in a special way is to combine graphical elements in the ggplot package.\nOperator overloading is especially useful for the equality operator =, but also for various subsetting operators [ or [[, as well as for the replacement operators [<-. We will talk about this next time.\n\n\nspecial vs builtin\nAnd just for completeness, Primitives and internal functions are also divided into special and builtin.\n\nprimitives = Filter(is.primitive, functions)\nsplit(primitives |> names(), sapply(primitives, typeof)) |>\n    sapply(head, n=10, simplify=FALSE) # sample of bultin and specials\n\n$builtin\n [1] \"-\"   \":\"   \"!\"   \"!=\"  \"(\"   \"*\"   \"/\"   \"&\"   \"%*%\" \"%/%\"\n\n$special\n [1] \"::\"   \":::\"  \"[\"    \"[[\"   \"[[<-\" \"[<-\"  \"{\"    \"@\"    \"@<-\"  \"&&\"  \n\n\nThe difference between them is that builtin functions evaluate all their arguments before being passed to the internal implementation, while special don’t evaluate their arguments. When profiling, builtin functions are also counted as a function calls, while special aren’t. Only a small number of functions are trully special :). And while we can look which primitives are builtin or special, we can’t do so with internal functions. For instance, according to the documentation, cbind is special internal, while grep is builtin internal. But when you to get their type, all you get is a closure.\n\nc(\"cbind\" = typeof(cbind), \"grep\" = typeof(grep))\n\n    cbind      grep \n\"closure\" \"closure\" \n\n\nUnless you want to join the development team of R, you do not need to know about this distinction."
  },
  {
    "objectID": "posts/baseR_AZ/2_functions.html#summary",
    "href": "posts/baseR_AZ/2_functions.html#summary",
    "title": "Base R from A to Z: Types of functions (2)",
    "section": "Summary",
    "text": "Summary\nEverything that happen in R is a function call, but for performance and parsing purpose, R has different types of functions. The classic functions in R are closures, but there are special kind of functions called primitives. Most operators are primitives and some primitives can also be generics, which connects them to the S3 dispatch system. This allows operators to be overloaded for user-defined classes, but not for basic types, despite not being typical S3 generics with UseMethod() call."
  },
  {
    "objectID": "posts/hello_world/index.html",
    "href": "posts/hello_world/index.html",
    "title": "Hello World!",
    "section": "",
    "text": "So I decided to do blogging. Again.\n\nWhat can you expect?\nSince I am computational biologists, you can expect some statistics and data science with biological motivations, usually evolutionary biology or bioinformatics.\nRegarding programming things, I will write mostly about R. I am kind of purist so I am one of the few people who prefers base R, including base graphics, so I will try to write a few articles regarding some forgotten treasures there. For example, since base does not divide its functions into namespaces (like Python’s standard library), it is hard to find a complex documentation about all the string-manipulation functions, so many people might not know that functions like trimws() even exists.\nOutside of R, I use also Python, and I finally starter using C/C++, at least in connection with R.\n\n\nWhy wrote blogs? Aren’t there already plenty of them?\nMy memory is terrible and I have a bad tendency of leaving projects unfinished. Blogposts will serve as a good written memory and something that will force me to finish projects to a state where I woldn’t be ashamed of them being public. This goes double for various learning projects. So you see, this blog is not for you, it is for me. But since most of the popular blogs originated in this way, I hope that eventually,\n\n\nMy previous attempts at blogging\nMy previous attempts at blogging didn’t get far. One was named “Mammoths on Mars”, a cool name, something about why developing science to such a degree that we could have mammoths on Mars would be so frigging awesome. But all I got from it was a half-assed post about why testing for phylogenetic signal from data that could arise from an evolutionary process is so complicated. Something that would plague me in the future. Not the half-assed blogpost, but the phylogenetic signal.\nThat was 8 years ago, I was just learning about static site generators which were becoming increasingly popular. Finally, small sites with pure HTML generated from markup format! And since I was using quite a bit Python at that time, I didn’t pick the omnipresent Jekyll, but a Python-based Pelican. But the documentation wasn’t there yet, everyone was talking about templates, but no one really explained what are those in the first places. That was not very beginner friendly.\nMy second attempt came years later, because I really wanted to understand all these templates. Which was a cool learning experience, because you can create a static site generator really easily! You can see the attempt here, only 7 direct dependencies (6, if I remove magrittr) and ~300 lines of code. The site is technically still lives, but I don’t update it often because getting good photos of food is so damn hard. Something about proper light and clean table.\nThe learning experience was cool, I learned a lot about CSS at least. I still dislike the whole experience, which was the reason why for the third time, I picked something pre-backed. And Quarto was recently released. So here we are, after a week fiddling with some internals, parameters, trying to modify the .scss bootstrap theme and finding that it doesn’t do anything. But perfect is the enemy of good, and I really want to write some blogposts and not fiddle with the parameters that much.\nSo here we are, now you know everything. Hope you will find content interesting or at least in some way entertaining."
  },
  {
    "objectID": "posts/containers/containers.html",
    "href": "posts/containers/containers.html",
    "title": "Errors, builds, and containers",
    "section": "",
    "text": "You have probably experienced it more than once. You try to run some code and instead of a result, you get a strange error saying that something is broken. You are scratching your head because the error doesn’t say clearly what is broken but points to some esoteric part of the code that should be right, or points very deep into subsystem or dependency, which points to another, only for you to discover in the end that the issue is in a completely different part.\nPreviously, this happened to me with devtools or testthat, when both packages added another dependency (or their dependency added a dependency) cli, which incorrectly handled the version string of one of the most common terminals on one of the most common Linux distro. What was more infuriating is finding that it was supposed to be fixed 6 months ago. Why the main branch wasn’t patched, and the bugged version was released on CRAN instead is a mystery to me.\nAnyway, enough of this rant, this blogpost is not about this, nor it is about R, but about another completely different issue.\n\n\n\nIn the beginning, well I don’t know since I wasn’t there, but much later than that we had GNU make to build C code. But the build became more and more complex, so more complex tools were developed. And these tools became, and still are, more and more complex. From my tiny experience, it was easier to write a package in Java, than to write and setup Ant or Gradle from scratch. But this might be due to my inexperience, I write code frequently although in compiled languages but I don’t write build scripts often enough.\n\n\n\nYou can encounter a similar issue if you want to build existing code as well. For instance, 7kaa, a commercial game that was open-sourced by Enlight, uses autotools. As you might get from the name, autotools is not a single tool, but a collection of tools. And if you don’t have any of those installed, you will get strange errors, especially if you didn’t notice a single warning at the start of the very long log file.\nIn the end, I managed to solve this issue by installing dependencies and potentially polluting my system. But what if I wanted to do this next time again? Would I remember what did I do wrong and how I solved the issue? Can I do something better to isolate the dependency issue in a testable and replicable way so that a completely new clean system can be guaranteed to succeed?"
  },
  {
    "objectID": "posts/containers/containers.html#podman",
    "href": "posts/containers/containers.html#podman",
    "title": "Errors, builds, and containers",
    "section": "Podman",
    "text": "Podman\npodman is a lesser-known cousin of Docker. The advantage of Podman is that it allows the creation of so-called rootless containers. This makes it a little easier to work with.\nBut what are Podman/Docker containers? Simply said, containers are packages containing an application, including dependencies and even OS. This means that with containers, you can create a replicable installation and deployment of applications on clean systems, which are isolated from your host system by the Docker/Podman engine.\n\nInstalling podman\nOn my system (Ubuntu 22.04), podman is easy to install:\nsudo apt install podman\nHowever, there is currently a bug, the config in /etc/containers/registries.conf does not come with correct presets. In fact, it does not come with any presets at all, it only contains documentation. This means that it is not connected to any repository, and the commands mentioned in the tutorial, such as:\npodman search [search term]\ndo not output anything.\nTo fix this, we need to append:\nunqualified-search-registries = [\"docker.io\"]\n\n[[registry]]\nlocation = \"docker.io\nto the /etc/containers/registries.conf. This will allow us to reach the docker.io repository.\nFor instance, to find an ubuntu image, we can now do:\npodman search ubuntu --limit 5\n\n\nSetting up container\nFirst, we download an image:\npodman pull docker.io/library/ubuntu\nNow we can run the container. We run it with the detach -d option, as otherwise the container would just execute all required code and stopped working. We also run the container with the -t option, to enable tty, or terminal. For repeatability, we provide a custom name with the --name command. After that, we attach to the container.\npodman run  --name \"test\" -dt docker.io/library/ubuntu:latest\npodman attach \"test\"\n\n# to clean the container after finishing, run:\npodman rm \"test\"\nWe are now inside the container, a brand new and clean Ubuntu 22.04 install, and we can start installing dependencies and building our app. After a few tries trying to find out the required libraries and dependencies, I reached this:\n# update apt\napt update\n\n# install build tools\napt install -y git automake autopoint autoconf autoconf-archive g++ make\n\n# install dependencies\napt install -y libsdl2-dev libenet-dev libopenal-dev libcurl4-openssl-dev gettext\n\n# clone repo and build binary\ngit clone https://github.com/the3dfxdude/7kaa/ home/7kaa\ncd home/7kaa\n./autogen.sh && ./configure && make\nAll we need now is to copy the binary and data with podman container cp [source] [dest]\n\n\nWriting a Containerfile\nWe have commands that we want to run, but having to run them manually is annoying. To automate this, we can write a Containerfile! Or Dockerfile, as Docker calls it, but the syntax is identical.\nMost container files consist of FROM, which specifies the image one is working with, RUN which runs various commands used to build the container, COPY which allows one to copy content to or from the container, and CMD which is then used to launch the applications themselves.\n# Containerfile\nFROM ubuntu:22.04\n\n# you can add multiple labels in a field=value format\nLABEL maintainer=\"j-moravec\"\n\n\nRUN apt-get update && apt-get install -y \\\n    git automake autopoint autoconf autoconf-archive g++ make \\\n    libsdl2-dev libenet-dev libopenal-dev libcurl4-openssl-dev gettext\n\nRUN git clone https://github.com/the3dfxdude/7kaa /home/7kaa\n\nWORKDIR /home/7kaa\nRUN ./autogen.sh && ./configure && make\nBuild this image with podman -t \"7kaa\" -f Containerfile."
  },
  {
    "objectID": "posts/containers/containers.html#appimage",
    "href": "posts/containers/containers.html#appimage",
    "title": "Errors, builds, and containers",
    "section": "AppImage",
    "text": "AppImage\nWe have a replicable way to build a binary. The issue is that it is built against a particular version of Ubuntu and against a particular version of SDL2, Enet, OpenAL and Curl, so the binary won’t work on every system. One way to bundle these dependencies is with AppImage.\nNote that to run AppImages, you need to have libfuse2 installed. Also, I will be using wget to download some files, so make sure you have it installed as well. Everything will be provided in the final containerfile.\nTo create AppImage, we need to create AppDir directory with a pre-specified format either manually, or with the help of the linuxdeploy tool. Linuxdeploy is a tool to create AppImages, so it not only creates the required directory structure, but also copies dependencies for provided binary, compiles provided icon and desktop file (which are required), and also creates AppImage itself. To make an AppImage with linuxdeploy, we need to:\n\ncreate AppDir structure\ncreate binary, icon and desktop files in a pre-specified format\ncreate AppImage\n\nLinuxdeploy allows working in an iterative format. That basically means that you mess around until it works. When you do this, note that provided binary, icon, and a desktop file are currently not updated with repeated runs, so the iterative approach does not work completely. Alas, after mocking around for a bit and trying to figure it out since the documentation is not great and in many areas resembles stump, this is what we will do:\n\nInstall 7kaa into AppDir\nconvert icon 7k.ico to the png format\ncreate desktop file\nrun linuxdeploy to put it all together and create AppImage\n\n\nInstalling into AppDir\nFirst, we need to build the application in a way that it is installed. But we do not want to actually install it into the image filesystem, but into the AppDir:\nmake install  DESTDIR=/home/7kaa/AppDir\nNote that due to the way the build system for 7kaa is set up, with multiple makefiles for various subsystems, we need to specify AppDir with an absolute path instead of relative.\nAnother thing that we need to do is move everything from /usr/local/ to /usr/. While both are valid locations and in fact /usr/local/ is more suitable from an administrative perspective, linuxdeploy does not recognize binary in /usr/local/bin/\nmv AppDir/usr/local/* AppDir/usr/\nrm -r AppDir/usr/local\n\n\nConverting icon to png\nThe icon that we have is in ico format, which comes from Windows and is not supported by the XDG Linux desktop specification, which accepts only png, svg and xpm.\nWe can use imagemagick to convert it with:\nconvert src/7k.ico 7k.png\nBut it is another dependency we need to include in our containerfile.\n\n\nCreating desktop file\nDesktop files are files that allow integration of your program/binary with your desktop, and they are required to create an AppImage. An example can be found on archlinux wiki, with full specification on freedesktop.org. Note that quite a few elements appear to be required by the linuxdeploy.\nA sample desktop file 7kaa.desktop might look like this:\n[Desktop Entry]\nType=Application\nName=7kaa\nComment=Seven Kingdoms: Ancient Adversaries\nPath=/usr/bin\nExec=7kaa\nIcon=7k # should not contain an extension\nCategories=Game; # one of several categories in specification\n\n\nRunning linuxdeploy\nNow, all we need to do is to run linuxdeploy, which will copy dependencies required by our binary (such as SDL2), and output an AppImage.\nwget -nv https://github.com/linuxdeploy/linuxdeploy/releases/download/1-alpha-20220822-1/linuxdeploy-x86_64.AppImage -o linuxdeploy.AppImage\nchmod +x linuxdeploy.AppImage\nVERSION=dev ./linuxdeploy.AppImage --appdir AppDir -d 7kaa.desktop -i 7k.png --output appimage\nWe have also set a version of the resulting appimage to dev by specifying the recognized environment variable VERSION.\n\n\nPointing 7kaa to its data\nNormally, this is all we would have to do. But if you try to run the AppImage, you will find out that the binary can’t find the data, this is because the binary is not in the same directory and the SKDATA environment variable is empty. We need to run the 7kaa binary in the same way that we run the linuxdeploy.AppImage, which essentially means we need to write our own runner script.\nThere are two ways how we can do it, either run a complex bash script that does some environment parsing and settings as required for AppImage, like imagemagick does, or we use just a simple runner script and use provided AppRun, which is a simple binary that does what we need and also parses the desktop file.\nFirst, create an AppDir/usr/bin/7krun\n#!/bin/env sh\n\nSKDATA=share/7kaa/ 7kaa\nand don’t forget to make it executable with chmod +x AppDir/usr/bin/7krun.\nThen download the AppRun binary:\nwget -nv https://github.com/AppImage/AppImageKit/releases/download/13/AppRun-x86_64 -o AppDir/AppRun\nand modify the desktop file so it executes 7krun instead of 7kaa:\n[Desktop Entry]\nType=Application\nName=7kaa\nComment=Seven Kingdoms: Ancient Adversaries\nPath=/usr/bin\nExec=7krun\nIcon=7k\nCategories=Game;\nand now just build the AppImage!\nVERSION=dev ./linuxdeploy.AppImage --appdir AppDir -d 7kaa.desktop -i 7k.png --output appimage\nand test with:\n./7kaa-dev-x86_64.AppImage\n\n\nAdding music\nDue to copyright reasons, music is distributed separately. In fact, one of the motivations for creating an AppImage was that on some Linux distributions, music is not distributed at all due to it not being FOSS.\nThe music is available at the 7kfans website as a separate download. We will download the archive, unpack it, transfer the music into the DATA directory and rebuild the AppImage.\n# force IPv4, the default IPv6 seems to be broken and just hang on\nwget -nv -c -4 https://www.7kfans.com/downloads/7kaa-music-2.15.tar.bz2\ntar -xf 7kaa-music-2.15.tar.bz2\nmv 7kaa-music/MUSIC/ AppDir/usr/share/7kaa/\n\nVERSION=dev ./linuxdeploy.AppImage --appdir AppDir -d 7kaa.desktop -i 7k.png --output appimage\nand test with:\n./7kaa-dev-x86_64.AppImage\n\n\nPutting it all together:\nIn the end, we will get this containerfile:\nFROM ubuntu:22.04\n\n# you can add multiple labels in a field=value format\nLABEL maintainer=\"j-moravec\"\n\n\nRUN apt-get update && apt-get install -y \\\n    git automake autopoint autoconf autoconf-archive g++ make \\\n    libsdl2-dev libenet-dev libopenal-dev libcurl4-openssl-dev gettext \\\n    imagemagick wget libfuse2 file\n\nRUN git clone https://github.com/the3dfxdude/7kaa /home/7kaa\n\nWORKDIR /home/7kaa\n\nRUN ./autogen.sh && \\\n    ./configure && \\\n    make install DESTDIR=/home/7kaa/AppDir && \\\n    mv AppDir/usr/local/* AppDir/usr/ && \\\n    rm -r AppDir/usr/local\n\nRUN convert src/7k.ico 7k.png && \\\n    echo '#!/bin/env sh' > AppDir/usr/bin/7krun && \\\n    echo '' AppDir/usr/bin/7krun && \\\n    chmod +x AppDir/usr/bin/7krun && \\\n    echo 'SKDATA=share/7kaa/ 7kaa' >> AppDir/usr/bin/7krun && \\\n    echo '[Desktop Entry]' > 7kaa.desktop && \\\n    echo 'Type=Application' >> 7kaa.desktop && \\\n    echo 'Name=7kaa' >> 7kaa.desktop && \\\n    echo 'Comment=Seven Kingdoms: Ancient Adversaries' >> 7kaa.desktop && \\\n    echo 'Path=/usr/bin' >> 7kaa.desktop && \\\n    echo 'Exec=7krun' >> 7kaa.desktop && \\\n    echo 'Icon=7k' >> 7kaa.desktop && \\\n    echo 'Categories=Game;' >> 7kaa.desktop\n\nRUN wget -nv -c -4 https://www.7kfans.com/downloads/7kaa-music-2.15.tar.bz2 && \\\n    tar -xf 7kaa-music-2.15.tar.bz2 && \\\n    mv 7kaa-music/MUSIC/ AppDir/usr/share/7kaa/\n\nRUN wget -nv -c https://github.com/AppImage/AppImageKit/releases/download/13/AppRun-x86_64 -O AppDir/AppRun && \\\n    chmod +x AppDir/AppRun && \\\n    wget -nv -c https://github.com/linuxdeploy/linuxdeploy/releases/download/1-alpha-20220822-1/linuxdeploy-x86_64.AppImage && \\\n    chmod +x linuxdeploy-x86_64.AppImage && \\\n    VERSION=dev ./linuxdeploy-x86_64.AppImage \\\n        --appimage-extract-and-run \\\n        --appdir AppDir \\\n        -d 7kaa.desktop \\\n        -i 7k.png \\\n        --output appimage\nNote that I did some minor tweeks to iron out potential issues.\nNow we should be able to run:\npodman build -t 7kaa -f Containerfile\nAnd half an hour later, when the image is finally built:\npodman create --name 7kaaAppImage 7kaa\npodman cp 7kaaAppImage:/home/7kaa/7kaa-dev-x86_64.AppImage .\npodman rm 7kaaAppImage\nchmod +x 7kaa-dev-x86_64.AppImage\n./7kaa-dev-x86_64.AppImage\nand everything should work."
  },
  {
    "objectID": "posts/containers/containers.html#conclusion",
    "href": "posts/containers/containers.html#conclusion",
    "title": "Errors, builds, and containers",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to use podman and dockerfile to create a replicable build of an AppImage for the Seven Kingdoms: Ancient Adversaries.\nHandling podman and contairnerfiles was rather easy, the biggest issue was making AppImages themselves. But we managed that as well."
  },
  {
    "objectID": "posts/containers/containers2.html",
    "href": "posts/containers/containers2.html",
    "title": "Errors, builds, and containers (2)",
    "section": "",
    "text": "Last time we created a Containerfile to build Seven Kingdoms: Ancient Adversaries from source. We then utilized Appimage to distribute the executable with all its dependencies for various Linux systems.\nThey way we created the containerfile is that we iteratively made it work and fixed any issues we have encountered. We did it by splitting steps into dependency install part, building binary, and then finally building the AppImage. To make the life a bit easier, these steps are cached, so they don’t need to be ran again if they are not changed. However, if we were missing dependency for AppImage, we had to rebuild everything from scratch, which wasn’t very efficient.\nI think we can do better. Since we first build the game from sources, and only then we created the AppImage, I think we could reasonably break this into two step process, essentially have a containerfile for build from the source, and another one for AppImage only. But connected. And only in a single file. That is what Multi-staged builds are."
  },
  {
    "objectID": "posts/containers/containers2.html#multi-staged-builds",
    "href": "posts/containers/containers2.html#multi-staged-builds",
    "title": "Errors, builds, and containers (2)",
    "section": "Multi-staged builds",
    "text": "Multi-staged builds\nAs described above, multi-staged builds are essentially several different separated steps that are chained together. They make things easier as reason about and reduce potential cross-contamination between different unrelated parts of containerfile as you need to define everything explicitely, including copying any files or artifacts from one stage to another.\nSo let’s prepare a multi-staged Containerfile2:\nFROM ubuntu:22.04 as binary\n\n# you can add multiple labels in a field=value format\nLABEL maintainer=\"Jiří Moravec\"\n\n# tool required to make binary\n# alphabetically sorted (kind of) according to best practices\nRUN apt-get update && apt-get install -y \\\n    automake \\\n    autopoint \\\n    autoconf \\\n    autoconf-archive \\\n    g++ \\\n    gettext \\\n    git \\\n    make\n\n# 7kaa dependencies\nRUN apt-get install -y \\\n    libcurl4-openssl-dev \\\n    libenet-dev \\\n    libopenal-dev \\\n    libsdl2-dev\n\nRUN git clone https://github.com/the3dfxdude/7kaa /home/7kaa\n\n\nWORKDIR /home/7kaa\n\nRUN ./autogen.sh && \\\n    ./configure && \\\n    make install DESTDIR=/home/7kaa/inst && \\\n    cp src/7k.ico /home/7kaa/inst\nThis is our first part. We call it binary (coz we are building a binary) and to make it more general, we install it into inst directory instead of the AppImage.\nThis is everything we need to build 7kaa binary straight from source. If there are other unstated dependencies, they are installed with what we are installing or they are part of the Ubuntu image.\nNote that if we were to run make instead of make install and copied only the binary, we wouldn’t get locale files, and the run would run only in English. But we already have German, Polish, Spanish and other translations, so why not use them as well!\nNow, we will create the second stage of our Containerfile2, where we will utilize the artifacts from the binary part to build an AppImage.\nFROM ubuntu:22.04 as appimage\n\n# copying artifacts\nCOPY --from=binary /home/7kaa/inst/* /home/7kaa/AppDir\n\n# 7kaa dependencies, no dev variants required!\nRUN apt-get update && apt-get install -y \\\n    libcurl4 \\\n    libenet7 \\\n    libopenal1 \\\n    libsdl2-2.0-0\n\n\nRUN apt-get install -y \\\n    imagemagick \\\n    file \\\n    lbzip2 \\\n    libfuse2 \\\n    wget\n\n\nWORKDIR /home/7kaa/AppDir\n\nRUN mv usr/local/* usr/ && \\\n    rm -r usr/local\n\nRUN convert 7k.ico 7k.png && rm 7k.ico\n\nRUN echo '#!/bin/env sh' > 7krun && \\\n    echo '' 7krun && \\\n    echo 'SKDATA=share/7kaa/ 7kaa' >> 7krun && \\\n    chmod +x bin/7krun && \\\n    mv 7krun usr/bin\n\nRUN echo '[Desktop Entry]' > 7kaa.desktop && \\\n    echo 'Type=Application' >> 7kaa.desktop && \\\n    echo 'Name=7kaa' >> 7kaa.desktop && \\\n    echo 'Comment=Seven Kingdoms: Ancient Adversaries' >> 7kaa.desktop && \\\n    echo 'Path=/usr/bin' >> 7kaa.desktop && \\\n    echo 'Exec=7krun' >> 7kaa.desktop && \\\n    echo 'Icon=7k' >> 7kaa.desktop && \\\n    echo 'Categories=Game;' >> 7kaa.desktop\n\nRUN wget -nv -c -4 https://www.7kfans.com/downloads/7kaa-music-2.15.tar.bz2 && \\\n    tar -xf 7kaa-music-2.15.tar.bz2 && \\\n    mv 7kaa-music/MUSIC/ usr/share/7kaa/ && \\\n    rm -r 7kaa-music\n\nWORKDIR /home/7kaa/\n\nRUN wget -nv -c https://github.com/AppImage/AppImageKit/releases/download/13/AppRun-x86_64 -O AppDir/AppRun && \\\n    chmod +x AppDir/AppRun && \\\n    wget -nv -c https://github.com/linuxdeploy/linuxdeploy/releases/download/1-alpha-20220822-1/linuxdeploy-x86_64.AppImage && \\\n    chmod +x linuxdeploy-x86_64.AppImage && \\\n    VERSION=dev ./linuxdeploy-x86_64.AppImage \\\n        --appimage-extract-and-run \\\n        --appdir AppDir \\\n        -d AppDir/7kaa.desktop \\\n        -i AppDir/7k.png \\\n        --output appimage\nSome of these steps, like the one creating 7kaa.desktop are relatively simple, but require a lot of weird bash code. I can see files like that be included in either the game repository, or a repository with a dockerfile and copied over. But also, there is an advantage having everything self-contained.\nA similar issue is with the icon. For the purpose of this build, I did not modified the original source (and now, 6 months after I started writing this document, the development have moved from GitHub to SourceForge), but a png icon should surely be in the source code. That would remove quite a hefty dependency."
  },
  {
    "objectID": "posts/containers/containers2.html#conclusion",
    "href": "posts/containers/containers2.html#conclusion",
    "title": "Errors, builds, and containers (2)",
    "section": "Conclusion",
    "text": "Conclusion\nMulti-staged builds are powerful tool. If setup correctly, they can save a lot of time rebuilding container after changes. They are also great tool during development since it takes quite a lot of effort to catch all missing dependencies and make everything working as it should. And during this time, you will be rebuilding a lot. You could think about it as different targets in a makefile.\nHowever, they are not without issues. You might have noticed that the final containerfile is quite a bit more complicated. In addition to this, each layer of multi-staged build adds to the size of the image."
  },
  {
    "objectID": "posts/containers/vmwin.html",
    "href": "posts/containers/vmwin.html",
    "title": "VirtualBox with Windows 11",
    "section": "",
    "text": "You might have probably guessed by my previous posts that I run Linux. In fact, I run exclusively Linux machine since 2010, my last Windows experience was Windows Vista and I don’t like to remember that very well.\nUnfortunately, even with the all powerful Wine, there are some applications that cannot be run on Linux, such as PowerBI. And since I am looking around what kind of industry skills one needs to have, PowerBI appeared quite a lot.\nInitially, I tried to install it on Wine, but there is quite a lot of issues with the newest dotnet (.NET) libraries, the newer one just plainly do not work with any version of Wine and there is a long-standing bug. S"
  },
  {
    "objectID": "posts/containers/vmwin.html#virtualbox",
    "href": "posts/containers/vmwin.html#virtualbox",
    "title": "VirtualBox with Windows 11",
    "section": "VirtualBox",
    "text": "VirtualBox\nLast time we were talking about containers, specifically Podman and Docker. These are type of software virtualizations. They also have a bigger cousin called Virtual Machine, which allows for virtualization of a whole OS. If you are confused a bit like me, this answer seems to explain the differences quite well.\nVirtual Machines are also divided into multiple types, the main distinction is mainly between type 1 and type 2 Hypervisors. Type 1 runs virtual machine directly on hardware, while type 2 runs on the host OS layer. But let’s top here.\nWhen I was looking around for a good VM, VirtualBox jumped at me. While it is from Oracle, it is free, open-source, and seems to be used quite a bit.\nAnother thing that jumped at me is that nowadays you can download Windows for free. Props to MS for this, looks like the times are getting better and barriers that hinders one to use software of their choice are falling.\n\nNot this way: VirtualBox 7.0\nAs any reasonable person, first thing I did was download the latest version of VirtualBox and installed Windows 11.\nThe installation was breeze, and there wasn’t anything that seemed wrong. However, issues started to pop and when I tried to run any app, I got reboots, freezing, and various graphical issues. Especially after I installed an extension that allows you to copy paste from one system to another or just drag and drop files, the operation system barely worked and nothing seemed to help.\nInternet suggested, that there seems to be some incompatibility between VB 7.0 and Win11, and that the previous version of VB works fine.\n\n\nThis way to go: VirtualBox 6.1\nThe first thing that surprised me is that installation process on the 6.1 was quite different than on the 7.0. For some reason, Windows 11 behaved differently already in this stage. The whole installation process was much more involved, notably it required MS account, which was not required on 7.0. Fortunately, in the end, everything worked perfectly, I could run and install PowerBI, and I was even able to test some Windows-only games that do not work on Wine.\nSo to stop me blaberring, are some tips and instructions for you if you are installing VM for the first time:\n\nUse SSD and not HDD\nDuring my first attempt, I used HDD, but this was a clear mistake. The time it takes to load VM is equivalent to time it takes to load OS on HDD, which means you will wait ages and the system will be quite slower. So just use SSD, the time it will take to load the whole OS won’t be much different from the time it takes you to start browser.\n\n\nNot enough resources bug\nWhile on VB 7.0 the installation goes smoothly, stuff gets a bit rough on VM 6.1. At the start of the installation, the Windows will proclaim that it doesn’t have enough resources, although 4GB ram and 64 GB space should be enough. You need to do some regedit:\n\nOn the installation screen, before you click INSTALL NOW, press SHIFT+F10 to start terminal\ntype regedit to start Registry Editor\nNavigate to HKEY_LOCAL_MACHINE\\SYSTEM\\Setup\nRight-click on Setup and select New => Key with name LabConfig\nRight-click on LabConfig, select New => DWORD (32-bit) and create a value named BypassTPMCheck\nSet BypassTPMCheck value to \"1\"\nIn the same way (New => DWORD (32-bit)), create BypassRAMCheck and BypassSecureBootCheck with values \"1\"\nClose the Registry Editor and close the terminal (e.g., type exit)\nClick INSTALL NOW and proceed with installation\n\n\n\nInstalling extension pack\nWhen you are running Windows, click on Devices in the VM menu, and then Insert Guest Additions CD Image, which will download the image from the VirtualBox website.\nNote that you need then access the loaded image and install it BOTH on your host OS machine, and on the virtualized one. Only then you can enable the shared clipboard and drag-and-drop features. These should work after reboot."
  },
  {
    "objectID": "posts/containers/vmwin.html#summary",
    "href": "posts/containers/vmwin.html#summary",
    "title": "VirtualBox with Windows 11",
    "section": "Summary",
    "text": "Summary\nAnd you are there, you should be able to run PowerBI, or any Windows-only SW, on your Linux machine like I did. Hope some of this was helpful for you. See you in the industry!"
  },
  {
    "objectID": "posts/pop/pop.html",
    "href": "posts/pop/pop.html",
    "title": "Make it pop!",
    "section": "",
    "text": "Functions like pop exists in many languages for iterable item, such as lists or arrays. While particulars change, in general pop removes an item from a container and return said item. This is typical for data structures called queue, stack, and deque, which beside pop implement functions for adding elements to this container and differ in whether they are first in first out (queue), first in last out (stack), or both ways (deque).\nWhile not common to R, these data structures are very convenient when you are consuming elements of an array, especially if the number of consumed elements can differ each iteration. For instance, you can consume command-line arguments when each parameter can have different number of arguments.\nSee following snippet.\nI could surely implement this by incrementing index i and it wouldn’t be much difficult. But this makes the code a bit cleaner."
  },
  {
    "objectID": "posts/pop/pop.html#helper-functions",
    "href": "posts/pop/pop.html#helper-functions",
    "title": "Make it pop!",
    "section": "Helper functions",
    "text": "Helper functions\nSince we will be doing some memory inspections, we will define two helper functions.\ninspect is simple mapping of the internal inspect function to a visible namespace. It will print the internal structure of an object. Think about it as a very complex str.\naddress is a simple C function compiled using the great inline package to simply return address of an object. This function is copied from data.table.\n\ninspect = function(x) .Internal(inspect(x))\naddress = inline::cfunction(signature(x = \"SEXP\"),\n    body = \"\n    char buffer[32];\n    snprintf(buffer, 32, \\\"%p\\\", (void *)x);\n    return(mkString(buffer));\n    \",\n    language = \"C\",\n    )"
  },
  {
    "objectID": "posts/pop/pop.html#definition",
    "href": "posts/pop/pop.html#definition",
    "title": "Make it pop!",
    "section": "Definition",
    "text": "Definition\nLet pop(x, n) be a function that returns n first elements of x and at the same time removes n elements from x.\nFor instance:\n\nvec = c(5, 3, 2, 10, 5)\npop(vec, 2) # [1] 5 3\nvec # [1] 2 10 5"
  },
  {
    "objectID": "posts/pop/pop.html#usign-assign",
    "href": "posts/pop/pop.html#usign-assign",
    "title": "Make it pop!",
    "section": "Usign assign",
    "text": "Usign assign\nWhen doing modification of a state, the first thing that might come to your mind is the <<- operator.\n\npop = function(x){\n    val = x[1]\n    x <<- x[-1]\n    val\n    }\n\nvec = c(5, 3, 8)\npop(vec)\n\n[1] 5\n\nvec\n\n[1] 5 3 8\n\nx\n\n[1] 3 8\n\n\nBut that won’t work, since you are modifying the variable x instead of the one passed to the pop function. You need to find the name of that variable and assign the modified vector to it.\n\npop = function(x, n = 1){\n    ns = seq_len(min(n, length(x)))\n    val = x[ns]\n    obj = x[-ns]\n    assign(deparse(substitute(x)), obj, envir=parent.frame())\n    val\n    }\n\nvec = c(5, 3, 8)\npop(vec)\n\n[1] 5\n\nvec\n\n[1] 3 8\n\n\nThere are few potential issues here. First of all we are assigning the value into the parent.frame() of the pop function, which won’t work if the pop is being called inside function, but that is a risk I am willing to take and a version that I ended up using.\nThe second issue is that we are re-allocating the vector during each call pop. When we are doing only a few operatios with short vectors, this is fine. But this can be an interesting challenge.\nHowever, one of the big advantages of this approach is how easy it is, and that object we are working with, a vector, is just an R vector. We don’t need to define any particular new behaviour to work with it."
  },
  {
    "objectID": "posts/pop/pop.html#function-factory",
    "href": "posts/pop/pop.html#function-factory",
    "title": "Make it pop!",
    "section": "Function factory",
    "text": "Function factory\nAnother way to do this might be to create a function factory that works as an iterator:\n\nnew_pop = function(x){\n    i = 1\n    x = x\n\n    function(n = 1){\n        if(i > length(x))\n            return(NULL)\n\n        ns = seq_len(min(n, length(x) - i + 1))\n        val = x[ns + i - 1]\n        i <<- i + n\n\n        cat(\"Address of 'x' is: \", address(x), \"\\n\")\n        val\n        }\n    }\n\nvec = c(5, 3, 8)\naddress(vec)\n\n[1] \"0x55b6d5aa3bc8\"\n\npop = new_pop(vec)\npop()\n\nAddress of 'x' is:  0x55b6d5aa3bc8 \n\n\n[1] 5\n\npop(2)\n\nAddress of 'x' is:  0x55b6d5aa3bc8 \n\n\n[1] 3 8\n\npop()\n\nNULL\n\n\nNotice that the address of the vector didn’t change! Since there is no internal modification, R will never have to create a copy. All we are changing is the iterating variable i.\nLike the previous example, this is a really simple to do and we do not need to define any additional functions and methods.\nThe disadvantage is that we don’t know the number of remaining elements, and we can’t look ahead. But surely, these are solvable problems with some parametrization or just"
  },
  {
    "objectID": "posts/pop/pop.html#environments",
    "href": "posts/pop/pop.html#environments",
    "title": "Make it pop!",
    "section": "Environments",
    "text": "Environments\nThe function factory worked because the i and x were stored in an enclosing environment of the returned function.\nWe can do something like that explicitly since environments are the only objects in R where pass-by-reference works. This is how RC and R6 are defined and we are essentially building classes.\n\nnew_stack = function(x){\n    env = new.env(parent = emptyenv(), size = 2)\n    env$x = x\n    env$i = 1 \n    env\n    }\n\npop = function(obj, n = 1){\n    if(obj$i > length(obj$x))\n        return(NULL)\n\n    ns = seq_len(min(n, length(obj$x) - obj$i + 1))\n    val = obj$x[ns + obj$i - 1]\n    obj$i = obj$i + n\n\n    cat(\"Address of 'obj$x' is: \", address(obj$x), \"\\n\")\n    val\n    }\n\nvec = c(5, 3, 8)\naddress(vec)\n\n[1] \"0x55b6d37f94a8\"\n\nstack = new_stack(vec)\naddress(stack)\n\n[1] \"0x55b6d5870ba0\"\n\npop(stack)\n\nAddress of 'obj$x' is:  0x55b6d37f94a8 \n\n\n[1] 5\n\npop(stack, 2)\n\nAddress of 'obj$x' is:  0x55b6d37f94a8 \n\n\n[1] 3 8\n\npop(stack)\n\nNULL\n\n\nThe address of the stack or the internal vector stack$obj doesn’t change."
  },
  {
    "objectID": "posts/pop/pop.html#rc-class",
    "href": "posts/pop/pop.html#rc-class",
    "title": "Make it pop!",
    "section": "RC class",
    "text": "RC class\nFrom this, it is only a small step towards fully fledged classes. As mentioned before, we have RC classes and R6 classes that implements classical OOP seamantics using environments. Generally R6 classes are preferred as they are more performant, but require dependency (the R6 package). RC classes are very similar and included in base R. So let’s try to use them.\n\nStack = setRefClass(\"Stack\",\n    fields = list(\n        items = \"vector\",\n        index = \"integer\"\n        ),\n\n    methods = list(\n        initialize = function(x){\n            items <<- x\n            index <<- 1L\n            },\n        pop = function(n = 1){\n            if(index > length(items))\n                return(NULL)\n\n            ns = seq_len(min(n, length(items) - index + 1))\n            val = items[ns + index - 1]\n            index <<- index + as.integer(n)\n\n            val\n            },\n\n        peek = function(n = 1){\n            if(index > length(items))\n                return(NULL)\n\n            ns = seq_len(min(n, length(items) - index + 1))\n            items[ns]\n            },\n\n        size = function(){\n            length(items) - index + 1\n            }\n        )\n    )\n\nvec = c(5, 3, 8)\nstack = Stack(vec)\nstack$pop()\n\n[1] 5\n\nstack$pop(2)\n\n[1] 3 8\n\nstack$pop()\n\nNULL"
  },
  {
    "objectID": "posts/pop/pop.html#bare-environment",
    "href": "posts/pop/pop.html#bare-environment",
    "title": "Make it pop!",
    "section": "Bare environment",
    "text": "Bare environment\nRC classes are quite hungry and you should generally use R6 classes instead (see RC vs R6 performance).\nThere is however an interestring trick in the above article. Bare environments.\n\nnew_stack = function(x){\n    i = 1\n\n    pop = function(n = 1){\n        if(i > length(x))\n            return(NULL)\n\n        ns = seq_len(min(n, length(x) - i + 1))\n        val = x[ns + i - 1]\n        i <<- i + n\n\n        val\n        }\n\n    peek = function(n = 1){\n        if(i > length(x))\n            return(NULL)\n\n        ns = seq_len(min(n, length(x) - i + 1))\n        x[ns]\n        }\n\n    size = function(){\n        length(x) - i + 1\n        }\n\n    structure(environment(), class = \"Stack\")\n    }\n\nvec = c(5, 3, 8)\nstack = new_stack(vec)\nstack$pop()\n\n[1] 5\n\nstack$peek()\n\n[1] 5\n\nstack$size()\n\n[1] 2\n\nstack$pop(2)\n\n[1] 3 8\n\n\nYou got most of the power of R6 classes for free right there in the base R! And they are much easier to work with than RC classes and much more performant."
  },
  {
    "objectID": "posts/pop/pop.html#attributes",
    "href": "posts/pop/pop.html#attributes",
    "title": "Make it pop!",
    "section": "Attributes",
    "text": "Attributes\nAll R objects have attributes. In the above example, set class attribute of an object generated by new_stack.\nCan we set class attribute without modification of the whole object?\n\npop = function(x, n = 1){\n    if(is.null(attr(x, \"index\")))\n        attr(x, \"index\") = 1\n\n        i = attr(x, \"index\")\n\n        if(i > length(x))\n            return(NULL)\n\n        ns = seq_len(min(n, length(x) - i + 1))\n        val = x[ns + i - 1]\n        attr(x, \"index\") = i + n\n\n        val\n    }\n\nvec = c(5, 3, 8)\npop(vec)\n\n[1] 5\n\npop(vec, 2)\n\n[1] 5 3\n\n\nUnfortunatelly, this doesn’t work since setting attribute is modification of an object. We would have to use .Call interface and modify it in C function:\n\nset_index = inline::cfunction(\n    signature(x = \"SEXP\", index = \"SEXP\"),\n    body = \"duplicate(x); setAttrib(x, install(\\\"index\\\"), index); return x;\",\n    language = \"C\"\n    )\n\npop = function(x, n = 1){\n    if(is.null(attr(x, \"index\")))\n        set_index(x, 1)\n\n        i = attr(x, \"index\")\n\n        if(i > length(x))\n            return(NULL)\n\n        ns = seq_len(min(n, length(x) - i + 1))\n        val = x[ns + i - 1]\n        set_index(x, i + n)\n        cat(\"Address of 'x' is: \", address(x), \"\\n\")\n\n        val\n    }\n\nvec = c(5, 3, 8)\naddress(vec)\n\n[1] \"0x55b6d3eb6e78\"\n\npop(vec)\n\nAddress of 'x' is:  0x55b6d3eb6e78 \n\n\n[1] 5\n\npop(vec, 2)\n\nAddress of 'x' is:  0x55b6d3eb6e78 \n\n\n[1] 3 8\n\n\nWith some C level magic, everything is possible! Address is the same, meaning that we are only modifying the attribute of a vector, but not the vector itself. To get the size or peek of the current stack, we would need to create a new functions that calculate the attribute. But since these functions do not need to do any in-place modification, it will not be a lot of work."
  },
  {
    "objectID": "posts/pop/pop.html#summary",
    "href": "posts/pop/pop.html#summary",
    "title": "Make it pop!",
    "section": "Summary",
    "text": "Summary\nIn this small post we have investigated how to make simple pop function that simulate “consumption” of elements from a vector. Including memory-efficient methods providing modify-in-place seamantics.\nIf we wanted to implement full stack, queue, and deque with the put or insert functions that would add elements to the vector, the situation would be quite different and different solutions might be better. But since I didn’t need this functionality, I could afford to implement these easy solutions."
  },
  {
    "objectID": "posts/CrashDataEDA/CrashDataEDA.html",
    "href": "posts/CrashDataEDA/CrashDataEDA.html",
    "title": "Exploratory Data Analysis: NZ Crash Data",
    "section": "",
    "text": "This is an Exploratory Data Analysis (EDA) for New Zealand Crash data. The aim of EDA is to explore data, find interesting patterns, and generate set of hypothesis that are worth to explore further, but also to find inconsistencies, coding errors, duplications and so on. For this reason, EDA is often the first step that needs to be done when looking at new data before other statistical or machine learning approaches are used. Ideally, EDA will be done in collaboration with a domain expert as many of the patterns or inconsistencies can be just a property of the data, be it from the way the data was collected, or expected patterns based on the mechanism that generate the data. Since these kind of patterns might influence the model-fitting and hypothesis testing steps later in the analysis, identifying these patterns and working with domain expert is an important step to get aware of these patterns so that they do not influence steps later and do not produce false positive relationships, or mask the true ones.\nEDA is not a hard science, but more of an art, and there are no hard rules. While some tools for automatic EDA exist, see The Landscape of R Packages for Automated Exploratory Data Analysis for review, EDA is in its essence interactive. You are going through data, looking at various data points, features, and patterns, and searching for interesting questions and answers.\nThe history of EDA is interesting and its modern history starts with Tukey, but in truth, any data analysis starts with looking at patterns in the data and in truth, EDA is as old as statistics, and perhaps even predates it. After all, this is how Bayes came with, and Laplace rediscovered the Bayesian theorem, or how various probability distributions were invented/discovered. For a good review on statistics data science, and EDA, see 50 years of Data Science. Or you can go directly for the Future of Data Analysis from Tukey written in 1961. For a good tutorial for EDA, the R for Data Science from Hadley is a great source."
  },
  {
    "objectID": "posts/CrashDataEDA/CrashDataEDA.html#about-this-work",
    "href": "posts/CrashDataEDA/CrashDataEDA.html#about-this-work",
    "title": "Exploratory Data Analysis: NZ Crash Data",
    "section": "About this work",
    "text": "About this work\nThis work was done as a part of interview for the Data Analyst position with New Zealand Ministry of Transport and got me an offer. The task was to look at geographical patterns, but since this is my first time working with the data, I think that EDA is a must to familiarize oneself with the data and how the data are coded, because interpretation of the patterns will strongly depend on this.\nAll the code as well as a version of this document is available on my github. The crash data is quite large, so I didn’t include a copy of it here, but I did include copy of some population data we will use for normalization later."
  },
  {
    "objectID": "posts/CrashDataEDA/CrashDataEDA.html#eda-nz-crash-data",
    "href": "posts/CrashDataEDA/CrashDataEDA.html#eda-nz-crash-data",
    "title": "Exploratory Data Analysis: NZ Crash Data",
    "section": "EDA: NZ Crash Data",
    "text": "EDA: NZ Crash Data\nThe purpose of this analysis is to explore New Zealand crash statistics data, and create insight on crashes by region, or other spatial area. And if we find some interesting relationship, we might try to do some little non-serious modelling. Why non-serious? Because we need domain expert to discuss the data with us, but we do not have a domain expert. Because of that, we do not know what are the limitations and assumptions in the data, so any modelling done in here is purely exploratory.\nAs pointed by the previous paragraph, there are some limitations. The challenge here is to do this in a limited time, so we will be able to do only very cursory exploration and look only at some overall patterns in the data, we won’t be going much into detail both by looking at year to year differences in various statistics, or by looking closely at small geographical areas, such as spikes in the city centres and so on. We also do not have a domain expert on hand, so we can’t discuss various findings, which means we do not know if any pattern we see is a true pattern, or just something resulting from the way the data way collected/encoded. This will be obvious quite later.\nFinally, as this is exploratory analysis, do not expect high quality infographics. Quick and dirty graphs are what we are looking for. In fact, in some some cases, I prefer to not even look at graphs, instead look at distribution of numbers. Good graphs take a time to setup, and sometimes might hide some obvious issues with the data. For instance, boxplots are great for visualising basic statistics, but it hides how the data are distributed inside the box, for instance, perfectly clean boxplot might not show a bimodal distribution of data.\n\nDownloading the data\nTo get the data, we download the CSV from https://opendata-nzta.opendata.arcgis.com.\nI downloaded a table updated on 6th July 2023, and there are 821 744 records. Unfortunately, there is no way how to provide a direct link to download the data, as this is hidden behind javascript action. There is a ArcGIS REST API, but it is not documented at this page, and even the main documentation, once you figure out that you are looking for Feature Service, is less than ideal.\nGiven enough time, this could be figured out, but by default, only 2000 records are downloaded, and since we are pressed for time, I chose to not explore this further. Last time I worked with a similar API (GenBank), there was a hard limit on how many records could be downloaded at once. To download everything, one had to split the query into multiple subqueries and collate the result. To do this right, I would need to either use or write a package to provide binding for these queries and this would take quite a bit of time. It is possible to do this without package, but it might get quite a bit difficult and error-prone.\n\n\nBasic data cleaning and exploration\nI downloaded the data into the data folder, so we can start looking at it and doing some basic cleaning.\nfilename = \"data/Crash_Analysis_System_(CAS)_data.csv\"\ndata = read.csv(filename)\n\nnames(data)\n##  [1] \"X\"                         \"Y\"                        \n##  [3] \"OBJECTID\"                  \"advisorySpeed\"            \n##  [5] \"areaUnitID\"                \"bicycle\"                  \n##  [7] \"bridge\"                    \"bus\"                      \n##  [9] \"carStationWagon\"           \"cliffBank\"                \n## [11] \"crashDirectionDescription\" \"crashFinancialYear\"       \n## [13] \"crashLocation1\"            \"crashLocation2\"           \n## [15] \"crashRoadSideRoad\"         \"crashSeverity\"            \n## [17] \"crashSHDescription\"        \"crashYear\"                \n## [19] \"debris\"                    \"directionRoleDescription\" \n## [21] \"ditch\"                     \"fatalCount\"               \n## [23] \"fence\"                     \"flatHill\"                 \n## [25] \"guardRail\"                 \"holiday\"                  \n## [27] \"houseOrBuilding\"           \"intersection\"             \n## [29] \"kerb\"                      \"light\"                    \n## [31] \"meshblockId\"               \"minorInjuryCount\"         \n## [33] \"moped\"                     \"motorcycle\"               \n## [35] \"NumberOfLanes\"             \"objectThrownOrDropped\"    \n## [37] \"otherObject\"               \"otherVehicleType\"         \n## [39] \"overBank\"                  \"parkedVehicle\"            \n## [41] \"pedestrian\"                \"phoneBoxEtc\"              \n## [43] \"postOrPole\"                \"region\"                   \n## [45] \"roadCharacter\"             \"roadLane\"                 \n## [47] \"roadSurface\"               \"roadworks\"                \n## [49] \"schoolBus\"                 \"seriousInjuryCount\"       \n## [51] \"slipOrFlood\"               \"speedLimit\"               \n## [53] \"strayAnimal\"               \"streetLight\"              \n## [55] \"suv\"                       \"taxi\"                     \n## [57] \"temporarySpeedLimit\"       \"tlaId\"                    \n## [59] \"tlaName\"                   \"trafficControl\"           \n## [61] \"trafficIsland\"             \"trafficSign\"              \n## [63] \"train\"                     \"tree\"                     \n## [65] \"truck\"                     \"unknownVehicleType\"       \n## [67] \"urban\"                     \"vanOrUtility\"             \n## [69] \"vehicle\"                   \"waterRiver\"               \n## [71] \"weatherA\"                  \"weatherB\"\nThere is quite lot of features in here. One big issue with this dataset is that there is a complete lack of description what these features mean. Previously, I worked with Ethnographic Atlas, and there is a whole codebook on the meaning of individual features and the way they were coded. IMHO, this is quite an issue that should be corrected and something that could be easily resolved if we had an expert on hand.\nFor instance, what is advisorySpeed? Advised speed lower than maximum allowed speed that is put in tight corners? That would make sense. But look at what happens when we create a contingency table (bivariate relationship) of advisorySpeed and speedLimit, which I assume is the maximum allowed speed:\ntable(\"Speed Limit\"=data$speedLimit, \"Advisory Speed\"=data$advisorySpeed, useNA=\"always\")\n##            Advisory Speed\n## Speed Limit     15     20     25     30     35     40     45     50     55\n##        2         0      0      0      0      0      0      0      0      0\n##        5         0      0      0      0      0      0      0      0      0\n##        6         0      0      0      0      0      0      0      0      0\n##        10        0      0      0      0      0      0      0      0      0\n##        15        0      0      0      0      0      0      0      0      0\n##        20       11      1      0      0      0      0      0      0      0\n##        30       24     20     33      0      0      0      0      0      0\n##        40        2      2      5      3      6      0      0      1      0\n##        50      172    353    637    915   1220    498   1254      7      2\n##        51        0      0      0      0      0      0      0      0      0\n##        60        6      1     25     11     99     11    120     27    119\n##        61        0      0      0      0      0      0      0      0      0\n##        70        4      7     70     29    224     20    201    102    207\n##        80        6      8    101     35    444      8    463     83    598\n##        90        0      0      0      0      0      0      3      0      1\n##        100      65     27    658    315   2053     88   2802    421   3750\n##        110       0      0      0      0      2      0      1      0      0\n##        <NA>      0      0      0      0      0      0      0      0      0\n##            Advisory Speed\n## Speed Limit     60     65     70     75     80     85     90     95   <NA>\n##        2         0      0      0      0      0      0      0      0      1\n##        5         0      0      0      0      0      0      0      0     15\n##        6         0      0      0      0      0      0      0      0      1\n##        10        0      0      0      0      0      0      0      0    813\n##        15        0      0      0      0      0      0      0      0     10\n##        20        0      0      0      0      0      0      0      0   1934\n##        30        0      0      0      0      0      0      0      0   7261\n##        40        0      0      0      0      0      0      0      0   1800\n##        50        0      1      0      2      0      0      0      0 488142\n##        51        0      0      0      0      0      0      0      0      1\n##        60        1      3      0      1      0      0      0      0  22471\n##        61        0      0      0      0      0      0      0      0      1\n##        70       29    200      1      1      0      1      0      0  24372\n##        80       41    488     60    255      0      0      0      0  37847\n##        90        0      0      0      4      1      3      0      0    465\n##        100     213   4556    501   3684    675   1798    153    289 204254\n##        110       0      1      0      0      0      0      0      1    174\n##        <NA>      0      0      0      0      0      0      0      0    838\nAdvisory Speed is usually bellow the Speed Limit, as expected, but I would expect that Advisory Speed is always bellow the Speed Limit. In some cases, this is not true. Are the assumptions we made about the meaning of both variables correct? If so, does that mean that there is a codding error? Or something else is happening, such as incorrect signage on road?\nFor instance, we could easily subset some of the strange datapoints, for instance, where Speed Limit is 50, and Advisory Speed is larger than that:\nsubset(data, data$speedLimit == 50 & data$advisorySpeed > 50)\n##              X       Y OBJECTID advisorySpeed areaUnitID bicycle bridge bus\n## 98658  1823770 5537280   160173            75     561811       0      0   0\n## 192652 1819785 5576514   316844            65     558700       0     NA   0\n## 295973 1572148 5170547   480920            55     596400       0      0   0\n## 517292 1751258 5950941   822065            55     505805       0     NA   0\n## 721471 1571780 5170316  1158036            75     596503       0      0   0\n##        carStationWagon cliffBank crashDirectionDescription crashFinancialYear\n## 98658                1         0                     South          2020/2021\n## 192652               1        NA                      West          2018/2019\n## 295973               0         0                     North          2018/2019\n## 517292               2        NA                     South          2021/2022\n## 721471               1         1                      East          2020/2021\n##                crashLocation1     crashLocation2 crashRoadSideRoad\n## 98658            RAILWAY ROAD       MAPLE STREET                NA\n## 192652               01N-0885    KOTUKUTUKU ROAD                NA\n## 295973     GOVERNORS BAY ROAD   SANDY BEACH ROAD                NA\n## 517292 HIBISCUS COAST HIGHWAY           GRUTS BR                NA\n## 721471        DYERS PASS ROAD GOVERNORS BAY ROAD                NA\n##           crashSeverity crashSHDescription crashYear debris\n## 98658  Non-Injury Crash                 No      2021      0\n## 192652      Minor Crash                Yes      2018     NA\n## 295973 Non-Injury Crash                 No      2019      0\n## 517292 Non-Injury Crash                 No      2021     NA\n## 721471 Non-Injury Crash                 No      2020      0\n##        directionRoleDescription ditch fatalCount fence  flatHill guardRail\n## 98658                     South     0          0     0      Flat         0\n## 192652                    South    NA          0    NA      Flat        NA\n## 295973                    South     0          0     0 Hill Road         0\n## 517292                    South    NA          0    NA Hill Road        NA\n## 721471                     East     0          0     0 Hill Road         0\n##        holiday houseOrBuilding intersection kerb      light meshblockId\n## 98658                        0           NA    0       Dark     1792100\n## 192652                      NA           NA   NA       Dark     1728600\n## 295973                       0           NA    0       Dark     2711400\n## 517292                      NA           NA   NA Bright sun      171102\n## 721471                       0           NA    0       Dark     2711501\n##        minorInjuryCount moped motorcycle NumberOfLanes objectThrownOrDropped\n## 98658                 0     0          0             2                     0\n## 192652                1     0          0             2                    NA\n## 295973                0     0          0             2                     0\n## 517292                0     0          0             2                    NA\n## 721471                0     0          0             2                     0\n##        otherObject otherVehicleType overBank parkedVehicle pedestrian\n## 98658            0                0        0             0         NA\n## 192652          NA                0       NA            NA         NA\n## 295973           0                0        0             0         NA\n## 517292          NA                0       NA            NA         NA\n## 721471           0                0        0             0         NA\n##        phoneBoxEtc postOrPole                    region roadCharacter roadLane\n## 98658            0          0 Manawatū-Whanganui Region           Nil    2-way\n## 192652          NA         NA Manawatū-Whanganui Region           Nil    2-way\n## 295973           0          0         Canterbury Region           Nil    2-way\n## 517292          NA         NA           Auckland Region           Nil    2-way\n## 721471           0          0         Canterbury Region           Nil    2-way\n##        roadSurface roadworks schoolBus seriousInjuryCount slipOrFlood\n## 98658       Sealed         0         0                  0           0\n## 192652      Sealed        NA         0                  0          NA\n## 295973      Sealed         0         0                  0           0\n## 517292      Sealed        NA         0                  0          NA\n## 721471      Sealed         0         0                  0           0\n##        speedLimit strayAnimal streetLight suv taxi temporarySpeedLimit tlaId\n## 98658          50           0        None   0    0                  NA    40\n## 192652         50          NA        None   0    0                  NA    38\n## 295973         50           0          On   0    0                  NA    60\n## 517292         50          NA        None   0    0                  NA    76\n## 721471         50           0        None   0    0                  NA    60\n##                      tlaName trafficControl trafficIsland trafficSign train\n## 98658  Palmerston North City            Nil             0           0     0\n## 192652   Rangitikei District        Unknown            NA          NA    NA\n## 295973     Christchurch City        Unknown             0           0     0\n## 517292              Auckland       Give way            NA          NA    NA\n## 721471     Christchurch City            Nil             0           0     0\n##        tree truck unknownVehicleType urban vanOrUtility vehicle waterRiver\n## 98658     0     0                  0 Urban            0       0          0\n## 192652   NA     0                  0 Urban            0      NA         NA\n## 295973    1     0                  0 Urban            1       0          0\n## 517292   NA     0                  0 Urban            0      NA         NA\n## 721471    0     0                  0 Urban            0       0          0\n##        weatherA    weatherB\n## 98658      Fine        Null\n## 192652     Fine        Null\n## 295973     Fine        Null\n## 517292     Fine Strong wind\n## 721471     Fine        Null\nbut I don’t see anything particularly wrong or strange.\nKnowing this, we will start cleaning the data, and by that I mostly mean dropping variables.\nFor instance, we won’t be working with many geographical variables, and we do not need their IDs, so we can drop X, Y, OBJECTID, areaUnitID, meshblockId, crashFinancialYear, and tlaId.\nThe tla stands for Territorial Local Authority, which divides regions into subregions (contained in region variable), so we will rename tlaName into subregion for better understandability.\ndata[c(\"X\", \"Y\", \"OBJECTID\", \"areaUnitID\", \"meshblockId\", \"crashFinancialYear\", \"tlaId\")] = NULL\n\nnames(data)[names(data) == \"tlaName\"] = \"subregion\"\nThere is more cleaning that we will do, but for that, we need to look at the data. A simple lapply(data, table) will do most of the work, since all the data is categorical or ordinal. But doing so for 65 variables is quite a bit too much, so we will divide the features according to their type. We will start with vehicleType and continue with other groups such as roadConditions, otherObjects and so on.\nAlso, to help us in this, I define the operator %-% so we can see what we already defined.\n\"%-%\" = function(x,y){x[!(x %in% y)]}\n\nvehicleType = c(\"bicycle\", \"bus\", \"carStationWagon\", \"moped\", \"motorcycle\",\n    \"otherVehicleType\", \"parkedVehicle\", \"schoolBus\", \"suv\", \"taxi\", \"train\",\n    \"truck\", \"unknownVehicleType\", \"vanOrUtility\", \"vehicle\")\n\nnames(data) %-% vehicleType\n##  [1] \"advisorySpeed\"             \"bridge\"                   \n##  [3] \"cliffBank\"                 \"crashDirectionDescription\"\n##  [5] \"crashLocation1\"            \"crashLocation2\"           \n##  [7] \"crashRoadSideRoad\"         \"crashSeverity\"            \n##  [9] \"crashSHDescription\"        \"crashYear\"                \n## [11] \"debris\"                    \"directionRoleDescription\" \n## [13] \"ditch\"                     \"fatalCount\"               \n## [15] \"fence\"                     \"flatHill\"                 \n## [17] \"guardRail\"                 \"holiday\"                  \n## [19] \"houseOrBuilding\"           \"intersection\"             \n## [21] \"kerb\"                      \"light\"                    \n## [23] \"minorInjuryCount\"          \"NumberOfLanes\"            \n## [25] \"objectThrownOrDropped\"     \"otherObject\"              \n## [27] \"overBank\"                  \"pedestrian\"               \n## [29] \"phoneBoxEtc\"               \"postOrPole\"               \n## [31] \"region\"                    \"roadCharacter\"            \n## [33] \"roadLane\"                  \"roadSurface\"              \n## [35] \"roadworks\"                 \"seriousInjuryCount\"       \n## [37] \"slipOrFlood\"               \"speedLimit\"               \n## [39] \"strayAnimal\"               \"streetLight\"              \n## [41] \"temporarySpeedLimit\"       \"subregion\"                \n## [43] \"trafficControl\"            \"trafficIsland\"            \n## [45] \"trafficSign\"               \"tree\"                     \n## [47] \"urban\"                     \"waterRiver\"               \n## [49] \"weatherA\"                  \"weatherB\"\nThis helps us filtering down the list of names until we ended with everything categorized into somewhat related variables. It is just for a better systematic exploration, it doesn’t have to be perfect. For that, we would need an expert.\nroadConditions = c(\"advisorySpeed\", \"temporarySpeedLimit\", \"speedLimit\", \"roadCharacter\",\n                   \"roadLane\", \"roadSurface\", \"roadworks\", \"trafficControl\", \"trafficSign\",\n                   \"streetLight\", \"NumberOfLanes\", \"intersection\", \"trafficIsland\")\n\nterrainFeature = c(\"bridge\", \"cliffBank\", \"ditch\", \"fence\", \"flatHill\",\n                   \"guardRail\", \"houseOrBuilding\", \"kerb\", \"overBank\",\n                   \"waterRiver\", \"slipOrFlood\")\n\notherObjects = c(\"debris\", \"objectThrownOrDropped\", \"otherObject\", \"pedestrian\",\n                 \"phoneBoxEtc\", \"postOrPole\", \"strayAnimal\", \"tree\")\n\ncrashSeverity = c(\"crashSeverity\", \"fatalCount\", \"minorInjuryCount\", \"seriousInjuryCount\")\n\nweather = c(\"light\", \"weatherA\", \"weatherB\")\n\nlocation = c(\"crashDirectionDescription\", \"crashLocation1\", \"crashLocation2\", \"directionRoleDescription\", \"region\", \"subregion\", \"urban\")\n\ntime = c(\"crashYear\", \"holiday\")\n\nother = c(\"crashRoadSideRoad\", \"crashSHDescription\")\nNow, we can look closely at all these variables and their distributions.\n\nOther variables\nLet’s start with the other category:\n# this is very efficient for ordinal or categorical variables:\ndata[other] |> lapply(table)\n## $crashRoadSideRoad\n## < table of extent 0 >\n## \n## $crashSHDescription\n## \n##      No Unknown     Yes \n##  580900      55  240789\nThe crashRoadSideRoad is empty and we can safely drop it.\nI didn’t know what crashSHDescription means, so I had to google around and apparently, there is an old key from 2019! The fields and values differ, but there is at least some explanation. Apparently, this variable indicate if the crash happened on State Highway (SH) or somewhere else.\nThe encoding of crashSHDescription is nice and readable, but if we meant to fit it into model, we would like binary variable with unknown coded as NA instead, so we recode it.\ndata$crashRoadSideRoad = NULL\n\n# probably my favourite way how to do recoding\ndata$crashSHDescription = setNames(\n    c(0,1,NA), c(\"No\",\"Yes\",\"Unknown\")\n    )[data$crashSHDescription]\n\n\nTime variables\nNow we look at the time group, but nothing strange is happening in here. The number of crashes through time might be an interesting, as well as the number of crashes over holidays, but we would need to normalize it per day to know if the frequency is any greater.\nThe no holiday could use a better name than just empty string, so I recode it.\ndata[time] |> lapply(table, useNA=\"ifany\")\n## $crashYear\n## \n##  2000  2001  2002  2003  2004  2005  2006  2007  2008  2009  2010  2011  2012 \n## 31996 36125 38045 37950 37051 38364 39778 41661 39535 38247 36870 32450 30443 \n##  2013  2014  2015  2016  2017  2018  2019  2020  2021  2022  2023 \n## 30109 29784 32103 37249 39314 38469 36919 32808 34080 27982  4412 \n## \n## $holiday\n## \n##                    Christmas New Year             Easter     Labour Weekend \n##             776922              20453               9463               7055 \n##    Queens Birthday \n##               7851\ndata$holiday = replace(data$holiday, data$holiday == \"\", \"Not holiday\")\n\ndata$crashYear |> table() |>\n    plot(las=2, ylab=\"\", lwd=5, frame.plot=FALSE,\n        main = \"Number of crashes through time\")\n\n\n\nplot of chunk time\n\n\nThere is interesting cyclical behaviour. Obviously the 2023 year is incomplete, so we do not take that in account, and in similar fashion, I am not sure how complete are data from 2000, but otherwise there is a dip from 2009 to 2016. I don’t know what happened at that time, since I arrived to NZ during 2014-2015 (so I single-handedly caused an increase of crashes, cool), and then again a decrease from 2019, which surely is due to Covid, but only on 2022 it we get global minimum, and 2022 is already after lockdowns. To be complete, 2007 is a global maximum of the number of crashes in our dataset.\n\n\nLocation variables\nNow to the location. There will be quite a few geographical variables, such as region and subregion, variables that describe the road where the crash happened, such as crashLocation1 and crashLocation2, whether the crash happened in urban environment, and two direction variables, which I don’t understand even after reading the key description, so I will drop them.\nThe urban variable is quite simple, it signify whether the crash happened in an urban environment. The crashes are about twice likely to happen in an urban environment compared to an open one. This is not that surprising as most people are concentrated in cities, but NZ is still relatively rural country and as we see later, most crashes happen on the State Highways, so the difference between urban and open crashes is not overwhelming. We will recode it from a categorical to binary variable.\ndata$crashDirectionDescription = NULL\ndata$directionRoleDescription = NULL\n\ndata$urban = setNames(c(0,1), c(\"Open\", \"Urban\"))[data$urban]\nNow to the region and subregion. In both cases, there are crashes that happened in the region marked as empty string. I assume that this means that the region is unknown and recode them as NA. But after looking at the data, I am not that sure about that, since their crashLocation is known. Again, expert knowledge would surely help and since the road and surely the dropped latitude and longitude are known, the region and subregion can be derived.\nIn total, there are 16 regions and 67 subregions. We can effectively visualize 16 regions, but 67 subregions would take some work.\ndata$region = with(data, {replace(region, region == \"\", NA)})\ndata$subregion = with(data, {replace(subregion, subregion == \"\", NA)})\n\nsource(\"src/graphics.r\") # I prepared some customized barplots\ndata$region |> table() |>\n    barplot(angle=35, cex=0.6, font=2, las=2,\n        main=\"Crashes in across regions\")\n\n\n\nplot of chunk location2\n\n\ndata$subregion |> table() |> sort(decreasing=TRUE) |> head()\n## \n##          Auckland Christchurch City   Wellington City     Hamilton City \n##            285346             53011             32876             28594 \n##      Dunedin City     Tauranga City \n##             24707             18973\nAuckland has the most crashes, with Christchurch second and Wellington third. A similar situation is happening with regions, although here the Waikato region is second.\nHowever, this is to be expected, isn’t it? Auckland will have the most crashes because Auckland is the biggest baddest city in NZ. We need to normalize this by population to get some unexpected insight.\nFor this, I downloaded some StatsNZ data for population, unfortunately it is only for 2019-2021, but anything will do. After all, we are not interested in trend and population will surely not change dramatically in the past 20 years.\nThe only potential problem is to match region names, I already did some cleaning in the population data, but seems that the NZ crash database does not like Maori spelling. For instance, there is Manawatū-Whanganui Region, but only Manawatu District.\nsource(\"src/population.r\")\npopulation = get_population_year(2021)\n\nregions = data$region |> table() |> names()\nregions[!regions %in% names(population)]\n## character(0)\nsubregions = data$subregion |> table() |> names()\nsubregions[!subregions %in% names(population)]\n## [1] \"Manawatu District\"  \"Taupo District\"     \"Whakatane District\"\n## [4] \"Whangarei District\"\nWe need to fix these regions.\n# base R is not really good at this, so here is a good solution from my pkg:\nreplace2 = function(x, values, replace, ...){\n    if(length(values) != length(replace))\n        stop(\"The vector `values` and `replace` must have the same length!\")\n\n    match = match(x, values)\n    x[!is.na(match)] = replace[match][!is.na(match)]\n    x\n    }\n\nbefore = c(\"Manawatu District\", \"Taupo District\", \"Whakatane District\", \"Whangarei District\")\nafter = c(\"Manawatū District\", \"Taupō District\", \"Whakatāne District\", \"Whangārei District\")\ndata$subregion = replace2(data$subregion, before, after)\nNow we can normalize the regional counts per population:\nregions = data$region |> table() |> names()\nsubregions = data$subregion |> table() |> names()\n\ndata$region |> table() |>\n    (function(x){x/population[regions]})() |>\n    barplot(angle=35, cex=0.6, font=2, las=2, labels=FALSE,\n        main = \"Crashes across regions normalized by population\")\n\n\n\nplot of chunk population3\n\n\ndata$subregion |> table() |>\n    (function(x){x/population[subregions]})() |>\n    sort(decreasing=TRUE) |> head()\n## \n##   Waitomo District   Ruapehu District  Kaikoura District  Westland District \n##          0.3231328          0.3065116          0.2948357          0.2676768 \n## Mackenzie District    Wairoa District \n##          0.2594891          0.2522124\nNow, that is interesting. Auckland is dethroned as the most dangerous city and instead West Coast Region is a hellhole! And Waitomo district, which I never heard about. Apparently, it sparsely populated rural area. It might be possible that there is a highway and as we will saw, most crashes happens on highway. So lets check it out.\nWaitomoDistrict = subset(data, subregion == \"Waitomo District\")\nWaitomoDistrict$crashLocation1 |> table() |> sort(decreasing=TRUE) |> head()\n## \n##         SH 3         SH 4        SH 30        SH 37 TE ANGA ROAD      RORA ST \n##         1409          372          315           86           73           64\nLooks like the hunch was confirmed. Most crashes happened on State Highways 3, 4 and 30.\nFinally, we will explore the crashLocation1 and crashLocation2. According to the key, the crashLocation1 is the primary road where the crash happened, while the crashLocation2 being the secondary one, side road, or place nearby. I don’t know what exactly it means, but in both cases, State Highways and other long roads are the most frequent places for crashes.\nAgain, this should not be much of a surprise, since these roads are very long, so assuming that the chance that the car crashes is the same for every single road, the longer the road, the more crashes we would expect to see.\nUnfortunately, there is not a good way for me to check this assumption and normalize the number in the same way as we did it for the regions with population. I doubt that there is a statistics that specifies how long each road is.\ndata$crashLocation1 |> table() |> length() # 37453 unique values\n## [1] 37453\ndata$crashLocation2 |> table() |> length() # 52060 unique values\n## [1] 52060\ndata$crashLocation1 |> table() |> sort(decreasing=TRUE) |> head()\n## \n##            SH 1N             SH 2            SH 1S             SH 3 \n##            54779            21225            17586            10898 \n## GREAT SOUTH ROAD             SH 6 \n##            10243             9834\ndata$crashLocation2 |> table() |> sort(decreasing=TRUE) |> head()\n## \n##            SH 1N GREAT SOUTH ROAD             SH 2 GREAT NORTH ROAD \n##             3765             3340             2337             1822 \n##         QUEEN ST            SH 1S \n##             1771             1742\n\n\nWeather variables\nThe three weather variables are light, weatherA and weatherB.\nI am not sure if the Dark category in the light variable means nighttime, or if this might mean that it was just very cloudy day, but I guess cloudy day is the Overcast category. In that case, about 30% of crashes happened during night. Since there is less traffic during night, this is quite significant increase from what I would expect, but I have no means of normalizing this to get precise answer.\ndata$light |> table(useNA=\"ifany\") |> (function(x){x/sum(x)})()\n## \n##  Bright sun        Dark    Overcast    Twilight     Unknown \n## 0.368695117 0.274927471 0.299935260 0.046695808 0.009746344\ndata$light[data$light == \"Unknown\"] = NA\n\ndata$weatherA |> table(useNA=\"ifany\")\n## \n##          Fine Hail or Sleet    Heavy rain    Light rain   Mist or Fog \n##        635621           132         33153        124210         11306 \n##          Null          Snow \n##         15778          1544\ndata$weatherA[data$weatherA == \"Null\"] = NA\nThe two other weather variables are quite peculiar. weatherA encodes weather, everything seems to be relatively standard, just using Null to encode unknown data (that is like the fifth different value for unknown data we have encountered, bruh). The majority of crashes happened during Fine weather and Light rain, which would be the majority of days, it rains quite a lot in NZ, while Hail and Snow are quite uncommon. Still, there was quite a lot of fog in Dunedin so I would expect more crashes during that time. We could check it out:\nFoggy = subset(data, weatherA == \"Mist or Fog\")\nFoggy$region |> table() |> sort(decreasing=TRUE)\n## \n##            Waikato Region           Auckland Region         Canterbury Region \n##                      2566                      2154                      1699 \n##              Otago Region          Northland Region         Wellington Region \n##                       932                       615                       613 \n## Manawatū-Whanganui Region      Bay of Plenty Region          Southland Region \n##                       506                       489                       447 \n##        Hawke's Bay Region         West Coast Region           Taranaki Region \n##                       444                       224                       196 \n##             Tasman Region           Gisborne Region        Marlborough Region \n##                       127                       112                       108 \n##             Nelson Region \n##                        52\nFoggy$subregion |> table() |> sort(decreasing=TRUE) |> head()\n## \n##           Auckland  Christchurch City   Waikato District      Hamilton City \n##               2154                800                645                484 \n##       Dunedin City Far North District \n##                458                293\nLooks like Waikato is the most misty region, at least regarding crashes. Auckland is still there, although it is not that dominant as we would expect from the total number of crashes and population. All things considered, Dunedin is not that misty, even Christchurch beats it. But again, if we normalized this by the total number of crashes or population, the numbers would surely look different. This doesn’t mean that this pattern doesn’t exist and it is not a valuable insight, just that there are different ways how to look on these patterns.\nNow, WeatherB is quite a bit weirder. It has two categories None and Null. Not sure if it means unknown data or just nothing further description. The overwhelming presence of the Null would suggest just no further information. Given this, I am not that willing to use this variable for further modelling. Frost and Strong Wind would certainly have an effect on the probability that a crash will occur or on their severity, but this variable is miscoded. Merging it with WeatherA would be meaningful, although it would introduce a bit too many categories, which is another reason for dropping this, or make a note to look at this later with an expert.\ndata$weatherB |> table()\n## \n##       Frost        None        Null Strong wind \n##        9254           5      798096       14389\nlapply(c(\"Frost\", \"Strong wind\"), function(x){subset(data, weatherB == x, select=weatherA) |> table()})\n## [[1]]\n## \n##          Fine Hail or Sleet    Heavy rain    Light rain   Mist or Fog \n##          7140            21            43           333           894 \n##          Snow \n##           385 \n## \n## [[2]]\n## \n##          Fine Hail or Sleet    Heavy rain    Light rain   Mist or Fog \n##          7217            23          3274          3360           135 \n##          Snow \n##           177\nWhen we look at interactions between weatherA and weatherB, we can see that Frost is associated a little bit more with Mist or Fogand Snow, while Strong wind is associated quite strongly with Heavy rain and Light rain. In both cases, the number of crashes during Fine is almost identical.\n\n\nCrash Severity variables\nNow we are getting into interesting data that we might explore rather with modelling.\nlapply(data[crashSeverity], table, useNA=\"ifany\")\n## $crashSeverity\n## \n##      Fatal Crash      Minor Crash Non-Injury Crash    Serious Crash \n##             7589           191336           575954            46865 \n## \n## $fatalCount\n## \n##      0      1      2      3      4      5      6      7      8      9   <NA> \n## 814154   6854    567    115     39      7      3      2      1      1      1 \n## \n## $minorInjuryCount\n## \n##      0      1      2      3      4      5      6      7      8      9     10 \n## 615625 165582  30358   6996   2164    649    228     83     23     13      7 \n##     11     12     13     14     15     16     18     21     26     30     34 \n##      2      1      1      1      2      2      2      1      1      1      1 \n##   <NA> \n##      1 \n## \n## $seriousInjuryCount\n## \n##      0      1      2      3      4      5      6      7      8      9     10 \n## 772759  43060   4596    924    271     86     28      8      5      1      3 \n##     12     14   <NA> \n##      1      1      1\nThe crashSeverity variable is a summary variable that tells us how severe was the crash. Fortunately, only 7589 crashes were fatal over the 20 years, with total of 8573 people perished. The most serious was a crash where 9 people died in total.\n# Total number of people died over the 20 year period:\ndata$fatalCount |> table() |> (function(x){x * as.numeric(names(x))})() |> sum()\n## [1] 8573\n# Explore the cases where deaths > 6\n# subset(data, crashSeverity > 6)\nLooking at the 4 cases with high death count, I don’t see anything particular. In all cases, this hppened on fine weather, State Highway with sealed road with speed limit 100. In one case there was temporary speed limit 30, maybe due to roadwork but the roadwork variable is set to unknown.\n\n\nOther objects\nCategory of “I don’t know where to put it”.\nlapply(data[otherObjects], table, useNA=\"ifany\")\n## $debris\n## \n##      0      1      2      3      4      5      6      7   <NA> \n## 330374   2376    131     24      4      2      1      1 488831 \n## \n## $objectThrownOrDropped\n## \n##      0      1      2      3      4   <NA> \n## 332232    636     35      8      2 488831 \n## \n## $otherObject\n## \n##      0      1      2      3      4      5   <NA> \n## 325189   7662     51      7      2      2 488831 \n## \n## $pedestrian\n## \n##      1      2      3      4      5      6   <NA> \n##  25681    785    110     23      3      3 795139 \n## \n## $phoneBoxEtc\n## \n##      0      1      2      3   <NA> \n## 328797   4096     19      1 488831 \n## \n## $postOrPole\n## \n##      0      1      2      3      4   <NA> \n## 292252  40439    214      7      1 488831 \n## \n## $strayAnimal\n## \n##      0      1      2      3   <NA> \n## 331843    994     72      4 488831 \n## \n## $tree\n## \n##      0      1      2      3   <NA> \n## 299466  33089    354      4 488831\nFrom the pattern of NA values, it looks like that this category is quite interconnected, with only pedestrian being the weird one. The pedestrian variable should probably be in the vehicles class.\nOut of all of these, the only common objects seem to be the postOrPole and tree with about 5 and 4 percent of all crashes respectively. Other objects are quite rare.\n\n\nTerrain features\nFirst thing that hits me is the pattern of NA. We already saw it in the otherObjects category. Looks like for more than half of the data, any further description is simply missing. So I looked again at the key and these all are not description of the state of the terrain, but how many times X was hit during the crash. That is, the fence was hit once in 68473 cases. Other than fence, only CliffBank and ditch appear to be somewhat common, but well bellow 5 percent of cases.\nThe only description of terrain is in fact flatHill, which is also a badly encoded variable with categories Flat, Hill Road and Null. So I make null about all this, remove the flatHill and encode it simply as a hill with 0, 1 and NA cases.\n# Its quite bit of text, so we won't use this:\n# lapply(data[terrainFeature], table, useNA=\"ifany\")\n\n# Instead we use barplots from src/graphics.r\ndata[terrainFeature ] |> lapply(table, useNA=\"ifany\") |> barplots()\n\n\n\nplot of chunk terrainFeatures\n\n\ndata$hill = setNames(c(0, 1, NA), c(\"Flat\", \"Hill Road\", \"Null\"))[data$flatHill]\ndata$flatHill = NULL\n\n\nRoad conditions\nRemember, this is EDA, the plots do not have to be pretty or even that detailed. When we are looking cursory on a large number of plots, all we need to know is if there is some basic pattern and if we need to look deeper.\ndata[roadConditions ] |> lapply(table, useNA=\"ifany\") |> barplots()\n\n\n\nplot of chunk roadConditions\n\n\nFor instance, we can already see that intersection is degenerated/constant variable and can be simply dropped. We can also see that we need to review Speed variables (advisorySpeed, temporarySpeedLimit, and speedLimit) separately, look at roadCharacter and trafficControl more closely, and recode a bunch of Null into NA.\nSo let’s look at the variables more closely.\ndata[c(\"advisorySpeed\", \"temporarySpeedLimit\", \"speedLimit\")] |>\n    lapply(table, useNA=\"ifany\")\n## $advisorySpeed\n## \n##     15     20     25     30     35     40     45     50     55     60     65 \n##    290    419   1529   1308   4048    625   4844    641   4677    284   5249 \n##     70     75     80     85     90     95   <NA> \n##    562   3947    676   1802    153    290 790400 \n## \n## $temporarySpeedLimit\n## \n##      8     10     20     30     40     45     50     60     70     75     80 \n##      1    106    151   5757    817      1   2547    396   1169      2   1501 \n##     90    100   <NA> \n##    134      1 809161 \n## \n## $speedLimit\n## \n##      2      5      6     10     15     20     30     40     50     51     60 \n##      1     15      1    813     10   1946   7338   1819 493203      1  22895 \n##     61     70     80     90    100    110   <NA> \n##      1  25468  40437    477 226302    179    838\ndata$advisorySpeed = NULL\ndata$temporarySpeedLimit = NULL\ndata$intersection = NULL\nThe number of NA values in advisorySpeed and temporarySpeedLimit suggest that the data are missing not because they are unknown, but because there is no advisory or temporary speed. This is another point we need to be aware when doing modelling, as interpretation will change drastically.\nFor instance, since this value is not missing, it would be folly to use methods to estimate it, such as through frequentist or bayesian mean. Instead, it is a conditional variable, either there is or isn’t advisory/temporary speed, or there is one with a certain value.\nI got a feeling that trying to model these variables in relationship to speedLimit would be something that would most resemble the conditions on road, i.e., looking at what is the speed limit and if it is significantly different from the other two variables. If it is, flipping a switch in a binary representation. For instance, if there is a sharp turn with advisory speed 25, it will mean something different on State Highway with speed limit 100 and smaller road with speed limit 30.\nBut this is really something that should be discussed with an expert. After all, this is why we are doing EDA after all. In the meantime, I will not use these variables (i.e., drop them), since properly exploring them will take quite a lot of time, I am already spending too much time on this, and I still can’t see the end.\nNow back to the speedLimit. I did not honestly know that there were places with speed limit 2. Looking at the item with subset(data, speedLimit == 2) tells me that it is a crash in Auckland during nighttime on a sealed road between two station wagons, one of which was parked, but no temporary speed limit or roadwork.\nNext we look at roadworks, trafficSign and trafficIsland. Unlike what I originally thought, these are items similar to a tree, riverBanks and similar, objects that were hit during the crash, and not road conditions. Again, not very interesting variables since we lack this information for more than half of the data points. I will again make note for a future me here and write down some thoughts. Feels to me like we could derive a single variable from all of these, such as “other objects were hit”. But it is also likely that these variables were collected for a reason, I can imagine crashes against trees being absolutely lethal, and encoding trees together with other variables might not be what we want. So maybe binarize them all and use Lasso to find which ones are significant?\ndata[c(\"roadworks\", \"trafficSign\", \"trafficIsland\") ] |> lapply(table, useNA=\"ifany\") |> barplots()\n\n\n\nplot of chunk roadConditions2\n\n\nRoad character is another variable that could use better encoding. There is a big category Nil cotaining the majority of the data. But it looks like No special feature or Normal rather than Unknown data. In here the encoding is quite obvious so I will recode it this way.\ndata$roadCharacter |> table(useNA=\"ifany\")\n## \n##        Bridge Motorway ramp           Nil      Overpass     Rail xing \n##         16365         11503        789988           646          2157 \n##    Speed hump    Tram lines        Tunnel     Underpass \n##           579            30           145           331\ndata$roadCharacter = replace2(data$roadCharacter, \"Nil\", \"Normal\")\ntrafficControl has not one, but two different “missing data” variables, Nil and Unknown. The key is not really helpful as it just says that these categories exists, but what is the difference between those two? For safety, I am dropping this variable.\nWe are left with roadLane, roadSurface, streetLight, and numberOfLanes. They looks well-behaved, we just need to recode missing data for three of them.\ndata$trafficControl = NULL\ndata$roadLane = replace2(data$roadLane, \"Null\", NA)\ndata$roadSurface = replace2(data$roadSurface, \"Null\", NA)\ndata$streetLight = replace2(data$streetLight, \"Null\", NA)\n\n\nVehicle Type\nAnd we are at the last variable class, the vehicleType class.\ndata[vehicleType] |> lapply(table, useNA=\"ifany\") |> barplots()\n\n\n\nplot of chunk vehicleType\n\n\nFrom the missing values, you can see that there are two classes. The parkedVehicle, train and vehicle, and the rest. The first tree are interpreted as “how many times X was struck during the crash”, while the other variables are keyed as “how many X were involved in the crash”. Quite the different interpetation, and the pattern of missing variables. I can feel something is happening in there, we saw the 488831 missing data quite a lot.\n# Not super readable, but you can see the general pattern\nis.na(data) |>\n    colSums() |>\n    sort(decreasing=TRUE) |>\n    barplot(angle=45, labels=FALSE, cex.axis=0.4, font=2,\n        main = \"Missing data across variables\")\n\n\n\nplot of chunk NApattern\n\n\n# For the list of variables:\nis.na(data) |> colSums() |> (function(x){subset(x, x == \"488831\")})() |> names()\n##  [1] \"bridge\"                \"cliffBank\"             \"debris\"               \n##  [4] \"ditch\"                 \"fence\"                 \"guardRail\"            \n##  [7] \"houseOrBuilding\"       \"kerb\"                  \"objectThrownOrDropped\"\n## [10] \"otherObject\"           \"overBank\"              \"parkedVehicle\"        \n## [13] \"phoneBoxEtc\"           \"postOrPole\"            \"roadworks\"            \n## [16] \"slipOrFlood\"           \"strayAnimal\"           \"trafficIsland\"        \n## [19] \"trafficSign\"           \"train\"                 \"tree\"                 \n## [22] \"vehicle\"               \"waterRiver\"\nAll of them are other objects that might have been hit during the crash.\nAnd this is all. We went through all the variables and looked at their individual patterns. Now we should move to relationship between two variables, but I run out of time, so maybe later!\n\n\n\nPatterns across regions\nWe do not have time to do full bivariate exploration, but we wanted to look at some patterns across regions.\nFirst, I want to look at which region is the deadliest. Don’t worry, I don’t have any unhealthy morbid obsession, but since we are investigating car crashes, this seems a natural thing to look at and try to identify causes. We already know that Auckland has the most crashes simply because it has the highest population, so we will normalize according to the number of crashes.\ncrashSeverity = table(data$region, data$crashSeverity)[\n    , c(\"Non-Injury Crash\", \"Minor Crash\", \"Serious Crash\", \"Fatal Crash\")\n    ] # sorted for convenience\ncol = palette.colors(4, \"Set 1\")\n\npar(\"mar\" = c(6,9,2,2))\n(crashSeverity / rowSums(crashSeverity)) |>\n    t() |>\n    graphics::barplot(horiz=TRUE, las=1, col=col, border=NA, cex.names=0.6, font=2,\n        main = \"Severity of crashes\")\nlegend(\"top\", legend = colnames(crashSeverity), fill = col, ncol = 2,\n    inset = c(0, 1.1),  xpd = TRUE, bty = \"n\")\n\n\n\nplot of chunk regions\n\n\nContrary to the number of crashes, Auckland seems to be a relatively safer region, since most crashes are without any injury. On the other hand, West Coast and Northland are more dangerous\npar(\"mar\" = c(4,9,2,2))\nfatalCount = table(data$region, data$fatalCount)\n(t(fatalCount) * as.numeric(colnames(fatalCount))) |>\n    colSums() |>\n    graphics::barplot(horiz=TRUE, las=1, border=NA, cex.names=0.6, col=col[4],\n        font=2, main=\"Total crash fatalities\")\n Another way to look at this is to look at total number of deaths, this include multiple deaths per crash. Despite Waikato having only 30% population of Auckland, it is way above Auckland in the number of deaths in crashes over 20 year period.\nAnd to finish it, to escape the morbidity, I will look at weather.\nweather = table(data$region, data$weatherA)[\n    , c(\"Fine\", \"Light rain\", \"Heavy rain\", \"Mist or Fog\", \"Hail or Sleet\", \"Snow\")\n    ] # sorted for convenience\ncol = palette.colors(6, \"Set 1\") |> rev()\npar(\"mar\" = c(6,9,2,2))\n(weather / rowSums(weather)) |>\n    t() |>\n    graphics::barplot(horiz=TRUE, las=1, col=col, border=NA, cex.names=0.6, font=2,\n        main = \"Weather during accidents\")\nlegend(\"top\", legend = colnames(weather), fill = col, ncol = 3,\n    inset = c(0, 1.1),  xpd = TRUE, bty = \"n\")\n Looks like Nelson and Marlborough are the sunniest regions, while Southland and Otago the snowiest. I can confirm, I lived in Dunedin, on the hills there. But usually the snow thawed during day, so you could enjoy it only at night or early in the morning. Still, snow in New Zealand, fun times.\n\n\nModelling\nHere we just do a quick look into the modelling. Some simple models are convenient to do during data-exploration, particularly the three methods are very convenient, be it CART or RandomForest. They can all handle continuous and categorical data, missing data, and are relatively robust, with RandomForest being also quite performant, often just little behind well-optimized Gradient Boosting algos.\nA big advantage of CART is that it provides a great graphical output and is easy to interpret. Although they are not as performant, they can help you discover patterns in data, which means they are great for EDA.\nFirst of all, we will make a model data containing a variables we want to use during modelling.\nThen we fit a simple CART. At first, we do not care about any test error, we want to just ascertain the pattern in the data.\nWe want to model crashSeverity based on various reasonable variables we have selected during our process. Lets start with types of vehicles involved, weather, light conditions, holiday time, hill, urban environment, road characters, and whether, but not other objects.\nWe won’t include fatalCount, minorInjuryCount and majorInjuryCount as this will cause data leakage.\nmdata = data[c(\n    \"bicycle\", \"bus\", \"carStationWagon\",\n    \"crashSeverity\", \"crashSHDescription\",\n    \"holiday\", \"light\", \"moped\", \"motorcycle\",\n    \"NumberOfLanes\", \"otherVehicleType\", \"pedestrian\",\n    \"roadCharacter\", \"roadLane\", \"roadSurface\",\n    \"schoolBus\", \"speedLimit\", \"streetLight\",\n    \"suv\", \"taxi\", \"truck\", \"unknownVehicleType\",\n    \"weatherA\", \"hill\"\n    )]\n# we also need to convert them to factors\nchars = lapply(mdata, class) == \"character\"\nmdata[chars] = lapply(mdata[chars], factor)\n\nlibrary(\"tree\")\ntree = tree(crashSeverity ~ ., data=mdata)\nplot(tree)\ntext(tree, pretty=0, cex=0.5)\n Uh, I have expected a more deeper tree. This is really bad, even if the model is ultimately not good, you would expect for CART to pick up some pattern.\nThe error is terrible, we have predicted over 80% of classes wrong.\nWe can see what is happening by looking at the contingency table. Remember that because of our mad model, we are classifying only between minor and serious crash.\npredicted = predict(tree, newdata = mdata, type=\"class\")\nerror = predicted !=  mdata$crashSeverity\ntable(\"Predicted\"=predicted, \"actual\"=data$crashSeverity)\n##                   actual\n## Predicted          Fatal Crash Minor Crash Non-Injury Crash Serious Crash\n##   Fatal Crash                0           0                0             0\n##   Minor Crash             1797      113086           391522         22476\n##   Non-Injury Crash           0           0                0             0\n##   Serious Crash           5792       78250           184432         24389\nLooks like we just can’t predict well non-injury crashes. These are distributed both among our predicted minor and serious crash.\nWe know that CART is bad, but we didn’t know that it is that bad. We would expect it to pick up on some signal, but either we have removed it by filtering some features, or type of crash just can’t be predicted from the data on hand.\nBut here is a thought. Non-injury is quite overrepresented in the data and thus we would need to build quite deep tree to find differences. I can get to this point my manipulating with mindev, so maybe there is something hidden deep inside, but there isn’t a single clear pattern.\nlibrary(\"randomForest\")\nset.seed(1) # for replicability\n\nmdata = na.roughfix(mdata) # randomForest doesn't handle missing values well\nmdata = mdata[sample(1:nrow(mdata), size=20000),] # subsample, otherwise my computer breaks\nrf = randomForest(crashSeverity ~ ., data=mdata)\nrf\n## \n## Call:\n##  randomForest(formula = crashSeverity ~ ., data = mdata) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 4\n## \n##         OOB estimate of  error rate: 27.78%\n## Confusion matrix:\n##                  Fatal Crash Minor Crash Non-Injury Crash Serious Crash\n## Fatal Crash                0          17              151            13\n## Minor Crash                0         628             3861            76\n## Non-Injury Crash           0         315            13739            22\n## Serious Crash              1         254              846            77\n##                  class.error\n## Fatal Crash       1.00000000\n## Minor Crash       0.86243154\n## Non-Injury Crash  0.02394146\n## Serious Crash     0.93463497\nThe OOB estimate of error is promising, but closer look at the confusion matrix and class error shows that we are unable to predict anything and the relatively low error is caused purely by the overrepresented Non-Injury Crash."
  },
  {
    "objectID": "posts/CrashDataEDA/CrashDataEDA.html#conclusion",
    "href": "posts/CrashDataEDA/CrashDataEDA.html#conclusion",
    "title": "Exploratory Data Analysis: NZ Crash Data",
    "section": "Conclusion",
    "text": "Conclusion\nI have performed Exploratory Data Analysis on the New Zealand Crash data. It was a bit contrived and we have spend a lot of time trying to clean the data and figure out what each variable or category means rather than looking more into patterns. This is another reason why you should always have an expert on hand, or become one, when you are working with data.\nAside of cleaning, we have did some exploration, notably into the fatalities. Auckland is not as terrifying as it might look like, it has a large number of crashes, but this is to be expected due to its large population, and most crashes are without injury. On the other hand, you should be a bit worried if you are living or driving in Waikato.\nIn the end, we weren’t able to find reason for why crashes become fatal. This is another reason why you should be vigilant and try to err on the safe side. Any crash, no matter the speed, road type, or weather condition can turn into serious injury or even become fatal. So drive safe, you are not the only one on the road."
  }
]