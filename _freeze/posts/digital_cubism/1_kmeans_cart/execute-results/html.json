{
  "hash": "bd12246b9acf1b309647247a59338868",
  "result": {
    "markdown": "---\ntitle: \"Digital Cubism (1)\"\ndescription: \"Kmeans and CART\"\ndate: \"2023-03-15\"\ncategories:\n    - R\n    - graphics\nimage: 1_kmeans.png\ntoc: true\ntoc-depth: 4\n---\n\n::: {.cell show='false'}\n\n```{.r .cell-code}\nknitr::knit_hooks$set(crop = knitr::hook_pdfcrop)\n```\n:::\n\n\nI generaly do not like cubism, to me it looks messy, with body parts all over the place.\nBut there is a subset of cubism that appeals to me, pieces like [Girl with Mandolin](https://en.wikipedia.org/wiki/Girl_with_a_Mandolin), with the sharp outline and geometric appearance, yet\nthe figure is still easily recognized.\n\n![Girl with mandolin, Credit: Wikipedia](girl_with_mandolin.jpg)\n\nThis gave me an idea to try approximate image with a series of geometric shapes.\nIdeally, we would use only a few squares, rectangles and so on, but achieve a recognizable image.\nMabye this is isn't truly cubism, but more something akin to [mosaic](https://en.wikipedia.org/wiki/Mosaic) or [Stained Glass](https://en.wikipedia.org/wiki/Stained_glass) technique but it sounds like fun.\n\nI wasn't the only one who came with this idea. Found an R blog [is.R()](is-r.tumblr.com), who approached this problem through [k-means](https://is-r.tumblr.com/post/36732821806/images-as-voronoi-tesselations).\n\nFor these experiments, I will use my own photo, there are plenty convenient colour contrasts, but you can use whatever you want.\n\n### k-means\n\nFirst, we will reproduce the technique through k-means.\nThis is quite smart idea, because the code is really simple.\nWe just need to unroll an image from its raster representation (RGB array) into a `data.frame`, and back.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nimage2df = function(img){\n    cols = ncol(img)\n    rows = nrow(img)\n\n    x = rep(seq_len(cols), each=rows)\n    y = rep(seq_len(rows), times=cols)\n\n    # R matrices are in column format, so we can just convert them to vectors\n    r = img[,,1] |> as.vector()\n    g = img[,,2] |> as.vector()\n    b = img[,,3] |> as.vector()\n\n    data.frame(x = x, y = y, r = r, g = g, b = b)\n    }\n\n\ndf2image = function(df){\n    img = array( dim = c(max(df$y), max(df$x), 3) )\n    img[,,1] = df$r\n    img[,,2] = df$g\n    img[,,3] = df$b\n\n    structure(img, class = c(\"image\", \"array\"))\n    }\n\n\nplot.image = function(x){\n    plot.new()\n    par(\"mar\" = c(0, 0, 0, 0))\n    plot.window(c(1, ncol(x)), c(1, nrow(x)), xaxs = \"i\", yaxs = \"i\", asp=1)\n    rasterImage(x, 1, 1, ncol(x), nrow(x), interpolate = FALSE)\n    }\n```\n:::\n\n\nOnce we have that, applying k-means is simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans.image = function(img, centers, iter.max = 10, ...){\n    df = image2df(img)\n    means = kmeans(df, centers, iter.max = iter.max, ...)\n    color = means$centers[means$cluster, 3:5]\n    df2 = cbind(df[,1:2], color)\n    img2 = df2image(df2)\n    img2\n    }\n```\n:::\n::: {.cell hash='1_kmeans_cart_cache/html/unnamed-chunk-4_99f5d81ab40e35fcb560e4e9b592c08c'}\n\n```{.r .cell-code}\nlibrary(\"jpeg\")\n\n# use your own image here\nimg = structure( readJPEG(\"profile.jpg\"), class = c(\"image\", \"array\") )\nkmeans.image(img, 500) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAll we do here is basically applying k-means on the coordinates. This means that all we are doing is constructing voronoi polygons. The colour information does not seem to have a strong effect because on the 0-1 scale, the colour differences are miniscule compared to differences between coordinates. Yet, running kmeans with colour calculates the average colour of each polygon.\n\nThe number of polygons used to approximate the image has quite an visual effect, from a more abstract cubic-like drawing, to a mosaic-like appearance.\n\n::: {.cell crop='true' hash='1_kmeans_cart_cache/html/unnamed-chunk-5_c244b04b5e539c10153282287d44ab5b'}\n\n```{.r .cell-code}\npar(\"mfrow\" = c(2,3))\nfor(i in c(50, 100, 200, 500, 1000, 2000))\n    kmeans.image(img, i) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe difference might be quite apparent for a terrain pictures, such as this one from New Zealand:\n\n\n::: {.cell crop='true' hash='1_kmeans_cart_cache/html/unnamed-chunk-6_99c5544826bd6ca8c63a3787f534e1c2'}\n\n```{.r .cell-code}\npar(\"mfrow\" = c(1,1))\nterrain = structure( readJPEG(\"terrain.jpg\"), class = c(\"image\", \"array\") )\nterrain |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(\"mfrow\" = c(2,3))\nfor(i in c(50, 100, 200, 500, 1000, 2000))\n    kmeans.image(terrain, i) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n:::\n\n\nOn the higher range (200 and more), it lookst just as pixelation. Small number of polygons are not able to approximate the look of the image well enough, but 200-1000 looks like a sweet spot.\n\n### CART\n\nOriginally I planned to go with this polygonal approach and do some adaptive voronoi polygons, move the points so that the error of the approximation is minimal. But then I was thinking, are there some other classical statistical methods that would be easily adaptable to this problem?\n\nThink what we want to do, divide image into areas. Well, the CART does just that, it divides the dimensions of the explanatory variables into subsets, and the response in each subset is the average response of all points in the subset. This is quite close to what we want to do!\nAnd since we have our image in a `data.frame` representation, running the CART algorithm is easy!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tree\")\ntree.image = function(img, ...){\n    df = image2df(img)\n    \n    col = list()\n    for(i in c(\"r\",\"g\",\"b\")){\n        fit = paste0(i, \" ~ x + y\") |> formula() |> tree(data=df, ...)\n        col[[i]] =  predict(fit, df)\n        }\n\n    df[names(col)] = col\n    img2 = df2image(df)\n    img2\n    }\n\ntree.image(img) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe disadvantage of this approach is that the `tree()` does not allows for a multiple dependent variables, so we need to run the algorithms in each colour space separately.\n\n\n::: {.cell crop='true'}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Modified version of plot.image\n# plot a selection of the colour layers\nplot.image = function(x, col=c(\"r\",\"g\",\"b\")){\n    col = match.arg(col, several = TRUE)\n    for( i in which(!c(\"r\",\"g\",\"b\") %in% col))\n        x[,,i] = 0\n\n    plot.new()\n    par(\"mar\" = c(0,0,0,0))\n    plot.window(c(1, ncol(x)), c(1, nrow(x)), xaxs = \"i\", yaxs = \"i\", asp=1)\n    rasterImage(x, 1, 1, ncol(x), nrow(x), interpolate=FALSE)\n    }\n\npredicted = tree.image(img)\n\npar(\"mfrow\"=c(1,3))\nfor(i in c(\"r\",\"g\",\"b\"))\n    plot.image(predicted, i)\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nSurprisingly, the information carried by green and blue is almost identical. Only reds seems to carry significantly different information.\n\nWhat controls the amount of approximation is the `mindev` parameter, other parameters starts to have an effect as a stopping parameters only when `mindev` is decreased significantly.\n\n\n::: {.cell crop='true' hash='1_kmeans_cart_cache/html/unnamed-chunk-9_88aca6086a5a43c670575038f50c0cb1'}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(\"mfrow\"=c(1,1))\ntree.image(img, mindev=10^-4) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npar(\"mfrow\" = c(1,3))\nfor(i in c(10^-3, 10^-4, 10^-5))\n    tree.image(img, mindev=i) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\nLooks like CART can produce a sharper borders, the outline of is already quite visible with `mindev=10^-3`. But it doesn't handle details well, unless we use a CART tree with a lot of tips.\n\nSo like before, lets look how this method works on terrain:\n\n::: {.cell crop='true' hash='1_kmeans_cart_cache/html/unnamed-chunk-10_ec86f5c37cd7ff5cb7a016ed95d0cdbd'}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(\"mfrow\"=c(1,1))\ntree.image(terrain, mindev=10^-4) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code  code-fold=\"true\"}\npar(\"mfrow\" = c(1, 3))\nfor(i in c(10^-3, 10^-4, 10^-5))\n    tree.image(terrain, mindev=i) |> plot.image()\n```\n\n::: {.cell-output-display}\n![](1_kmeans_cart_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n:::\n\n\n### Summary\nYou can do some cool things with R in just a few lines if you apply basic machine-learning methods creatively. The results are not exactly what I wanted however, so I will try to look into this problem a bit more, such as implementing the CART algorithm for multiple response variables.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}